
\documentclass[10pt]{article}
\input{preamble.tex}

% @jlp: put in overleaf
% @jlp: use my fonts for title and main text

\title{\vspace{-50pt}\db{
%On Why Principal Component Analysis Does the Wrong Thing
%LOL $>$ PCA
% Supervised Manifold Learning via Linear Optimal Low-rank Embedding
 Do Not Use Principal Components Analysis (PCA) for Prediction Problems, LOL is Typically Better
% Wide Supervised Manifold Learning by Low-Rank Optimal Linear Discriminant Analysis (LOL)
}}
\author{Joshua T.~Vogelstein,  Minh Tang, Da Zheng, Randal Burns, Mauro Maggioni}
\date{}

% Research Articles (up to ~4500 words, including references, notes and captions, or ~5 printed pages) are expected to present a major advance. Research Articles include an abstract, an introduction, up to six figures or tables, sections with brief subheadings, and about 40 references. Materials and Methods should be included in supplementary materials, which should also include information needed to support the paper's conclusions.

% intro: 1500
% results

\begin{document}
\maketitle

%%% 1 idea per sentence, have a compound sentence when requiring gestalt meaning

%%% 1st sentence should carry the meaning of the whole paragraph

\begin{abstract}
It is increasingly common to acquire scientific data with hundreds, millions, or billions of features. 
The goal  of these massive datasets is often to discover directions (or dimensions) that separate the data into two classes (for example, healthy and diseased). 
Unfortunately, when the dimensionality of the feature set is larger than the sample size, there are an infinite number of equally good options. 
Moreover, directly finding a small set of dimensions that maximize classification accuracy is computationally intractable.  
Instead, the nearly ubiquitous approach is to utilize Principal Components Analysis (PCA) to find a low-dimensional representation of the features that maximize  variance.
However, in general there is no reason to suspect that the dimensions that maximize classification accuracy will be close to those that maximize variance.
Indeed, we show that, under reasonable assumptions, those two sets of directions will be approximately independent.
To mitigate this issue, we developed a closed-form 
% @jovo: is closed form right here?
method called ``Linear Optimal Low-rank'' embedding (\Lol), which extends PCA by 
by jointly utilizing both the dimensions that maximize classification accuracy (ignoring the variance) and the dimensions that maximize variance (ignoring the classification task). 
We show via a combination of theoretical results and simulations that \Lol~finds a better low-dimensional representation of the data for subsequent classification under  relatively general settings, meaning that no matter what the dimensionality of the original data, or the dimensionality of embedded data, or sample size, \Lol~finds a better representation of the data for subsequent classification, while adding negligible additional computational cost. Additional numerical experiments demonstrate that the same intuition may fruitfully be applied to hypothesis testing and regression.
We demonstrate the performance of \Lol~on three different applications: (i) sparse genetics, (ii) dense images, and (iii) connectomics application with over 500 million features comprising $>$400 gigabytes.
In each case, \Lol~outperforms the other methods, while only requiring a few minutes on a single machine to run even on the huge dataset. Our open source implementation of \Lol~is easy to use, and computationally efficient, making it poised to tackle supervised dimensionality reduction challenges across disciplines.
\end{abstract}

\vspace{15pt}

% \clearpage
\linenumbers

% intro: we get 64, we have 157.  need to reduce by 2x


% properties we want classifier to have:
%1. simple geometric intuition (uses means and variances)
%2. can be applied to multiclass problems, and generalized to other problems
%3. theory (for small p)
%4. efficient alg (for small p)

% but LDA doesn't have:
%4. strong theoretical justification for performance for any dimensionality
%4. efficient implementations for big data

% par 2
% manifold learning solves the wrong problem, and doesn't have OOS, or scale (except PCA)

% par 3
% sparse methods solve an overly constrained problem, don't scale, often only work for 2-class problems (don't generalize).

% no method for high-d data that addresses also desiderata satisfactorily

% par 4: LOL 
% uses mean & variance
% can be extend
% theory for large p
% implementation for large p

% par 5 resolution
% empirical demonstrations
%  easy to run, provably better, empirically better, just as fast.


% @jlp: add citations, make comments if you don't know which one they should be.

Supervised learning---the art and science of estimating statistical relationships using labeled training data---is a crucial tool in scientific discovery.  Supervised learning has been enabled a wide variety of basic and applied findings, ranging from discovering biomarkers in omics data \cite{Vogelstein2014a} to object recognition from images \cite{Krizhevsky2012}.
% , and includes celebrated methods such as support vector machines, random forests, and deep networks \cite{Hastie2004}.
A special case is classification; a classifier predicts the ``class'' of a novel observation via partitioning the space of observations (for example, predicting male  or female from MRI scans). One of the most foundational and important approaches to classification is called ``Fisher's Linear Discriminant Analysis'' (\Lda) \cite{Fisher1925a}. 
\Lda~has a number of highly desirable properties for a reference classifier.  
First, it is built based on very simple geometric reasoning: when the data are Gaussian, all the information is in the means and variances, so the optimal classifier uses both the means and the variances.  
Second, due to its simplicity, \Lda~can be applied to multiclass problems, and can easily be extended to other problems.
Third, theorems guarantee that when sample size is large, and dimensionality is small, \Lda~converges to the optimal classifier under the Gaussian assumption.
And finally, again, because it is so simple, algorithms for implementing it are highly efficient.


%which is based on very simple geometric reasoning.  In particular, it is guaranteed to find the optimal linear classifier under the assumption that the data are Gaussian. Moreover, it can be applied no matter how many classes are present. Finally, its solution is analytic, therefore obviating costly numerical optimizations and enabling scalability. This solid theoretical foundation, easy interpretation, computational efficiency, and widespread utility perhaps contribute to \Lda's longevity, as it remains one of {the} few reference classifiers nearly a century later.

Modern scientific datasets, however, present challenges for classification that were not addressed in Fisher's era.
Specifically, the dimensionality of datasets is quickly ballooning.
Currently, across many scientific disciplines, the raw data might consist of hundreds of millions  of features or dimensions; for example, an entire genome or connectome.  While the dimensionality of these data have increased precipitously, the sample sizes have not witnessed a concomitant increase.
This ``large p, small n'' problem is disastrous for many classical statistical approaches because they were designed with ``small p, large n'' in mind. \Lda~in particular estimates a hyperplane in $p-1$ dimensions when the data are $p$ dimensional.  But there are an infinite number of $p-1$ dimensional hyperplanes that fit the data exactly when $p>n$.  To visualize this, imagine fitting a line to a single point, or a plane to two points.  In each case, one can choose any rotation, and still fit the data perfectly.  
Therefore, without further constraints, algorithms will ``over-fit'', effectively randomly choose one of the many equally good fits, typically yielding a line or plane that is far from the optimal one.
%Doing so, however, requires the algorithm to make a choice that is independent of the existing data, and therefore unlikely to be correct.  
%This problem is called ``over-fitting''.
%To address this challenge, ``supervised manifold learning''
Supervised manifold learning is field devoted to combating this 
%Many statistical and machine learning approaches have been developed
%% over the last 100 years
%to combat 
this over-fitting issue  by searching for a small number of dimensions that maximize predictive accuracy.
Two complementary strategies have been pursued.

Perhaps the most prominent strategy is to use principal components analysis (\Pca) \cite{PCA} to ``pre-process'' the data, that is, to reduce the dimensionality of the data prior to a subsequent classification using the much lower dimensional data.  While hugely successful, \Pca~is a wholly \emph{unsupervised} dimensionality reduction technique, meaning that \Pca~does not utilize the class labels while learning the low dimensional representation.  This results in dimensions that have no statistical guarantee of being close to the best ones, and in fact, as we show below, with high probability will be completely wrong.  Other unsupervised nonlinear dimensionality reduction techniques,  called manifold learning, are ill-equipped to address this problem because they typically only learn a low-dimensional representation for a set of points; thus, they are unable to be applied to  new test data.   Moreover, they often require costly numerical methods that do not scale, and lack theoretical justification in this setting. 



%A closely related approach for supervised learning is called ``reduced-rank \Lda'' (\Rrlda).  As we will show below, while \Pca~does not optimally utilize the class labels in its low-dimensional representation, \Rrlda~actually \emph{discards} this information.


A different strategy that focuses on utilizing the class label information is called  ``sparsity''. Sparse methods  find a small subset of the original features to use for subsequent inference \cite{sparsity}. There are many such approaches (often called ``feature selection'' or ``feature screening''); the advantage of these approaches is that the result is sometimes more easily interpretable.  The disadvantage, however, is that exactly solving the problem is computationally intractable, requiring time that increases exponentially in the number of features.  Various approximations enable efficient algorithms that have provable guarantees under certain limiting assumptions.  \Lasso~is a particularly popular algorithm that has these properties. Unfortunately, \Lasso~cannot run on millions of dimensions,  it has a hyper-parameter that requires careful tuning, and  often produces spurious answers even when the unrealistically strict assumptions are met \cite{lasso-fail}.
A more recent approach is called ``regularized optimal affine discriminant'' (\Road). \Road~finds the optimal sparse dimensionality reduction under certain Gaussian assumptions.  \Road, however, can only be applied in two-class settings, and requires solving a computationally costly numerical optimization problem, and thus does not scale to large dimensionality.


There is therefore a gap: the field lacks the high-dimensional analog of Fisher's \Lda, that is, a method based on simple geometric intuition, that can be applied to multiclass and more general problems, with theorems that guarantee good performance for arbitrarily large dimensionality, and an efficient implementation that scales to hundreds of millions of features. 
%
%analysts desire a method that has a simple geometric intuition even in high-dimensions and can be applied when there are any number of classes, but also has theoretical guarantees, without making addition assumptions about the data and requiring carefully tuning hyperparameters, and can scale to arbitrarily high-dimensional data in a computationally efficient fashion.
%
To address these concerns,  we developed ``Linear Optimal Low-rank'' embedding (\Lol). The key intuition behind \Lol~is that we can jointly utilize both  directions that are informative with regard to the classification task (the means) and the directions that maximize the variance (the covariance), much like \Lda.  But by virtue of utilizing random matrix theory, we are able to prove that \Lol~finds a better low dimensional representation than PCA and other linear methods under the Gaussian assumption. This is true regardless of the dimensionality of the features, the number of samples, and the number of dimensions in which we project.  
%
%We demonstrate \Lol~outperforms---or at least performs no worse than---several reference benchmarks along every  consideration mentioned above.
%This includes proving that \Lol~statistically dominates \Pca~and related linear methods under Gaussian assumption.
% the improvement over several standards, along both statistical considerations (bias and variance) for the simple Gaussian setting mentioned above.
%Moreover, we provide an  implementation that improves on previous computational capabilities in terms of stability and scalability.
Numerical experiments quantitatively demonstrate the improvement of \Lol~over reference methods on a wide range of simulated experiments that both satisfy our theoretical assumptions, and go beyond them. 
%
%By virtue of \Lol~being built upon geometric intuition, it is easy to extend it along various dimensions.  Additional numerical experiments demonstrate that practice matches  intuition, in particular, generalizations of \Lol~can easily be constructed to match different geometric assumptions of the data.  Indeed, even when data are sampled from distributions outside our theory and intuition,  \Lol~achieves smaller error than its competitors. 
In fact, one can extend \Lol~outside of classification problems to  regression and hypothesis testing and obtain qualitatively similar results.  Computationally, \Lol~is always numerically stable, and requires no more computational space, and smaller computational time, than other linear methods.  Moreover, we provide a highly scalable implementation that efficiently runs on datasets with hundreds of millions of features comprising hundreds of gigaabytes.
We then tested \Lol~against a set of standard methods on  several benchmark datasets:  there was not a single method that outperformed \Lol~in any dimension.
We provide open source implementations of \Lol~in both MATLAB and R to support further applications and extensions.  
%In addition to the above quantitative properties, \Lol~is intuitive, only has a single hyper-parameter which is exceedingly easy to tune, and we provide free and open source implementations, in MATLAB and R, as well as pre-configured environments that will enable anybody to run \Lol~on commodity machines, including laptops, workstations, and cloud instances.  \Lol~is not likely to necessarily be the best algorithm on any given new high-dimensional challenge problem.  Rather, we suspect \Lol~might be useful as one of the first algorithms to try on big and wide data.  Moreover, 


This work therefore makes four complementary challenges to the machine learning literature.  
First, we provide geometric intuition for how supervised manifold learning methods can work, suggestive of potential algorithms.  
Second, we provide one example algorithm that satisfies the desiderata, specifically that is simple enough to admit theoretical investigations and efficient implementations.  
Third, we provide a theoretical framework for evaluating supervised manifold learning for classification that does not depend on the subsequent classifier.
And fourth, we provide a scalable implementation that can run on half-terabyte datasets on single machines in a few minutes. 
The arguments and methodology developed herein provide a comprehensive framework for developing big supervised manifold learning algorithms to tackle other data science challenges.




%\section*{Results}
% get 192 lines, have 240, need to remove 50.

\subsection*{Supervised Manifold Learning}
%Linear Optimal Low-rank}
% An Illustrative Real Data Example of Supervised Linear Manifold Learning}



% \begin{wrapfigure}{R}{0.7\textwidth}%[h!]
\begin{figure}
\centering % l b r t
%  trim={<left> <lower> <right> <upper>}
\includegraphics{../Figs/mnist2.pdf} %[height=2in],trim=1cm 0cm 0cm 1.0cm,clip=true
\caption{Schematic illustrating Linear Optimal Low-rank Embedding as a supervised manifold learning technique. 
% Illustrating four different classifiers---\sct{Lasso}~(top), \Lda~(second),  \Pca~(third), and \Lol~(bottom)---for embedding images of the digits 3, 7, and 8 (from MNIST), each of which is 28 $\times$ 28 = 784 dimensional.
\textbf{(A)} 300 training samples of the numbers 3, 7, and 8 from the MNIST dataset (100 samples per digit);  each sample is a 28 $\times$ 28 = 784 dimensional image (boundary colors are  for visualization purposes).
\textbf{(B)} The first four projection matrices learned by \Lol.  Each is a linear combination of the sample images.
% Note that \sct{Lasso}~is sparse and supervised, \Pca which is dense and unsupervised, and both \Lda~and \Lol~are dense and supervised.
\textbf{(C)} Projecting 500 new (test) samples into the top two learned dimensions;
 % using each approach.
 digits color coded as in (A). \Lol~embedded data form three distinct clusters.
\textbf{(D)}  Use the low-dimensional data to learn a classifier.  The estimated  distributions for 3 and 8 of test samples after
projecting data into two dimensions and using LDA to classify demonstrate that 3 and 8 are easily separable by linear methods after \Lol~projections (color of line indicates the digit).
% The vertical line shows the classification threshold.
The filled area is the estimated error rate; the goal of any classification algorithm is to minimize that area. \Lol~is performing well on this high-dimensional real data example.
% Clearly, \Lol~exhibits the best separation after embedding, which results in the best classification performance.
}
\label{f:mnist}
% \end{wrapfigure}
\end{figure}

A general strategy for supervised manifold learning is schematiized in Figure \ref{f:mnist}.
Step \textbf{(A)},  obtain or select $n$ training samples of high-dimensional data.  For concreteness, we utilize one of the most popular benchmark datasets, the MNIST dataset \cite{mnist}.  This dataset consists of $n=60$,$000$ examples of images of the digits $0$ through $9$.  Each such image is represented by a $28\times28$ matrix, which means that the observed  dimensionality of the data is $p=28^2=784$.  Because we are motivated by the $n \ll p$ scenario, we subsample the data to select $n=300$ examples of the numbers $3$, $7$, and $8$ ($100$ of each).
%
Step \textbf{(B)},  learn a ``projection'' that maps the high-dimensional data to a low dimension representation.  One can either ignore the class label data, that is, ignore which images correspond to which digits (as PCA and most manifold learning techniques do), or try to use them (as sparse methods do).  \Lol~uses the class labels to learn projections that are linear combinations of the original data samples (panel B shows the first four projections that \Lol~learns, each of which looks like a combination of the original images).  
%
Step \textbf{(C)}, use the learned projections to map the high-dimensional data into the lower dimensional space. This step requires having learned a projection that can be applied to new (test) data samples for which we do not know the true class labels.  Nonlinear manifold learning methods typically are unable to be applied in this way (see for example, \citet{oos}).  \Lol, however, can project new samples in such a way as to separate the data into classes (in panel C, two-dimensional points represent the original images, color coded by their digit label, are well separated).
Finally, step \textbf{(D)}, using the low-dimensional representation of the data, learn a classifier.  A good classifier correctly identifies as many points as possible with the correct label.  Panel D shows that when using \Lda~on the low-dimensional data from \Lol, the data points are mostly linearly separable.  
Specifically, the two curves correspond to the distribution of $3$'s (in blue) and $8$'s (in green) after applying \Lda~to the data projected using \Lol, and the area of overlap (in black) corresponds to the fraction of errors, so smaller is better. 



%(C) use that projection to project the data into the lower dimensional space,
%(D) classify the projected  samples using a standard classifier.
% We consider  four different linear dimensionality reduction methods---\sct{Lasso}, \Rrlda, \Pca, and \Lol---each of which we compose with a linear classifier to form high-dimensional classifiers. \footnote{Although the original \sct{Lasso}~does not first find a low dimensional representation and then classify, more recent generalizations do (for exampel, \citet{Zou2006a}), and then improve on \sct{Lasso}'s theoretical and empirical properties, so will always mean Adaptive \sct{Lasso} here.}

%To illustrate how \Lol~works,  first consider one of the most popular benchmark datasets, the MNIST dataset \cite{mnist}.  This dataset consists of $n=60$,$000$ examples of images of the digits 0 through 9.  Each such image is represented by a 28$\times$28 matrix, which means that the observed (or ambient) dimensionality of the data is $p=28^2=784$.  Because we are motivated by the $n \ll p$ scenario, we subsample the data to select $n=300$ examples of the numbers $3$, $7$, and $8$ ($100$ of each).
%We then apply
% all four approaches
%\Lol~to this subsample of the MNIST dataset, learning a projection,  projecting the data, and learning a classifier using the projected data.
%
% \sct{Lasso}, by virtue of being a sparse method, finds the pixels that most discriminate the 3 classes (column B).  The resulting embeddings mostly live along the boundaries (column C), because these images are close to binary, and therefore, images either have or do not have a particular pixel. Indeed, although the images themselves are nearly sparse (over 80\% of the pixels in the dataset have intensity $\leq 0.05$),  a low-dimensional discriminant boundary does not seem to be sparse, as evidenced by substantial overlap between the posteriors for 3 and 8 (panel D).
% %
% \Rrlda~does poorly, essentially failing to discriminate these digits at all. This is because the low dimensional embedding implicit in \Rrlda~subtracts out the difference of the means, only utilizing the dimensions that maximize variance (see Appendix \ref{sec:pca} for details). In this setting, it seems the directions that maximally separate the classes are not well aligned with the directions of maximal variance.
% On the other hand, \Pca~does reasonably well, as it implictly includes the discriminant directions merely by not subtracting them out, but does not utilize them particularly well.


% @jovo:something in here about how LOL learns the embedding
%\Lol~is our proposed supervised linear manifold learning method.  The projection matrices it learns are linear combinations of the original digits.
% This is not surprising, as they are all linear combinations of the training examples.
% The resulting embeddings however, look quite different.
%Upon projecting new samples into the low dimensional space, they are clearly separable into thee different classes, even if only using the first two dimensions.
%When training a linear classifier on the data, it is very easy to learn a linear  discriminant boundary that performs well.

 % The three different classes are very clearly separated by even the first two dimensions.  The result of these embeddings yields classifiers whose performance is obvious from looking at the embeddings: \Lol~achieves significantly smaller error than the other  approaches.
% This numerical experiment justifies the use of \Lol, we next build a geometric intuition for \Lol~in simple simulated examples, to better illustrate when we can expect \Lol~to outperform other methods, and perhaps more importantly, when we expect  \Lol~to fail.

% \Lol~is our proposed supervised linear manifold learning method.  The projection matrices it learns look qualitatively much like those of \Rrlda~and \Pca. This is not surprising, as they are all linear combinations of the training examples.
% The resulting embeddings however, look quite different.
%  The three different classes are very clearly separated by even the first two dimensions.  The result of these embeddings yields classifiers whose performance is obvious from looking at the embeddings: \Lol~achieves significantly smaller error than the other  approaches.
% This numerical experiment justifies the use of \Lol, we next build a geometric intuition for \Lol~in simple simulated examples, to better illustrate when we can expect \Lol~to outperform other methods, and perhaps more importantly, when we expect  \Lol~to fail.

% \clearpage
\subsection*{Linear Gaussian Intuition}

% The above real data example suggests the geometric intuition for when \Lol~outperforms its sparse and unsupervised counterparts.
To build intuition about when \Lol~performs well, and when it does not,
% further investigate, both theoretically and numerically,
we consider the simplest high-dimensional classification setting. 
We observe $n$ samples $(\bx_i,y_i)$, where $\bx_i$ are $p$ dimensional feature vectors, and $y_i$ is the binary class label, that is $y_i$ is either $0$ or $1$.
We assume that both classes are distributed according to a multivariate Gaussian distribution, and the two classes have the same covariance matrix $\bSig$ and data from either class is equally likely, so that the only difference between the classes is their means, $\bmu^1$ and $\bmu^2$.
 % (we call this the Linear Discriminant Analysis (\Lda) model; see Methods for details).
%
% To motivate \Lol, and the following simulations, lets c
% Consider what the optimal projection would be in this scenario.
The optimal low-dimensional projection is analytically available in this scenario---commonly referred to as Fisher's Linear Discriminant Analysis (\Lda)---it is the dot product of the difference of means and the inverse covariance matrix, $(\bmu^1-\bmu^2)\T \bSig^{-1}$ \cite{Bickel2004a} (see Methods for derivation).
When the distribution of the data are unavailable, as in all real data problems, machine learning methods estimate the parameters instead.  
Unfortunately, when $n<p$, the estimated covariance matrix will not be invertible, so analysts must use something else.
For example, \Pca~utilizes the pooled sample mean, $\mh{\bmu}= \frac{1}{n} \sum_{i=1}^n \bx_i$ and the pooled sample covariance matrix,  $\mh{\bSig}$ with entries $\mh{\bSig}_{kl}=\frac{1}{n} \sum_{i=1}^n (x_{ik} - \mu_k) (x_{il} - \mu_l)$.  
%utilizes only the covariance  of the data $\bSig$, and ignores the difference between the means $\bdel$.
 % (because \Pca~for classification operates on the class-conditionally centered data, see Methods for details).
The \Pca~projection is the top $d$ eigenvectors of the pooled sample covariance matrix, thus completely ignoring the class labels.

%, thus ignoring the sample class conditional means, that is, the means for each class, $\mh{\bmu}_j = \frac{1}{n_j} \sum_{i : y_i = j} \bx_i$.
%would then project the data on the top $d$ eigenvectors of the  sample covariance matrix.

\textbf{The key insight of our work is that we can combine the class means and the covariance matrix in a simple fashion, rather than just the covariance matrix, to find a low dimensional projection.}
This is motivated  by Fisher's \Lda, which utilizes both means and variance, and should therefore improve performance over \Pca~which only utilizes the variances. 
%Na\"ively, this should typically improve performance, because in this stylized scenario, both are important.
% @jovo: for discussion
%The \sct{F2M} literature has a similar insight, but a different construction that requires the dimensionality to be smaller than the sample size \cite{Li1991a, Tishby1999a, Globerson2003a, Cook2005a,Fukumizu2004a}.
More specifically, for a two-class problem, \Lol~first computes the sample mean of each class,  $\mh{\bmu}^j = \frac{1}{n_j} \sum_{i : y_i = j} \bx_i$, where $n_j$ is the number of samples in class $j$.  Second, \Lol~estimates the difference between  means,  $\mh{\bdel}=\mh{\bmu}^1 - \mh{\bmu}^2$.  
Third, \Lol~computes the class-conditional covariance matrix, 
$\mt{\bSig}$ with entries $\mt{\bSig}_{kl}= \sum_{j=1}^J \frac{1}{n} \sum_{i : y_i = j} (x_{ik} - \mu^j_{k}) bx_{il} - \mu^j_{l})$.  In other words, \Lol~centers each data point with respect to its own classes mean, rather than the overall pooled mean, and then computes the covariances.    
Fourth, \Lol~computes the eigenvectors of this class-conditionally centered covariance. 
And finally, \Lol~simply concatenates the difference of the means with the top $d-1$ eigenvectors of $\mt{\bSig}$. 
Note that the sample class-conditional covariance matrix estimates the population covariance, $\bSig$, whereas the sample pooled covariance matrix is distorted by the difference of the class means.  
All together, \Lol~estimates both the difference of the means and the covariance matrix, just like Fisher's \Lda.  

%simply concatenates the difference of the means with the top $d$ eigenvectors of the class-conditionally centered covariance matrix.
%Whereas \Pca~subtracts the overall mean prior to computing an eigendecomposition of the covariance matrix, 
%\Lol~subtracts the mean \emph{from each class}, therefore obtaining a more accurate estimate of the shared covariance matrix.
%By also including the difference of the means, \Lol~captures both the dimensions of maximal variance and the dimensions that characterize the difference between the classes, that is, the supervised information.
%   
%computes the eigendecomposition of the centered covariance (ignoring the classes), \Lol~utilizes the eig
%This is equivalent to first projecting onto the difference of the means vector, and then projecting the residuals onto the first $d$ principle components.
%Thus, it requires almost no additional computational time or complexity over that of \Pca, rather, merely estimates the difference of the means.
%In this sense, \Lol~can be thought of as a very simple  ``supervised \Pca''.



\begin{figure}[h!]%{R}{0.7\textwidth}%[h!]
% \begin{SCfigure}
\centering
\includegraphics[width=0.8\linewidth,trim=0in 0in 1.5in 0in,clip=true]{../Figs/cigars_est.pdf}%l b r t
\caption{
\Lol~achieves near optimal performance for a wide variety of Gaussian distributions.
Each point is sampled from a multivariate Gaussian;
the three columns correspond to different simulation parameters (see Methods for details).
In each of 3 simulations, we sample $n=100$ points in $p=1000$ dimensions, so $n \ll p$. 
And for each approach, we embed into the top 20 dimensions. Note that we use the sample estimates, rather than the true population values of the parameters.  The five columns show (in decreasing order):
\textbf{Row 1}: A scatter plot of the first two dimensions of the sampled points, with class 0 and 1 as black and gray dots, respectively.
\textbf{Row 2} - \textbf{Row 5}: the posteriors after projecting using different manifold learning techniques, including 
\textbf{Row 2} \Pca.
\textbf{Row 3} \Pca', a method that projects onto the top $d$ eigenvectors of  sample class-conditional covariance \cite{RRLDA},
\textbf{Row 4} \Road, a sparse method designed specifically for this model \cite{Fan2012a}.
\textbf{Row 5} \Lol, our  proposed method.
\textbf{Row6} the Bayes optimal classifier.
%
\textbf{(A)} The mean difference vector is aligned with the direction of maximal variance, maxing it ideal for both \Pca~or \Rrlda~to discover the discriminant dimension and a sparse solution. In this setting, the results are  similar for all methods, and essentially optimal.
\textbf{(B)} The mean difference vector is orthogonal to the direction of maximal variance, making \Pca~perform worse, \Rrlda~is at chance, but sparse methods and \Lol~can still recover the correct dimensions, achieving nearly optimal performance.
\textbf{(C)} Same as B, but the data are rotated, in this case, only \Lol~performs well.
Note that \Lol~is closest to Bayes optimal in all three settings.
}
\label{f:cigars}
\end{figure}
% \vspace{-30pt}


Figure \ref{f:cigars} shows three different examples of data sampled from the Gaussian model to geometrically illustrate this intuition (see Methods for details).
In each, the top row shows, for  $n=100$ training samples,  the first two dimensions  of a $p=1000$ dimensional space, so $n \ll p$.  
The next four rows each show the distribution of test data after using \Lda~on the low-dimensional representation (solid line for class $0$ and dashed line for class $1$, and the vertical line is the estimated boundary between the two classes). 

Figure \ref{f:cigars}{\color{magenta}A} shows an example we call  ``stacked cigars''.
In this example  all  dimensions are uncorrelated with one another.
Moreover, the difference between the means and direction of maximum variance are both large along the same dimension.
This is an idealized setting for \Pca, because \Pca~finds the direction of maximal variance, which happens to correspond to the direction of maximal separation of the classes.
We also compare this to a method we refer to as \Pca'. which uses the top $d$ eigenvectors of the sample class-conditional covariance matrix, $\mt{\bSig}$. 
As it turns out, composing this projection with \Lda~is equivalent to a method called ``Reduced Rank \Lda'' \cite{RRLDA} (see Appendix \ref{sec:RRLDA} for proof). 
\Pca' performs well here too, for the same reason that \Pca~does.
Because all dimensions are uncorrelated, and one dimension contains most of the information discriminating between the two classes, this is a good scenario for sparse methods.
Indeed,  \Road, a sparse classifier designed for precisely this scenario,  does an excellent job finding the most useful dimensions \cite{ROAD}.
% Again, in this case, because the directions of maximal variance and discrimination are the same, \Rrlda~performs well here too. 
\Lol, using both the difference of means and the directions of maximal variance also does well.  
To calibrate all of these methods, we also show the performance of the optimal classifier.


Figure \ref{f:cigars}{\color{magenta}B} shows an example which is worse for  \Pca.
In particular, the variance is getting larger for subsequent dimensions, $\sigma_1 < \sigma_2 < \cdots < \sigma_p$, while the magnitudes of the difference between the means are decreasing with dimension, $\delta_1 > \delta_2 < \cdots > \delta_p$.
Because \Pca~operates on the pooled sample covariance matrix, the dimensions with the maximum difference are included in the estimate, and therefore, \Pca~finds some of them, while also finding some of the dimensions of maximum variance, therefore performing fairly well.
\Pca', however, by virtue of subtracting out the difference of the means, is now completely at chance performance.
\Road~is not hampered by this problem, it is also able to find the directions of maximal discrimination, rather than those of maximal variance.
Again, \Lol, by using both the means and the covariance, does extremely well.


Figure \ref{f:cigars}{\color{magenta}C}  is exactly the same as B, except the data have been randomly rotated in all 1000 dimensions.  This means that none of the original coordinates have much information, rather, linear combinations of them do.
This is evidenced by observing the scatter plot, which shows that the first two dimensions  fail to disambiguate the two classes.
\Pca, being rotationally invariant, performs approximately as well in this scenario as in B.
\Pca' is not helped by this random rotation, so still performs at chance levels.
Because there is no small number of features that separate the data well,  \Road~fails.
 \Lol~performs nearly as well here as it does in the other examples.
 
 Collectively, these three examples demonstrate when, based purely on geometric intuition, that \Lol~performs as expected in a variety of Gaussian settings.


\subsection*{Statistical Theory}

The above numerical experiments provide the intuition to guide our theoretical developments. 
% We specifically compare \Lol~to other projection methods, in particular, evaluating when appending the difference of the means yields improved performance to any 
\begin{thm} \label{t:LDA}
\Lol~is always better than or equal to \Pca' (and nearly any other linear projection) under the Gaussian model, and better than or equal to \Pca~with relatively weak conditions.  This is true for all possible observed dimensionality of the data, and number of dimensions into which we embed, for sufficiently large sample sizes. Moreover, under relatively weak assumptions, these conditions hold almost surely as the number of dimensions increases.
\end{thm}
A formal statement of the theorem and proof are provided in Appendix \ref{sec:theory}.  The condition for \Lol~to be better than \Pca~is essentially that the $d^{th}$ eigenvector of the pooled sample covariance matrix has less information about classification than the difference of the means vector.  
%
The implication of the above theorem is that it is better to incorporate the mean difference vector into the projection matrix than not.  The \emph{degree} of improvement is a function of the embedding dimension $d$, the  dimensionality of the feature set $p$, and the parameters (see Methods for details and proof), but the \emph{existence} of an improvement, or at least no worse performance, is independent of those factors.  It is worth specifying exactly what ``better'' means in this context.  
In this context, it is desirable to have a notion of better that  is agnostic to the subsequent classifier, that is, a metric that  quantifies how good an embedding in, no matter which classifier we will use.  We utilized Chernoff Information to calculate the distance between the distributions after embedding. Chernoff information  is fundamentally related to the expected classification error;  specifically, it is the exponential convergence rate for the Bayes error. 

%Note that better here includes both bias and variance.  In the population version of these two methods, there is no variance, so only the bias is important.  Moreover, we know the conditions under which \Lol~is strictly better than \Pca, they are formally stated in Assumption \ref{a:2} (in Methods).  Informally, \Lol~is better than \Pca~whenever the angle between the mean vector and the top $d$ principal components is large enough. Under reasonable assumptions, in the appendix, we show that this happens $84\%$ of the time.
% Note that Theorem \ref{t:LDA} is true regardless of the truncation dimension and ambient (observed) dimension.



\subsection*{Numerical Experiments Extending Our Theoretical Results}

% In the above numerical and theoretical investigations, we fixed $d$, the number of dimensions to embed into.  Much unsupervised manifold learning theory typically focuses on finding the ``true'' intrinsic dimensionality of the data.   The analogous question for supervised manifold learning would be to find the true intrinsic dimensionality of the discriminant boundary.  However, in real data problems, typically, their is no perfect low dimensional representation because of noise.
% , rather, the more data we obtain, the higher-dimensional discriminant boundary we can estimate, and the close to Bayes optimal we can perform.

% Thus, in all the following simulations, the true ambient dimensionality of the data is equal to the dimensionality of the optimal discriminant boundary (given infinite data).  In other words, there does not exist a discriminant space that is lower dimensional than the ambient space, so we cannot find the ``intrinsic dimension'' of the data or the discriminant boundary.  Rather, we face a trade-off: keeping more dimensions reduces bias, but increases variance.  The optimal bias/variance trade-off depends on the distribution of the data, as well as the sample size \cite{Trunk1979a}.

% We formalize this notion for the \Lda model and proof the following:
% \begin{thm} \label{t:n}
% Under the \Lda~ model, estimated \Lol~is better than \Pca.
% \end{thm}
% Note that the degree of improvement is a function of the number of samples $n$, in addition to the embedding dimension $d$, the ambient dimensionality $p$, and the parameters (see Methods for details and proof).

%In the previous section, we proved that \Lol~is often better than \Pca, and never worse under the \Lda~model, regardless of observed dimensionality, embedded dimensionality, and parameters, assuming the parameters are given.  In this section, 
Here we numerically investigate the performance of \Lol~versus \Pca~and other methods empirically using simulations, both under the model assumptions for which our theorems hold, as well as more general assumptions for which we currently lack theory.
For each of four different scenarios, we sample 
%what happens when the parameters must be estimated from training data, and in particular, from 
$n=100$ training samples each with  $p=100$ features; therefore, FIsher's \Lda~cannot solve the problem because there are infinitely many ways to overfit.  
For each setting we evaluate the misclassification rate on held out data for all number of dimensions to embed into.  The comparison algorithms are \Pca, \Pca', and two sparse methods \Lasso~\cite{Zou2006a}, and \Road~\cite{Fan2012a}. \Road~is a sparse approach specifically designed for  Gaussian data, but only works for two-class problems, whereas \Lasso~was designed for finding sparse dimensions and can be applied to any number of classes.  

%These results strengthen our theoretical results by providing exact magnitudes of performance, rather than merely better than guarantees, in addition to operating in a finite sample setting, more akin to real data scenarios.

\para{Theoretical model} We begin by investigating two scenarios that satisfy the \Lda~model assumptions required by our proofs. First, consider  the rotated trunk example from Figure \ref{f:cigars}{\color{magenta}C} as well as a ``Toeplitz'' example, as depicted in Figures \ref{f:properties}{\color{magenta}A} and {\color{magenta}B}, respectively.  In both scenarios, for all dimensions, \Lol~achieves a lower error rate than either of its competitors, often dramatically so.


\para{Multiple Classes} \Lol~can trivially be extended to $>2$ class situations, unlike \Road.  In brief, \Lol~computes the mean of each class, and then selecets one mean to be the reference, and computes the difference between all the other means and the reference one.  Under the linearity assumptiosn, this approach does not lose any information relative to computing the distance between all pairs of means  (see Methods for details).  We generated data again from the same Trunk example, but added a third class whose mean is the zero vector.  We used \Lasso~as the sparse method approach, which utterly fails in this near sparse setting.  As before, \Lol~outperforms the other methods for all dimensions. 



\begin{figure}[h!]
\centering
\includegraphics[width=0.8\linewidth]{../Figs/plot_sims}
\caption{
Seven simulations demonstrating \Lol~achieves superior finite sample performance over competitors both in settings for which we have asymptotic theoretical guarantees, and those for which we do not.
% that even when the true discriminant boundary is high-dimensional, \Lol~can find a low-dimensional projection that wins the bias-variance trade-off against competing methods.
For the first three, the top panels depict the means (top), the shared covariance matrix (middle).  For the next three, the top panels depict a 2D scatter plot (left), mean and level set of one standard deviation of covariance matrix (right).  For all seven simulations, the bottom panel shows misclassification rate as a function of the number of embedded dimensions, for several different classifiers.  The simulations settings are as follows:
\textbf{(A)} Rotated Trunk: same as Figure \ref{f:cigars}C.
\textbf{(B)} Toeplitz: another setting where mean difference is not well correlated with any eigenvector, and no ambient coordinate is particularly useful on its own.
\textbf{(C)} 3 Class variant of the rotated Trunk example to demonstrate that \Lol~naturally adapts, and excels in, multi-class problems.
\textbf{(D)} Fat Tails: a common phenomenon in real data that is more general than our theory supports.
\textbf{(E)} QDA: \Qoq, a variant of \Lol~when each class has a unique covariance, outperforms \Lol, as expected, when the true discriminant boundary is a quadratic, rather than linear, function.
\textbf{(F)} Outliers: adding high-dimensional outliers degrades performance of standard eigensolvers, but those can easily be replaced in \Lol~for a robust variants (called \Lrl).
\textbf{(G)} XOR: a high-dimensional stochastic generalization of XOR, demonstrating that \Qoq~works even in scenarios that are quite distinct from the original motivating problems.
In all 7 cases, \Lol, or the appropriate generalization thereof, outperforms unsupervised or sparse methods.  Moreover, the optimal embedding dimension is never the true discriminant dimension, but rather, a smaller number jointly determined by parameter settings and sample size.
}
\label{f:properties}
\end{figure}




\para{Fat Tails} Figure \ref{f:properties}{\color{magenta}D} shows a sparse example with ``fat tails'' to mirror real data settings better. More specifically, each class is the sum of multiple Gaussians, with the same mean, but different covariances (see Methods for details). The qualitative results are consistent with those of the previous numerical experiments, even though  we have no theoretical guarantees here. More specifically, \Lol~outperforms all other methods for all dimensions.


% @jovo: what happened to these ones?
\para{QDA} Sometimes, it makes more sense to model each class as having a unique covariance matrix, rather than a shared covariance matrix.  Assuming everything is Gaussian, the optimal classifier in this scenario is called  Quadratic Discriminant Analysis (QDA) \cite{Hastie2004}.  Intuitively then, we can modify \Lol~to compute the eigenvectors separately for each class, and concatenate them (sorting them according to their singular values).  Moreover, rather than classifying the projected data with \Lda, we can then classify the projected data with QDA.  Indeed, simulating data according to such a model (Figure \ref{f:properties}E), \Lol~performs slightly better than chance, regardless of the number of dimensions we use to project, whereas QOQ (for Quadratic Optimal QDA) performs significantly better regardless of how many dimensions it keeps. This demonstrates a  straightforward generalization of \Lol, available to us because of the simplicity and intuitiveness of \Lol.

\para{Outliers}  Outliers persist in many real data sets.  Finding outliers, especially in high-dimensional data, is both tedious and difficult.  Therefore, it is often advantageous to have estimators that are robust to certain kinds of outliers \cite{Huber1981a,Rousseeuw1999a,Ferrari2010a}.  \Pca~and eigenvector computation are particularly sensitive to outliers \cite{Candes2009b}.  Because \Lol~is so simple and modular, we can replace typical eigenvector computation with a robust variant thereof, such as the geometric median subspace embedding \cite{Zhang2014a}.  Figure \ref{f:properties}F shows an example where we generated  $n/2$ training samples according to the simple \Lda~model, but then added another $n/2$ training samples from a noise model.  \Lrl~(Linear Robust Low-Rank), performs better than \Lol~regardless of the number of dimensions we keep. This simulation setting further demonstrates the flexibility of the \Lol~framework to be extensible to other, more complicated scenarios.

\para{XOR}  XOR is perhaps the simplest nonlinear problem, the problem that led to the demise of the perceptron, prior to its resurgence after the development of multi-layer perceptrons \cite{Bishop2006a}.  Thus, in our opinion, it is warranted to check whether any new classification method can perform well in this scenario.  The classical (two-dimensional) XOR problem is quite simple: the output of a classifier is zero if both inputs are the same (00 or 11), and the output is one if the inputs differ (01 or 10).  Figure \ref{f:properties}G shows a high dimensional and stochastic variant of XOR (see Methods for details).  This simulation was designed such that standard classifiers, such as support vector machines and random forests, achieve chance levels (not shown).  \Lol, performs moderately better than chance, and QOQ performs significantly better than chance, regardless of the chosen dimensionality.  This demonstrates that our classifiers developed herein, though quite simple and intuition, can perform well even in settings where the data are badly modeled by our underlying assumptions.  This mirrors previous findings where the so-called ``idiots's Bayes'' classifier outperforms more sophisticated classifiers \cite{Bickel2004a}.  In fact, we think of our work as finding intermediate points between idiot's Bayes (or na\"ive Bayes) and \Fld, by enabling degrees of regularization by changing the dimensionality used.


\subsection*{Computational Efficiency}

When the dimensionality $p$ or sample size $n$ is large, the main bottleneck or concern is sometimes merely the ability to run anything on the data, rather than its predictive accuracy. Fortunately, \Lol~not only exhibits improved performance over standard methods, it also has several algorithmic and implementation level designs that make it computationally efficient.
First, \Lol~admits a closed form solution, enabling it to leverage highly optimized linear algebraic routines rather than the costly iterative programming techniques currently required for sparse or dictionary learning type problems.   This means that when the dataset is small enough, it will run very quickly.  
Specifically, the implementation is linear in both sample size and dimensionality (Figure \ref{f:speed}{\color{magenta}A}; light green line).
When the data size is larger than the memory of computer, however, other techniques are required.  
Second, \Lol~is designed to be parallelizable. %, meaning that it can be implemented to utilize modern hardware architectures.  
Typical parallelization strategies distribute work across compute nodes in a large cluster.  However, doing so comes with a severe communication cost between the nodes.  Instead, we therefore leverage recent advances in computer architecture, including multicore processors and solid-state drives.  
Building on FlashX \cite{FlashGraph, FlashEigen, FlashMatrix}, we developed  extremely efficient \Lol~implementations with an R interface for ease of use, including both  an in memory implementation when the data are small enough to be kept in RAM, and a semi-external memory implementation for larger data.
% @Jovo: fix this later
Our implementations enables us to run \Lol~on essentially arbitrarily large data,  achieving  in-memory speeds for small data, and enabling the same speeds for multi-terabyte data (Figure \ref{f:speed}{\color{magenta}A}, dark green line).
Third, because \Lol~is so simple, we can use randomized approximate algorithms to further accelerate its performance.  
In particular, random projections---for which the data are multiplied by a lower-dimensional random matrix---have been shown to provide excellent approximation eigenvectors \cite{Candes2006b}.  Moreover, very sparse random projections, in which the elements of the matrix are mostly zero, with $\pm 1$ randomly distributed, have been shown to be effective, and have significant computational benefits \cite{Hastie2006}.
 We therefore further modified FlashX to incorporate very sparse random projections, which we denote by Linear Approximate Low-rank (\Lal) .   \Lal~shows an order of magnitude improvement in both the in-memory and semi-external memory implementations (Figure \ref{f:speed}{\color{magenta}A}; orange lines).
 
 These empirical observations mirror the theoretical bounds of performance.  In particular, given $T$ threads with sparsity $c$, our implementation achieves a computational complexity of $\mc{O}(npd/Tc)$, with an optimal speed up and scale up (not shown).
 Moreover, the error for \Lol~and \Lal~are substantially smaller than \Pca~for this setting for all dimensions (Figure \ref{f:speed}{\color{magenta}B}). 
 

%
%Figure \ref{f:speed}{\color{magenta}A} demonstrates both the in memory and semi-external memory \cite{SEM_SpMM} computing models of our implementation.  For the in memory implementation (light green line), we see that the run time increases linearly (optimally) with the number of dimensions, requiring about 11 minutes to run \Lol~on a $p=32$,$000$,$000$ dimensional problem with $n=2000$ samples;  nearly  half a terabyte of data.   This linear increase is the optimal scale up according to Ahmdel's Law \cite{Amdahl1967}.  
%%This example already requires 477 gigabyte (GB) to store the data, but $<$1 GB to perform the computations.  
%To run \Lol~on larger data we developed a semi-external memory implementation, which stores the data matrix on solid state drives, and the low-rank estimates in RAM \cite{abello1998functional}.  
%%This strategy allows us to run \Lol~as long as the data can fit on disk, and the low-rank approximation can fit in RAM.  
%The semi-external memory implementation (dark green line) achieves the same performance as the in memory implementation whenever the data are small enough for in memory, and continues scaling optimally (linearly) as the dimensionality further increases to $p=128,000,000$ dimensions, a multi-terabyte scenario.
%
%%Another option for making \Lol~scale up better is to utilize randomized algorithms.  
% We therefore further modified FlashX to incorporate random projections and very sparse random projections, which we denote Linear Approximate Low-rank (\Lal).   Figure \ref{f:speed}{\color{magenta}A} shows an order of magnitude improvement in both the in-memory and semi-external memory implementations using very sparse random projections.
%%
%Figure \ref{f:speed}{\color{magenta}B} shows the error for \Lol~and \Pca~in this scenario,  to demonstrate that even in this extremely high dimensional setting, \Lol~still outperforms \Pca. As expected, \Lal~performs as well as \Lol~for this high-dimensional settings.



% To quantify the computational efficiency of \Lol~and its variants, Figure \ref{f:speed} shows the wall time it takes to run each method on the stacked cigars problem, varying the ambient dimensionality, embedded dimensionality, and sample size.  Note that for completeness, we include two additional variants of \Lol: \Lal~and \Lfl.  \Lfl~(short for Linear Fast Low-rank) replaces the standard \Svd~algorithm with a randomized variant, which can be much faster in certain situations \cite{Halko2011a}.  \Lal~(short for Linear Approximate Low-rank) goes even one step further, replacing \Svd~with random projections \cite{Candes2006a}.  This variant of \Lol~is the fastest, its runtime is  least sensitive to $(p,d,n)$, and its accuracy is often commensurate (or better) than other variants of \Lol.  The runtime of all the variants of \Lol~are quite similar to \sct{Fld $\circ$ Pca}.  Given, given \Lol's improved accuracy, and nearly identical simplicity, it seems there is very little reason to not use \Lol~instead of \sct{Fld $\circ$ Pca}.

\begin{figure}[h!]
\centering
% \includegraphics[width=1\linewidth]{../Figs/speed_test}
\includegraphics[width=1\linewidth]{../Figs/scalability}
\caption{
Computational efficiency of various low-dimensional projection methods. In all cases, $n=2000$, and we used spherically symmetric  simulation parameters (see Methods for details).   We compare \Pca~with the projection step of \Lol~(light green for in memory, dark green for semi-external memory ) and \Lal~(light orange for in-memory, dark orange for semi-external memory) for different observed dimensions ($p$). \textbf{(A)} \Lol~exhibits optimal (linear) scale up and scale out, requiring only 46 minutes to find the embedding on a 2TB dataset, and only 3 minutes using \Lal (the sparse constant of sparse random projection $c=\dfrac{1}{\sqrt{p}}$). \textbf{(B)} Error for \Lal~is the same as \Lol~in this setting, and both are significantly better than \PoF~for all choices of embedding dimension.
% from \Lol, \Qoq, \Lrl, \Lfl, and \Lal, for different values of $(p,d)$.  The addition of the mean difference vector is essentially negligible.  Moreover, for small $d$, the \Lfl~is advantageous.  \Lal~is always fastest, and its performance is often comparable to other methods (not shown).
}
\label{f:speed}
\end{figure}

%\para{Computational Theory}

%We reify the above computational experiments with theoretical statements.  Computing the mean for each class requires $\mc{O}(n_j p)$ floating point operations (flops), where $n_j$ is the number of samples per class. Subtracting the means requires $\mc{O}((J-1) p)$ flops, where $J$ is the total number of classes.  Computing the first $d$ singular triples (left and right singular vectors, and singular value) requires $\mc{O}(n p d)$ flops.
%The sparse random projection however only requires $\mc{O}(n p d / c)$, where $c1$ is the sparsity of the sparse random projection matrix.
%%The randomized version of \Svd~however only requires $\mc{O}(d^2 (n + p))+ (q+1) 2 d M$, where $M$ is the flop count required for matrix vector multiply.
%Since, $d \ll n,p$, this can substantially reduce computational complexity.  Regardless, \Lol~computation is clearly bottlenecked by the \Pca~computation or random projection for single threaded operations.
%
%These empirical estimates of computational complexity match the theoretically optimal rates.  
%In particular, with $n$ samples in $p$ dimensions embedded into $d$ dimensions give $T$ threads, the computational complexity is $\mc{O}(n p d / T)$ flops for \Lol, or and $\mc{O}(n p d / c T)$ flops for \Lal, where $c$ is the sparsity of the random projection matrix.


%Our parallel in memory implementation scales-up optimally (linearly) with the number of threads, requiring $\mc{O}(n p d / T)$ flops for \Lol,
%and $\mc{O}(n p d / c T)$ flops for \Lal,
%where $T$ is the number of threads. 
% Our parallel semi-external memory implementation scales-out optimally (linearly) as well.  This is in agreement with the linear fit of the lines in Figure \ref{f:speed}A.


% first computing a dot product, which requires $\mc{O}(n p d/T)$ space and time. Given that dot product, the result is an $n \times n$ matrix, so computing the first $d$ singular triples only requires $\mc{O}( n^2 d / T)$ space and time.  Therefore, the most computationally taxing operation is the dot product.  This is, in fact, the motivation for developing a randomized version of \Lol.  Specifically, if we employ a very sparse random projection matrix \cite{??} with only $z$ non-zeros, then the dot product operation reduces to $\mc{O}(n z p/T)$, which can yield a significant computational savings.



\subsection*{Benchmark Real Data Applications}



\begin{figure}[h!]
\centering
\includegraphics[width=1\linewidth]{../Figs/plot_real}
\caption{
For three datasets, we benchmark \Lol~(green) versus standard classification methods, 
%including support vector machines (blue up triangles), 
\Pca'~(cyan),
\Pca~(magenta),
\Road~(orange), and 
\sct{Lasso}~(dark blue).  For \Lol, \Pca, and \Pca', we compose the embedding with Fisher's \Lda.  
%and random forest (orange diamonds).

The three panels show the misclassification rate (vertical axis) and the number of projection dimensions (horizontal axis).  \textbf{(A)} A standard sparse colon cancer genetics dataset. \textbf{(B)} A dense image dataset from Figure \ref{f:mnist}, but this time using all 10 digits. \textbf{(C)} A magnetic resonance imaging dataset with over five hundred million features.  In all cases, for all number of projection dimensions, \Lol~does as well or better than all other methods.  

%Top panels show error rate as a function of log$_2$ number of embedded dimensions (for \Lol, \Road, and \sct{Lasso}) or cost (for SVM).
%% \Lol~consistently achieves lower error for fewer dimensions.
%Bottom panels show the minimum error rate achieved by each of the five algorithms versus time.
%The lower left dark gray (upper right light gray) rectangle is the area in which any algorithm is \emph{better}  (worse) than \Lol~in terms of both accuracy and efficiency.
%\textbf{(A)} Prostate: a standard sparse dataset.  1-dimensional \Lol~does very well, although keeping $2^5$ ambient coordinates slightly improves performance, at a significant cost of compute time (two orders of magnitude), with minimal additional interpretability.
%\textbf{(B)} Colon: another standard sparse dataset.  Here, 2-4 dimensions of \Lol~outperforms all other approaches considered regardless of how many dimensions they keep.
%\textbf{(C)} MNIST: 10 image categories, so \Road~is not possible.  \Lol~does very well regardless of the number of dimensions kept.  SVN marginally improves on \Lol~accuracy, at a significant cost in computation (two orders of magnitude).
%\textbf{(D)} CIFAR-10: a higher dimensional and newer 10 category image classification problem.  Results are qualitatively similar to C.
%%
%Note that, for all four of the problems, there is no algorithmperforming better and faster than \Lol; rather, most algorithms typically perform worse and slower (though some are more accurate and much more computationally expensive).
% This suggests that regardless of how one subjectively weights computational efficiency versus accuracy, \Lol~is the best default algorithm in a variety of real data settings.
}
\label{f:realdata}
\end{figure}

Although \Lol~both statistically and computationally outperforms its natural competitors both in theory and in a variety of simulation settings, real data often tell a different story.
%
% To more comprehensively understand the relative advantages and disadvantages of \Lol~with respect to other high-dimensional classification approaches, in addition to evaluating its performance in theory, and in a variety of numerical simulations, it is important to evaluate it also on benchmark datasets.
We have  therefore selected four commonly used high-dimensional datasets to compare \Lol~to several state-of-the-art algorithms (see Methods for details).  For each dataset, we compare \Lol~to (i) support vector machines (SVM), (ii) \Road, (iii) lasso, (iv) and random forest (RF).  Because in practice all these approaches have ``hyperparameters'' to tune, we consider several possible values for  SVM, lasso, and \Lol~(but not RF, as its runtime was too high).  Figure \ref{f:realdata} shows the results for all four datasets. For each, we use MATLAB implementations, partially because \Road~provides MATLAB code, and also because of MATLAB's ubiquity in certain communities.  Because we also have R and FlashX implementations of \Lol, comparisons using other languages would be straightforward.





Qualitatively, the results are similar across datasets: \Lol~achieves high accuracy and computational efficiency as compared to the other methodologies.  Considering Figure \ref{f:realdata}A and B, two popular sparse settings, we find that \Lol~can find very low dimensional projections with very good accuracy. For the prostate data, with a sufficiently non-sparse solution for \Road, it slightly outperforms \Lol, but at substantial computational cost; in particular, \Road~takes about 100 times longer to run on this dataset.   Figure \ref{f:realdata}C and D are 10-class problems, so \Road~is no longer possible.  Here, SVM can again slightly outperform \Lol, but again, requiring 100 fold additional computational time.  In all cases, the beloved random forest classifier performs subpar. In all four scenarios, there is never an algorithm that achieves smaller error in less time, as indicated by the dark gray box being empty in the lower panels of Figure \ref{f:realdata}A-D.


\subsection*{Extensions to Other Supervised Learning Problems}

The utility of incorporating the mean difference vector into supervised machine learning for big and wide data extends beyond merely classification.  In particular, hypothesis testing can be considered as a special case of classification, with a particular loss function.  Therefore we apply the same idea to a hypothesis testing scenario.  The multivariate generalization of the t-test, called Hotelling's Test, suffers from the same problem as does the classification problem; namely, it requires inverting an estimate of the covariance matrix whose estimate would be low-rank and therefore singular, in the high-dimensional setting. To mitigate this issue in the hypothesis testing scenario, prior art applied similar tricks as they have done in the classification setting. One particularly nice and related example is that of  Lopes et al. \cite{Lopes2011a}, who addresses this dilemma by using random projections to obtain a low-dimensional representation, following by applying Hotelling's Test in the lower dimensional subspace.  Figure \ref{f:generalizations}A and B shows the power of their test alongside the power of the same approach, but using the \Lol~projection rather than random projections.  The two different simulations include the simulated settings considered in their manuscript (see Methods for details).  The results make it clear that the \Lol~test has higher power for essentially all scenarios.  Moreover, it is not merely the replacing random projections with \Pca~(solid magenta line), nor simply incorporating the mean difference vector (dashed green line), but rather, it appears that \Lol~for testing uses both modifications to improve performance.

High-dimensional  regression is another supervised learning method that can utilize the \Lol~idea. Linear regression, like classification and Hotelling's Test, requires inverting a  matrix as well.  By projecting the data only a lower dimensional subspace first, followed by linear regression on the low-dimensional data, we can mitigate the curse of high-dimensions.  To choose the projection matrix, we partition the data into K partitions, based on the percentile of the target variable, we obtain a K class classification problem.  Then, we can apply \Lol~to learn the embedding.  Figure \ref{f:generalizations}C shows an example of this approach, contrasted with \Lasso~and partial least squares, in a sparse simulation setting (see Methods for details). \Lol~is able to find a better low-dimensional projection than \Lasso, and performs significantly better than partial least squares, for essentially all choices of number of dimensions to embed into.




\begin{figure}
% \begin{wrapfigure}{R}{0.7\textwidth} %[h!]
\centering
\includegraphics[width=1\linewidth]{../Figs/regression_power}
\caption{
The intuition of including the mean difference vector is equally useful for other supervised manifold learning problems, including testing and regression.
\textbf{(A)} and \textbf{(B)} show two different high-dimensional testing settings, as described in Methods.  Power is plotted against the decay rate of the spectrum, which approximates the effective number of dimensions.  \Lol~composed with Hotelling's test outperforms the random projections variants described in \cite{Lopes2011a}, as well as several other variants.
\textbf{(C)} A sparse high-dimensional regression setting, as described in Methods, designed for sparse methods to perform well.  Log$_{10}$ mean squared error is plotted against the number of embedded dimensions.
\Lol~composed with linear regression outperforms \sct{Lasso}~(cyan), the classic sparse regression method, as well as partial least squares (PLS; black).
% In the legend, 'A' denote either 'linear regression' (in C), or 'Hotelling' (in A and B).
These three simulation settings therefore demonstrate the generality of this technique.
}
\label{f:generalizations}
% \end{wrapfigure}
\end{figure}
% \clearpage


\section*{Discussion}
% get 64 lines, only using 20.


%However, in the face of high-dimensional data, \Pca~does not weight the discriminant directions sufficiently, and therefore performs only moderately well.\footnote{When having to estimate the eigenvector from the data, \Pca~performs even worse.  This is because when $n \ll p$, \Pca~is an inconsistent estimator with large variance \cite{Baik2006a,Paul2007a}.}

%
% To understand \Lol, consider the following simplified scenario.  Assume there exists two classes of data, both of which are sampled according to a high-dimensional Gaussian distribution, and both have the same covariance matrix.  Assuming that there are the same number of observations from each class  implies that the only difference between the two classes are their means.  Figure \ref{f:cigars} shows three different examples of this setting, each with 100 total samples in 1,000 dimensions (only the first two dimensions are shown).
% In panel \textbf{(A)}, each dimension is independent of the others, but the second dimension is different: the means in the second dimension are further away from one another than they are in all the others, and the variance in that dimension is smaller.  This setting, which we called ``stacked cigars'', is relatively easy for all the above described methods, because the direction of maximal variance is also the direction that is maximally informative about the classification task.
% @jovo: possibly continue this line of thought?
%
% @jovo: possibly discussion
% The first century of statistical pattern recognition focused on elegant SL approaches for low-dimensional problems (e.g., Fisher's Linear Discriminant (FLD) \cite{Fisher}), and dimensionality reduction techniques for high-dimensional problems (e.g., Principal Components Analysis (PCA) \cite{PCA}), but did not seriously investigate high-dimensional SL approaches.  More recent work has focused on high-dimensional SL approaches, including sparse methods (e.g., Lasso \cite{Lasso}) and supervised dictionary learning techniques \cite{Mairal2009}.  However, theoretical guarantees for sparse methods require very strict assumptions that are often far from accurate \cite{??}, meaning that sparse methods tend to exhibit significant bias.  Moreover, their susceptibility to correlations of the predictor variables can lead to substantial variance, insofar that which variables are selector can be highly noise dependent \cite{Meinshausen2010c}.
% Dictionary learning, on the other hand, has scant theoretical guarantees in any setting \cite{??}.  Moreover, because dictionary learning tends to consider a large complex discriminant model, its resulting estimates are highly dependent on initialization choice.  Finally, computationally, both are problematic for big data, in that they require large amounts of time, and implementations on scalable platforms are currently unavailable.
%
% @jovo: possibly discussion
% In complementary work, generalizations of principal components to include various kinds of nonlinear dimensionality reduction techniques, collectively referred to as  ``manifold learning'', have been developing recently \cite{??}. These tools are predominantly ``unsupervised'', and therefore, do not explicitly search for a discriminant boundary, and therefore may add significant bias to the problem at hand.  Adding supervision to these techniques so far has been difficult; methods proposed thus far embed the data in an unsupervised fashion first, again, potentially adding bias \cite{Belkin2004a}, and require substantial computations.   Thus, a supervised manifold learning algorithm designed for one of the simplest possible high-dimensional classification problems remains amiss.
%
% @jovo: probably for discussion
% The computer science community has been developing tools for big data for several decades now. Specifically, algorithm complexity has lowered significantly, and ability to scale-up and scale-out has increased significantly.  This is true both for simple statistical primitives, such as computing a mean, and more complicated primitives like PCA \cite{??}.  More recently, a large number of distributed machine learning libraries have become available, including Apache Spark's mllib,  H20, Dato, and Vowpal Wabbit \cite{??}.  These focus almost entirely on large sample size and low dimensionality regime, whereas the motivating problems of interest for this work are small sample size and high-dimensionality. Moreover, they utilize a distributed platform that is susceptible to low bandwidth communication between nodes, meaning that as one adds resources, the computations cannot scale out optimally.
%
% Thus, a gap remains in the literature for a statistical tool that, under relatively general assumptions, achieves both small bias and variance, can be made robust, and has easily tunable hyper-parameters.  And complementarily, an implementation of that algorithm that is numerically stable, requires minimal space and time, is highly scalable for high-dimensional problems. Importantly, the method should perform well both on simulated and real data experiments.  And ideally, the software is intuitive, free and open source, and easy to use.
%
% To address these concerns,  we describe a novel solution, called Low-Rank Optimal Linear Discriminant Analysis (\Lol).
% We developed \Lol~by considering the above described simple scenario, and realizing that the optimal dimension reduction for such a setting would have to combine both the means and the covariances, because together those two parameters determine the optimal decision boundary.
%
% @jovo: discussion?
% While other methods have previous had a similar insight (in particular the ``First Two Moment'' (F2M) methods \cite{Li1991a, Tishby1999a, Globerson2003a, Cook2005a,Fukumizu2004a}), F2M methods only operate in low-dimensional settings.
%
% Therefore, we decided to combine the means and covariances differently, essentially by first projecting the data onto the means, and then effectively computing PCA on the ``residuals''.  Running FLD after projecting the data onto the subspace spanned jointly by the means and the top principal components comprises the \Lol~method.  In addition to MATLAB and R, we also implemented \Lol~using FlashX, a recently developed big data analytics library \cite{??}.  We were therefore able to take advantage of matrix parallelizations optimized for machines with solid state drives.

We have introduced a very simple, yet new, methodology to improve performance on supervised learning problems with big and wide data.  In particular, we have proposed a supervised manifold learning procedure that utilizes both the difference of the means, and the covariance matrices, and proved that it performs better than first applying \Pca~to the data under reasonable assumptions.  This is in stark contrast to most previous approaches, which only utilize the covariance matrices (or kernel variants thereof), or try to solve a difficult optimization theoretic problem.  In addition to demonstrating the statistical accuracy and computational efficiency of \Lol~on simulated and real classification problems, we also demonstrate how the same idea can also be used for other kinds of supervised learning problems, including regression and hypothesis testing. Theoretical guarantees suggest that this line of research is promising and can be extended to other more general settings and tasks.


% \subsection*{Qualitative Design Considerations}

% Finally, in addition to the above mentioned quantitative criteria, there are also many qualitative criteria that merit consideration at each level of analysis.  For the problem specification level,  the model choices also determine the  \textbf{appropriateness}.  Indeed, real decision criteria are often quite difficult to specify precisely, and so our problem specification is almost always related to but not exactly the problem we hope to achieve.  For example, while we might want to parse a scene, we often tackle a simpler problem of checking whether an object of a particular kind is present because it is easier.  For the algorithm level, the \textbf{interpretability} of the resulting estimator is often an important consideration.  For example, when developing biomarkers, it is important that the doctors and insurance companies can understand the decision making process of the algorithm, otherwise they might not believe the results. For the implementation level,  how \textbf{accessible} is the implementation is crucial.  In particular as more people outside the ivory academic tower---such as citizen scientists and even academics in the developing world---are contributing to data science and scientific discovery in general, access to the implementation is increasingly important.  Finally, at the platform level, the \textbf{ease of use} of the platform is increasingly important.  For example, requiring tedious installations of many different software libraries, or specialized hardware, can make the platform difficult to employ.

\para{Related Work}
%
One of the first publications to compose \Fld~with an unsupervised learning method was the celebrated Fisherfaces paper \cite{Belhumeur1997a}.  The authors showed via a sequence of numerical experiments the utility of embedding with \Pca~prior to classifying with \Fld.  We extend this work by adding a supervised component to the initial embedding.  Moreover, we provide the geometric intuition for why and when this is advantageous, as well as show numerous examples demonstrating its superiority.  %Finally, we have matrix concentration inequalities proving the advantages of \Lol~over Fisherfaces.
We also prove that Fisherfaces can be implemented very efficiently (see Methods), while shedding light on why it is performing relatively well in general.

% Most manifold learning methods, while exhibiting both strong theoretical \cite{Eckart1936a,deSilva2003, Allard2012} and empirical performance, are fully unsupervised.  Thus, in classification problems, they discover a low-dimensional representation of the data, ignoring the labels.  This can be highly problematic when the discriminant dimensions and the directions of maximal variance in the learned manifold are not aligned (see Figure \ref{f:mnist} for an example).  Supervised dimensionality reduction techniques, therefore, combine the best of both worlds, explicitly searching for low-dimensional discriminant boundaries.

% A set of methods from the statistics community is collectively referred to as   ``sufficient dimensionality reduction'' (SIR) or ``first two moments'' (F2M) methods  \cite{Li1991a, Tishby1999a, Globerson2003a, Cook2005a,Fukumizu2004a}.  These methods are theoretically elegant, but typically require the sample size to be larger than the number of observed dimensions (although see \cite{Cook2013} for some promising work).  Other approaches formulate an optimization problem, such as projection pursuit \cite{Huber1985a}, empirical risk minimization \cite{Belkin2006a}, or supervised dictionary learning \cite{Mairal2009}.  These methods are limited because they are prone to fall into local minima, they require costly iterative algorithms, and lack any theoretical guarantees \cite{Belkin2006a}.   Thus, there remains a gap in the literature: a supervised learning method with theoretical convergence guarantees appropriate when the dimensionality is orders of magnitude larger than the sample size.



\para{Next Steps}
%
The \Lol~idea, appending the mean difference vector to convert unsupervised manifold learning to supervised manifold learning, has many potential applications.  We have presented the first few.  Incorporating additional nonlinearities via kernel methods \cite{Mika1999a}, ensemble methods such as random forests \cite{Breiman2001a}, and multiscale methods \cite{Allard2012}
% ,  and more scalable implementations \cite{Chang2011a},
are all of immediate interest. MATLAB, R, and FlashR code for the experiments performed in this manuscript is available from \url{http://docs.neurodata.io/LOL/}.

\clearpage
\appendix

\section{Simulations}

% \subsection{Simulations}

For most simulation settings, each class is Guassian:
$f_{x|y} = \mc{N}(\bmu_y,\mb{\Sigma_y})$,
$f_y = \mc{B}(\pi)$.
We typically assume that both classes are equally like, $\pi=0.5$, and the covariance matrices are the same, $\bSig_0=\bSig_1=\bSig$. Under such assumptions, we merely specify $\bth=\{\bmu_0,\bmu_1, \bSig\}$.


\paragraph*{Stacked Cigars}
\begin{compactitem}
\item $\bmu_0=\mb{0}$,
\item $\bmu_1=(a, b, a, \ldots, a)$,
\item $\bSig$ is a diagonal matrix, with diagonal vector, $\bd=(1,b,1\ldots,1)$,
\end{compactitem}
where $a=0.15$ and $b=4$.


\paragraph*{Trunk}
\begin{compactitem}
\item $\bmu_0=b/\sqrt{(1, 3, 5, \ldots, 2p)}$,
\item $\bmu_1=-\bmu_0$,
\item $\bSig$ is a diagonal matrix, with diagonal vector, $\bd=100/\sqrt{(p, p-1, p-2, \ldots, 1)}$,
\end{compactitem}
where $b=4$.

% if ~isfield(task,'b'), b=4; else b=task.b; end
% mu1=b./sqrt(1:2:2*D)';
% mu0=-mu1;

% Sigma=eye(D);
% Sigma(1:D+1:end)=100./sqrt(D:-1:1);

\paragraph*{Rotated Trunk} Same as Trunk, but the data are randomly rotated, that is, we sample $\mb{Q}$ uniformly from the set of p-dimensional rotation matrices, and then set:
\begin{compactitem}
\item $\bmu_0 \leftarrow \mb{Q} \bmu_0$,
\item $\bmu_1 \leftarrow \mb{Q} \bmu_1$,
\item $\bSig \leftarrow \mb{Q} \bSig \mb{Q}\T$.
\end{compactitem}


\paragraph*{Toeplitz}
\begin{compactitem}
\item $\bmu_0=b \times (1,-1,1,-1,\ldots,1)$,
\item $\bmu_1= - \bmu_0$,
\item $\bSig$ is a Toeplitz matrix, where the top row is $\rho^{(0,1,2,\ldots,p-1)}$,
\end{compactitem}
where $b$ is a function of the Toeplitz matrix such that the noise stays constant as dimensionality increases, and $\rho=0.5$.

% D1=10;
% rho=0.5;
% c=rho.^(0:D1-1);
% A = toeplitz(c);
% K1=sum(A(:));

% c=rho.^(0:D-1);
% A = toeplitz(c);
% K=sum(A(:));

% mudelt=(K1*b^2/K)^0.5/2;
% mu0 = ones(D,1);
% mu0(2:2:end)=-1;
% mu0=mudelt*mu0;
% mu1=-mu0;

% Sigma=A;


\paragraph*{3 Classes} Same as Trunk, but with a third mean equal to the zero vector, $\mu_2=\mb{0}$.


\paragraph*{Fat Tails} For this setting, each class is actually a mixture of two Gaussians with the same mean (the two classes have the same covariances):
\begin{compactitem}
\item $\bmu_0=\mb{0}$,
\item $\bmu_1=(0,\ldots,0,1,\ldots,1)$, where the first $s=10$ elements are zero,
\item $\bSig_0$ is a matrix with one's on the diagonal, and $0.2$ on the off diagonal,
\item $\bSig_1 = 15 \times \bSig_0$,
\end{compactitem}
and then we randomly rotated as in the rotated Trunk example.




\paragraph*{QDA} A generalization of the Toeplitz setting, where the two classes have two different covariance matrices, meaning that the optimal discriminant boundary is quadratic.
\begin{compactitem}
\item $\bmu_0=b \times (1,-1,1,-1,\ldots,1)$,
\item $\bmu_1=-\mb{Q} \times (\bmu_0+0.1)$,
\item $\bSig_0$ is the same Toeplitz matrix as described above, and
\item $\bSig_1 = \mb{Q} \bSig_0 \mb{Q}\T$.
\end{compactitem}



% if ~isfield(task,'b'), b=0.4; else b=task.b; end
% D1=10;

% rho=0.5;
% c=rho.^(0:D1-1);
% A = toeplitz(c);
% K1=sum(A(:));

% c=rho.^(0:D-1);
% A = toeplitz(c);
% K=sum(A(:));
% Sigma=A;

% mudelt=(K1*b^2/K)^0.5/2;
% mu0 = ones(D,1);
% mu0(2:2:end)=-1;
% mu0=mudelt*mu0;


% [Q, ~] = qr(randn(D));
% if det(Q)<-.99
%     Q(:,1)=-Q(:,1);
% end

% th=pi/4;
% Q(1:2,1:2)=[cos(th) -sin(th); sin(th) cos(th)];
% Q(1,3:end)=0;
% Q(2,3:end)=0;
% Q(3:end,1)=0;
% Q(3:end,2)=0;

% Sigma(:,:,2)=Q*Sigma*Q';
% mu1=-Q*(mu0+0.1);



\paragraph*{Outliers} In this dataset, we generate $n/2$ samples from an inlier model, and the remaining $n/2$ samples from an outlier model.  For the inlier model, we first generate a random $d \times p$ dimensional orthonormal matrix, $V$, where $d=p/10$.  Then, the first half of the inlier points are generated by $f_{x|0}$, the next half by $f_{x|1}$, and the remaining points generated by $f_{x | \emptyset}$. For the outliers, we sampled their class randomly from a fair Bernoulli distribution:
\begin{compactitem}
\item $f_{x|0} = \mc{N}_d(0,\sigma^2) \times V\T$,
\item $f_{x|1} = \mc{N}_d(0,\sigma^2) \times V\T + b$,
\item $f_{x|\emptyset} = \mc{N}_p(\mb{0},\sigma^2\mb{I})$,
\item $f_{\emptyset} = \mc{B}(0.5)$.
\end{compactitem}
where we set $\sigma=0.1$ and $b=0.5$.

% Ninlier=task.n/2;
% n0=Ninlier/2;
% n1=n0;
% Noutlier=task.n-Ninlier;
% task.D=200;
% d=round(task.D/10);
% noise=0.1;
% offset=0.5;
% V=orth(rand(task.D,d));
% X0=randn(n0,d)*V';
% X1=randn(n1,d)*V'+offset;
% X=[X0;X1; randn(Noutlier,task.D)];
% X=X+randn(size(X))*noise;

% Y=[zeros(n0,1); ones(n1,1); rand(Noutlier,1)>0.5]+1;




\clearpage

\begin{figure}
\centering
\includegraphics[width=1\linewidth,trim=0.5in 4.5in 0.5in 0.5in,clip=true]{../Figs/table} %l b r t
\caption{Table of algorithms and their properties for high-dimensional data. Gray elements indicate that results are demonstrated in the Figure labeled in the bottom row. 'X' denotes relatively good performance for a given setting, or has the particular property.
}
\label{f:table}
\end{figure}



\clearpage





\section[theory]{Theoretical Background}


\subsection{The Classification Problem}

Let $(\bX,Y)$ be a pair of random variables, jointly sampled from $F :=F_{\bX,Y}=F_{\bX|Y}F_{Y}$.
Let $\bX$ be a multivariate vector-valued random variable, such that its realizations live in p dimensional Euclidean space, $\bx \in \Real^p$.  Let $Y$ be a categorical random variable, whose realizations are discrete,  $y \in \{0,1,\ldots C\}$.  The goal of a classification problem is to find a function $g(\bx)$ such that its output tends to be the true class label $y$:
\begin{align*} %\label{eq:bayes}
g^*(\bx) := \argmax_{g \in \mc{G}} \PP[g(\bx) = y].
\end{align*}
When the joint distribution of the data is known, then the Bayes optimal solution is:
\begin{align}  \label{eq:R}
g^*(\bx) := \argmax_y f_{y|\bx} = \argmax_y f_{\bx|y}f_y =\argmax_y \{\log f_{\bx|y} + \log f_y \}
\end{align}
Denote expected misclassification rate of classifier $g$ for a given joint distribution $F$,
\begin{align*}
L^F_g := \EE[g(\bx) \neq y] := \int \PP[g(\bx) \neq y] f_{\bx,y} d\bx dy,
\end{align*}
where $\EE$ is the expectation, which in this case, is with respect to $F_{XY}$.
For brevity, we often simply write $L_g$, and we define $L_* := L_{g^*}$.


\subsection{Linear Discriminant Analysis (\Lda)}

Linear Discriminant Analysis (\Lda) is an approach to classification that uses a linear function of the first two moments of the distribution of the data.  More specifically, let $\mu_j=\EE[F_{X|Y=j}]$ denote the class conditional mean, and let $\bSig=\EE[F_{X}^2]$ denote the joint covariance matrix, and $\pi_j=\PP[Y=j]$.   Using this notation, we can define the \Lda~classifier:
\begin{align*}
g_{\Lda}(\bx)&:=\argmin_y \frac{1}{2} (\bx-\bmu_0)\T \bSig^{-1}(\bx-\bmu_0) + \II\{Y=y\}  \log \pi_y,
\end{align*}
where $\II\{ \cdot\}$ is one when its argument is true, and zero otherwise.
Let $L_{\Lda}^F$ be the misclassification rate of the above classifier for distribution $F$.
%
Assuming equal class prior and centered means,  $\pi_0=\pi_1$ and $(\bmu_0+\bmu1)/2=\mb{0}$, re-arranging a bit, we obtain
\begin{align*}
g_{\Lda}(\bx) :=  \argmin_y \bx\T \bSig^{-1} \bmu_y.
\end{align*}
In words, the  \Lda~classifier chooses the class for whom the projection of an input vector $\bx$, onto $\bSig^{-1} \bmu_y$, is maximized.
%
When there are only two classes, this further simplies to
\begin{align*}
g_{2-\Lda}(\bx) :=  \II\{ \bx\T \bSig^{-1} \bdel > 0 \},
\end{align*}
where $\bdel=\bmu_0-\bmu_1$.   Note that the equal class prior and centered means assumptions merely changes the threshold constant from $0$ to something else.

\subsection{\Lda~Model}

A statistical model is  a family of distributions indexed by a parameter $\bth \in \bTh$, $\mc{F}_{\bth}=\{F_{\bth} : \bth \in \bTh \}$.
Consider the special case of the above where $F_{\bX|Y=y}$ is a multivariate Gaussian distribution,
$\mc{N}(\bmu_y,\bSig)$, where each class has its own mean, but all classes have the same covariance.
We refer to this model as the \Lda~model.
Let $\bth=(\bpi,\bmu,\bSig)$, and let $\bTh_{C-\Lda}=( \triangle_C, \Real^{p \times C},\Real_{\succ 0}^{p \times p})$, where $\bmu=(\bmu_1,\ldots, \bmu_C)$, $\triangle_C$ is the $C$ dimensional simplex, that is $\triangle_C = \{ \bx : x_i \geq 0 \forall i, \sum_i x_i = 1\}$, and $\Real_{\succ 0}^{p \times p}$ is the set of positive definite  $p \times p$ matrices. Denote
$\mc{F}_{\Lda}=\{F_{\bth} : \bth \in \bTh_{\Lda}\}$, dropping the superscript $C$ for brevity where appropriate.
The following lemma is well known:
\begin{lem}
$L_{\Lda}^F=L_*^F$ for any $F \in \mc{F}_{\Lda}$.
\end{lem}

\begin{proof}
Under the \Lda~model, the Bayes optimal classifier is available by plugging the explicit distributions into Eq.~\eqref{eq:R}.
\end{proof}




\section[projections]{Projection Based Classifiers}


Let $\bA \in \Real^{d \times p}$ be an orthonormal matrix, that is, a matrix that projects p dimensional data into a d dimensional subspace, where $\bA\bA\T$ is the $d \times d$ identity matrix, and $\bA\T \bA$ skis symmetric $p \times p$ matrix with rank d.   The question that motivated this work is: what is the best projection matrix that we can estimate, to use to ``pre-process'' the data prior to applying \Lda.
% \begin{lem}
% $g_{\Lda}^F(\bA \bx)= \II \{ (\bA \bx)\T \bSig^{-1}_A \bdel_A > 0\}$.
% \end{lem}
Projecting the data $\bx$ onto a low-dimensional subspace, and the classifying via \Lda~in that subspace is equivalent to redefining the parameters in the low-dimensional subspace,
$\bSig_A=\bA \bSig \bA\T \in \Real^{d \times d}$ and $\bdel_A = \bA \bdel \in \Real^d$, and then using $g_{\Lda}$.  When $C=2$, $\pi_0=\pi_1$, and $(\mu_0+\mu_1)/2=\mb{0}$, this amounts to:
\begin{align} \label{eq:g_A}
g^d_A(x) := \II \{ (\bA \bx)\T \bSig^{-1}_A \bdel_A > 0\}, \text{ where } \bA \in \Real^{d \times p}.
\end{align}
Let $L^d_A :=\int \PP[g_A(\bx)=y] f_{\bx,y} d\bx dy$.
Our goal therefore is to be able to choose $A$ for a given parameter setting $\bth=(\bpi, \bdel,\bSig)$, such that $L_A$ is as small as possible (note that $L_A$ will never be smaller than $L_*$).

Formally, we seek to solve the following optimization problem:
% \begin{align} \label{eq:A}
% \bA_* = \argmin_{\bA \in \Real^{p \times d}} L_A.
% \end{align}
\begin{equation} \label{eq:A}
\begin{aligned}
& \underset{\bA}{\text{minimize}}
& & \EE [ \II \{ \bx\T \bA\T \bSig^{-1}_A \bdel_A > 0\} \neq y] \\
& \text{subject to} & & \bA \in \Real^{p \times d}, \quad \bA \bA\T = \bI_{d \times d},
\end{aligned}
\end{equation}
where $\bI_{u \times v}$ is the $u \times v$ identity matrix identity, that is, $\bI(i,j)=1$ for all $i=j \leq \min(u,v)$, and zero otherwise.
Let $\mc{A}^d=\{\bA : \bA \in \Real^{d \times p}, \bA \bA\T = \bI_{d \times d}\}$, and let $\mc{A}_* \subset \mc{A}$ be the set of $\bA$  that minimize Eq.~\eqref{eq:A}, and let $\bA_* \in \mc{A}_*$ (where we dropped the superscript $d$ for brevity).   Let $L_{\bA}^*=L_{\bA_*}$ be the misclassification rate for any $\bA \in \mc{A}_*$, that is, $L_{\bA}^*$ is the Bayes optimal misclassification rate for the classifier that composes $\bA$ with \Lda.


In our opinion, Eq.~\eqref{eq:A} is the simplest supervised manifold learning problem there is: a two-class classification problem, where the data are multivariate Gaussians with shared covariances, the manifold is linear, and the classification is done via \Lda.
Nonetheless, solving Eq.~\eqref{eq:A} is difficult, because we do not know how to evaluate the integral analytically, and we do not know any algorithms that are guaranteed to find the global optimum in finite time.  This has led to previous work using a surrogate function \cite{not sure who}.
We proceed by studying a few natural choices for $\bA$.





\subsection{Bayes Optimal Projection}

% Let $\mb{\bA}\T=\bSig^{-1} \bdel$.
\begin{lem}
$\bdel\T  \bSig^{-1} \in \mc{A}_*$
\end{lem}

\begin{proof}
Let $\bB = (\bSig^{-1} \bdel)\T = \bdel\T (\bSig^{-1})\T = \bdel\T \bSig^{-1}$, so that $\bB\T = \bSig^{-1} \bdel$,
% = \bdel\T \bOm\T
and plugging this in to Eq.~\eqref{eq:g_A}, we obtain
\begin{align*}
g_{B}(x) &= \II \{ \bx \bB\T  \bSig^{-1}_{B} \bdel_{B} > 0\} &
\\&= \II \{ \bx\T \bSig^{-1} \bdel \times (\bSig^{-1}_{B} \bdel_{B}) > 0\} & \text{plugging in $\bB$}
\\&= \II \{ \bx\T \bSig^{-1} \bdel k > 0\} & \text{because $\bSig^{-1}_{B} \bdel_{B} > 0$}.
\end{align*}
In other words, letting $\bB$ be the Bayes optimal projection recovers the Bayes classifier, as it should.
Or, more formally, for any $F \in \mc{F}_{\Lda}$, $L_{\bdel\T \bSig^{-1}} = L_*$
\end{proof}

\subsection[PCA]{Principle Components Analysis (\Pca) Projection}
\label{sec:pca}

Principle Components Analysis (\Pca) finds the directions of maximal variance in a dataset.  \Pca~is closely related to eigendecompositions and singular value decompositions (\Svd).  In particular, the top principle component of a matrix $\bX \in \Real^{p \times n}$, whose columns are centered, is the eigenvector with the largest corresponding eigenvalue of the centered covariance matrix $\bX \bX\T$.  \Svd~enables one to estimate this eigenvector without ever forming the outer product matrix, because \Svd~factorizes a matrix $\bX$ into $\bU \bS \bV\T$, where  $\bU$ and $\bV$ are orthonormal  ${p \times n}$ matrices, and $\bS$ is a diagonal matrix, whose diagonal values are decreasing,  $s_1 \geq s_2 \geq \cdots > s_n$.  Defining $\bU =[\bu_1, \bu_2, \ldots, \bu_n]$, where each $\bu_i \in \Real^p$, then $\bu_i$ is the $i^{th}$ eigenvector, and $s_i$ is the square root of the $i^{th}$ eigenvalue of $\bX \bX\T$.  Let $\bA^{\Pca}_d =[\bu_1, \ldots , \bu_d]$ be the truncated \Pca~orthonormal matrix.

The \Pca~matrix is perhaps the most obvious choice of a orthonormal matrix for several reasons.  First, truncated \Pca~minimizes the squared error loss between the original data matrix and all possible rank d representations:
\begin{align*}
\argmin_{A \in \Real^{d \times p} : \bA \bA\T = \bI_{d \times d}} \norm{ \bX - \bA^T \bA }_F^2.
\end{align*}
Second, the ubiquity of \Pca~has led to a large number of highly optimized numerical libraries for computing \Pca~(for example, LAPACK \cite{Anderson1999a}).

Moreover, let $\bU_d=[\bu_1,\ldots,\bu_d] \in \Real^{p \times d}$, and note that $\bU_d\T \bU_d = \bI_{d \times p}$ and $\bU_d\T \bU_d  = \bI_{p \times d}$.  Similarly, let $\bU \bS \bU\T = \bSig$, and $\bU \bS^{-1} \bU\T = \bSig^{-1}$.  Let $\bS_d$ be the matrix whose diagonal entries are the eigenvalues, up to the $d^{th}$ one, that is $\bS_d(i,j)=s_i$ for $i=j \leq d$ and zero otherwise.  Similarly, $\bSig_d=\bU \bS_d \bU\T=\bU_d \bS_d \bU_d\T$.

Let $g_{\Pca}^d:=g_{A_{\Pca}^d}$, and let $L_{\Pca}^d:=L_{A_{\Pca}^d}$.
And let $g_{\Lda}^d := \II \{ x \bSig_d^{-1} \bdel > 0\}$ be the regularized \Lda~classifier, that is, the \Lda~classifier, but sets the bottom $p-d$ eigenvalues to zero.

\begin{lem}
$L_{\Pca}^d = L_{\Rrlda}^d$.
\end{lem}

\begin{proof}
Plugging $\bU_d$ into Eq.~\eqref{eq:g_A} for $\bA$, and considering only the left side of the operand, we have
\begin{align*}
(\bA \bx)\T \bSig^{-1}_A \bdel_A &= \bx\T \bA\T \bA \bSig^{-1} \bA\T \bA \bdel,
\\&= \bx\T  \bU_d\bU_d\T \bSig^{-1} \bU_d\bU_d\T \bdel,
\\&= \bx\T  \bU_d \bU_d\T \bU \bS^{-1} \bU \bU_d\bU_d\T \bdel,
\\&= \bx\T  \bU_d \bI_{d \times p} \bS^{-1} \bI_{p \times d} \bU_d\T \bdel,
\\&= \bx\T  \bU_d \bS^{-1}_d  \bU_d\T \bdel ,
\\&= \bx\T  \bSig^{-1}_d  \bdel.
\end{align*}
\end{proof}

The implication of this lemma is that if one desires to implement \Rrlda, rather than first learning the eigenvectors and then learning \Lda, one can instead directly implement regularized \Lda~by setting the bottom $p-d$ eigenvalues to zero.




\subsection[LOL]{Linear Optimal Low-Rank (\Lol) Projection}


The basic idea of \Lol~is to use both $\bdel$ and the top $d$ eigenvectors.  Most na\"ively, we could simply concatenate the two, $\bA_{\Lol}^d=[\bdel,\bA_{\Pca}^{d-1}]$.
Recall that eigenvectors are orthonormal.  To maintain orthonormality, we could easily apply Gram-Schmidt,  $\bA_{\Lol}^d=$ \sct{Orth}$([\bdel, \bA_{\Pca}^{d-1}])$.
Both in practice and in theory (as will be shown below), this orthogonalization step does not matter much.

to ensure that they are balanced appropriately, we normalize $\bdel$

each vector in $\bdel$ to have norm unity.  Formally, let $\mt{\bdel}_j = \bdel_j / \norm{\bdel_j}$, where $\bdel_j$ is the $j^{th}$ difference of the mean vector (remember, the number of vectors is equal to $C-1$, where $C$ is the total number of classes), and let  $\bA_{\Lol}^d=[\mt{\bdel}, \bA_{\Pca}^{d-(C-1)}]$.
The eigenvectors are all normalized and orthogonal to one another; to impose orthogonality between $\mt{\bdel}$ and the eigenvectors, we could use any number of numerically optimized algorithms.  However, in practice, orthogonalizing does not matter very much, so we do not bother. We formally demonstrate this below.




\section[LDA]{Theoretical Properties of LDA based Classifiers}


\subsection{\Lda~is rotationally invariant}

For certain classification tasks, the ambient coordinates have intrinsic value, for example, when simple interpretability is desired.  However, in many other contexts, interpretability is less important \cite{Breiman2001b}.  When the exploitation task at hand is invariant to rotations, then we have no reason to restrict our search space to be sparse in the ambient coordinates, rather, for example, we can consider sparsity in the eigenvector basis.  Fisherfaces is one example of a rotationally invariant classifier, under certain model assumptions.
Let  $\bW$ be a rotation matrix, that is $\bW \in \mc{W}=\{\bW : \bW\T = \bW^{-1}$ and det$(\bW)=1\}$.
Moreover, let $\bW \circ F$ denote the distribution $F$ after transformation by an operator $\bW$.  For example, if $F=\mc{N}(\bmu,\bSig)$ then $\bW \circ F=\mc{N}(\bW  \bmu, \bW \bSig \bW\T)$.

\begin{defi}
A rotationally invariant classifier has the following property:
$$L_g^F = L_g^{W \circ F}, \qquad F \in \mc{F}.$$
In words, the Bayes risk of using classifier $g$ on distribution $F$ is unchanged if $F$ is first rotated, for any $F \in \mc{F}$.
\end{defi}


Now, we can state the main lemma of this subsection:  \Lda~is rotationally invariant.
\begin{lem} \label{l:rot}
$L_{\Lda}^F = L_{\Lda}^{W \circ F}$, for any $F \in \mc{F}$.
\end{lem}

\begin{proof}
\Lda~simply becomes thresholding $\bx\T \bSig^{-1} \bdel$.  Thus, we can demonstrate rotational invariance by demonstrating that $\bx\T \bSig^{-1} \bdel$ is rotationally invariant.

% First, note that for any distribution $F \in \mc{F}_{\Lda}$, we can reparameterize it such that $\bSig$ is diagonal.  This follows because for any $\bSig$, we can represent it as $\bSig=\bU \bS \bU$, and there exists a $\bW$ such that
% from the following:
% \begin{align}
% \bSig = \bU \bS \bU =
% \end{align}

\begin{align*}
% \bx\T \bSig^{-1} \bdel &=
(\bW \bx) \T  (\bW \bSig \bW\T )^{-1} \bW \bdel  %& \text{from Lemma \ref{l:rot}}\\
&= \bx\T \bW\T  (\bW \bU \bS \bU\T \bW\T)^{-1} \bW \bdel & \text{by substituting $\bU \bS \bU\T$ for $\bSig$} \\
&= \bx\T \bW\T  (\mt{\bU} \bS \mt{\bU}\T)^{-1} \bW \bdel & \text{by letting $\mt{\bU}=\bW \bU$} \\
&= \bx\T \bW\T  (\mt{\bU} \bS^{-1} \mt{\bU}\T) \bW \bdel & \text{by the laws of matrix inverse} \\
&= \bx\T \bW\T  \bW \bU \bS^{-1}  \bU\T \bW\T \bW \bdel & \text{by un-substituting $\bW \bU=\mt{\bU}$} \\
&= \bx\T  \bU \bS^{-1}  \bU\T  \bdel  & \text{because $\bW\T \bW = \bI$} \\
&= \bx\T   \bSig^{-1} \bdel & \text{by un-substituting $\bU \bS^{-1} \bU\T = \bSig$}
\end{align*}
\end{proof}

One implication of this lemma is that we can reparameterize without loss of generality.  Specifically, defining $\bW := \bU\T$ yields a change of variables: $\bSig \mapsto \bS$ and $\bdel \mapsto \bU\T \bdel := \bdel''$, where $\bS$ is a diagonal covariance matrix.  Moreover, let $\bd=(\sigma_1,\ldots, \sigma_D)\T$ be the vector of eignevalues, then $\bS^{-1} {\bdel'}=\bd^{-1} \odot \mt{\bdel}$, where $\odot$ is the Hadamard (entrywise) product.  The \Lda~classifier may therefore be encoded by a unit vector, $\mt{\bd}:= \frac{1}{m} \bd^{-1} \odot \mt{\bdel'}$, and its magnitude, $m:=\norm{\bd^{-1} \odot \mt{\bdel}}$.
This will be useful later.




\subsection[]{Rotation of Projection Based Linear Classifiers $g_A$}

By a similar argument as above, one can easily show that:

\begin{align*}
(\bA  \bW \bx) \T  (\bA \bW  \bSig  \bW\T \bA\T)^{-1} \bA \bW \bdel
&= \bx\T (\bW\T \bA\T) (\bA \bW) \bSig^{-1} (\bW\T \bA\T) (\bA \bW) \bdel \\
&= \bx\T \bY\T \bY \bSig^{-1} \bY\T \bY \bdel \\
&= \bx\T \bZ \bSig^{-1} \bZ\T \bdel \\
&= \bx\T (\bZ \bSig \bZ\T)^{-1} \bdel = \bx\T \mt{\bSig}_d^{-1} \bdel,
% (\bA\T \bA \bx) \T  \bSig^{-1} \bA\T \bA \bdel = (\bA \bx)\T \bSig^{-1}_A \bdel_A.
\end{align*}
% \end{proof}
where $\bY = \bA \bW \in \Real^{d \times p}$ so that $\bZ=\bY\T \bY$ is a symmetric ${p \times p}$ matrix of rank $d$.  In other words, rotating and then projecting is equivalent to a change of basis.
The implications of the above is:
\begin{lem}
$g_A$ is rotationally invariant if and only if span($\bA$)=span($\bSig_d$).
In other words, \Pca~is the only rotationally invariant projection.
\end{lem}

\subsection{Chernoff information}
We now introduce the notion of the Chernoff information, which serves as our surrogate measure for the Bayes error of any classification procedure given the {\em projected} data -- in the context of this paper the projection is via \Lol~or PCA. Our discussion of the Chernoff information is under the context of decision rules for hypothesis testing, nevertheless, as evidenced by the fact that the Maximum A Posterior decision rule -- equivalently the Bayes classifier -- achieves the Chernoff information rate, this distinction between hypothesis testing and classification is mainly for ease of exposition.

Let $F_0$ and $F_1$ be two absolutely continuous multivariate distribution in $\Omega \subset \mathbb{R}^{d}$ with density function $f_0$ and $f_1$, respectively. Suppose that $Y_1, Y_2, \dots, Y_m$ are independent and identically distributed random variables, with $Y_i$ distributed either $F_0$ or $F_1$. We are interested in testing the simple null hypothesis $\mathbb{H}_0 \colon F = F_0$ against the simple alternative hypothesis $\mathbb{H}_1 \colon F = F_1$. A test $T$ is a sequence of mapping $T_m \colon \Omega^{m} \mapsto \{0,1\}$ such that given $Y_1 = y_1, Y_2 = y_2, \dots, Y_m = y_m$, the test rejects $\mathbb{H}_0$ in favor of $\mathbb{H}_1$ if $T_m(y_1, y_2, \dots, y_m) = 1$; similarly, the test rejects $\mathbb{H}_1$ in favor of $\mathbb{H}_0$ if $T_m(y_1, y_2, \dots, y_m) = 0$.
The Neyman-Pearson lemma states that, given $Y_1 = y_1, Y_2 = y_2, \dots, Y_m = y_m$ and a threshold $\eta_m \in \mathbb{R}$, the likelihood ratio test which rejects $\mathbb{H}_0$ in favor of $\mathbb{H}_1$ whenever
$$ \Bigl(\sum_{i=1}^{m} \log{f_0(y_i)} - \sum_{i=1}^{m} \log{f_1(y_i)} \Bigr) \leq \eta_m $$
is the most powerful test at significance level $\alpha_m = \alpha(\eta_m)$, i.e., the likelihood ratio test minimizes the type-II error $\beta_m$ subject to the contrainst that the type-I error is at most $\alpha_m$.

Assuming that $\pi \in (0,1)$ is a prior probability that $\mathbb{H}_0$ is true. Then, for a given $\alpha_m^{*} \in (0,1)$, let $\beta_m^{*} = \beta_m^{*}(\alpha_m^{*})$ be the type-II error associated with the likelihood ratio test when the type-I error is at most $\alpha_m^{*}$. The quantity $\inf_{\alpha_m^{*} \in (0,1)} \pi \alpha_m^{*} + (1 - \pi) \beta_m^{*}$ is then the Bayes risk in deciding between $\mathbb{H}_0$ and $\mathbb{H}_1$ given the $m$ independent random variables $Y_1, Y_2, \dots, Y_m$. A classical result of Chernoff \cite{chernoff_1952} states that the Bayes risk is intrinsically linked to a quantity known as the {\em Chernoff information}. More specifically, let $C(F_0, F_1)$ be the quantity
\begin{equation}
\label{eq:chernoff-defn}
\begin{split} C(F_0, F_1) & = - \log \, \Bigl[\, \inf_{t \in (0,1)} \int_{\mathbb{R}^{d}} f_0^{t}(\bm{x}) f_1^{1-t}(\bm{x}) \mathrm{d}\bm{x} \Bigr] \\
&= \sup_{t \in (0,1)} \Bigl[ - \log \int_{\mathbb{R}^{d}} f_0^{t}(\bm{x}) f_1^{1-t}(\bm{x}) \mathrm{d}\bm{x} \Bigr]
\end{split}
\end{equation}
Then we have
\begin{equation}
\label{eq:chernoff-binary}
\begin{split}
\lim_{m \rightarrow \infty} \frac{1}{m} \inf_{\alpha_m^{*} \in (0,1)} \log( \pi \alpha_m^{*} + (1 - \pi) \beta_m^{*}) & = - \, C(F_0, F_1).
\end{split}
\end{equation}
Thus $C(F_0, F_1)$ is the {\em exponential} rate at which the Bayes error $\inf_{\alpha_m^{*} \in (0,1)} \pi \alpha_m^{*} + (1 - \pi) \beta_m^{*}$ decreases as $m \rightarrow \infty$; we also note that the $C(F_0, F_1)$ is independent of $\pi$. We also define, for a given $t \in (0,1)$ the Chernoff divergence $C_t(F_0, F_1) $ between $F_0$ and $F_1$ by
$$ C_{t}(F_0,F_1) = - \log \int_{\mathbb{R}^{d}} f_0^{t}(\bm{x}) f_1^{1-t}(\bm{x}) \mathrm{d}\bm{x}. $$
The Chernoff divergence is an example of a $f$-divergence as defined in \cite{Csizar}. When $t = 1/2$, $C_t(F_0,F_1)$ is the Bhattacharyya distance between $F_0$ and $F_1$.

The result of Eq.~\eqref{eq:chernoff-binary} can be extended to $K + 1 \geq 2$ hypothesis, with the exponential rate being the minimum of the Chernoff information between any pair of hypothesis. More specifically, let $F_0, F_1, \dots, F_{K}$ be distributions on $\mathbb{R}^{d}$ and let $Y_1, Y_2, \dots, Y_m$ be independent and identically distributed random variables with distribution $F \in \{F_0, F_1, \dots, F_K\}$. Our inference task is in determining the distribution of the $Y_i$ among the $K+1$ hypothesis $\mathbb{H}_0 \colon F = F_0, \dots, \mathbb{H}_{K} \colon F = F_K$. Suppose also that hypothesis $\mathbb{H}_k$ has {\em a priori} probabibility $\pi_k$. For any decision rule $\delta$, the risk of $\delta$ is $r(\delta) = \sum_{k} \pi_k \sum_{l \not = k} \alpha_{lk}(\delta) $ where $\alpha_{lk}(\delta)$ is the probability of accepting hypothesis $\mathbb{H}_l$ when hypothesis $\mathbb{H}_k$ is true. Then we have \cite{leang-johnson}
\begin{equation}
\label{eq:chernoff-multiple}
\inf_{\delta} \lim_{m \rightarrow \infty}  \frac{r(\delta)}{m} = - \min_{k \not = l} C(F_k, F_l).
\end{equation}
where the infimum is over all decision rule $\delta$, i.e., for any $\delta$, $r(\delta)$ decreases to $0$ as $m \rightarrow \infty$ at a rate no faster than $\exp(- m \min_{k \not = l} C(F_k, F_l))$.
%It was also shown in \cite{leang-johnson} that the {\em Maximum A Posterior} decision rule achieves this rate.

When the distributions $F_0$ and $F_1$ are multivariate normal, that is, $F_0 =  \mathcal{N}(\mu_0, \Sigma_0)$ and $F_1 = \mathcal{N}(\mu_1, \Sigma_1)$; then, denoting by $\Sigma_t = t \Sigma_0 + (1 - t) \Sigma_1$, we have
\begin{equation*}
C(F_0, F_1) = \sup_{t \in (0,1)} \Bigl(\frac{t(1 - t)}{2} (\mu_1 - \mu_2)^{\top}\Sigma_t^{-1}(\mu_1 - \mu_2) + \frac{1}{2} \log \frac{|\Sigma_t|}{|\Sigma_0|^{t} |\Sigma_1|^{1 - t}}  \Bigr).
\end{equation*}

\subsection{Projecting data and Chernoff information}
We now discuss how the Chernoff information characterizes the effect a linear transformation $A$ of the data has on classification accuracy.
We start with the following simple result whose proof follows directly from Eq.~\eqref{eq:chernoff-multiple}.
\begin{lem}
\label{lem:chernoff-1}
Let $F_0 = \mathcal{N}(\mu_0, \Sigma)$ and $F_1 \sim \mathcal{N}(\mu_1, \Sigma)$ be two multivariate normals with equal covariance matrices. For any linear transformation $A$, let $F_0^{(A)}$ and $F_1^{(A)}$ denotes the distribution of $AX$ when $X \sim F_0$ and $X \sim F_1$, respectively. We then have
\begin{equation}
\begin{split}
C(F_0^{(A)}, F_1^{(A)}) &= \frac{1}{8} (\mu_1 - \mu_0)^{\top} A^{\top} (A \Sigma A^{\top})^{-1} A (\mu_1 - \mu_0) \\ & = \frac{1}{8} (\mu_1 - \mu_0)^{\top} \Sigma^{-1/2} \Sigma^{1/2} A^{\top} (A \Sigma A^{\top})^{-1} A \Sigma^{1/2} \Sigma^{-1/2} (\mu_1 - \mu_0) \\
&= \frac{1}{8} \|P_{\Sigma^{1/2} A^{\top}} \Sigma^{-1/2} (\mu_1 - \mu_0) \|_{F}^{2}
\end{split}
\end{equation}
where $P_{Z} = Z(Z^{\top} Z)^{-1} Z^{\top}$ denotes the matrix corresponding to the orthogonal projection onto the columns of $Z$.
\end{lem}

Thus for a classification problem where $X | Y = 0$ and $X | Y = 1$ are distributed multivariate normals with mean $\mu_0$ and $\mu_1$ and the same covariance matrix $\Sigma$, Lemma~\ref{lem:chernoff-1} then states that for any two linear transformations $A$ and $B$, the transformed data $AX$ is to be preferred over the transformed data $BX$ if
$$ (\mu_1 - \mu_0)^{\top} A^{\top} (A \Sigma A^{\top})^{-1} A (\mu_1 - \mu_0) > (\mu_1 - \mu_0)^{\top} B^{\top} (B \Sigma B^{\top})^{-1} B (\mu_1 - \mu_0). $$

As an example, suppose $\Sigma$ is diagonal with distinct eigenvalues where the diagonal entries of $\Sigma$ are in non-increasing order.
Denote by $\delta = \mu_1 - \mu_0$ and let
$A = \delta^{\top}$ and $B = e_1^{\top} = (1,0,0,\dots,0)$ be the linear transformations for \Lol~and PCA of $X$ into $\mathbb{R}$. We then have
\begin{gather*}
C(F_0^{(A)}, F_1^{(A)}) = \frac{(\delta^{\top} \delta)^2}{\delta^{\top} \Sigma \delta} = \frac{(\sum_{i} \delta_i^{2})^{2}}{\sum_{i} \delta_i^{2} \lambda_i}; \quad
C(F_0^{(B)}, F_1^{(B)}) = \frac{\delta_1^2}{\lambda_1}
\end{gather*}
where $\lambda_1$ is the largest eigenvalue of $\Sigma$. Suppose furthermore that $\delta_1 \leq \delta_2 \leq \dots \leq \delta_p$ and $\lambda_1 > \lambda_2 > \dots > \lambda_p$. Then $C(F_0^{(A)}, F_1^{(A)})$ can be lower-bounded as
$$ C(F_0^{(A)}, F_1^{(A)}) = \frac{(\sum_{i} \delta_i^{2})^{2}}{\sum_{i} \delta_i^{2} \lambda_i} \geq \frac{(p \delta_1^2)^2}{p \delta_p^{2} \lambda_1} $$
and hence $C(F_0^{(A)}, F_1^{(A)}) > C(F_0^{(B)}, F_1^{(B)})$ provided $p \delta_1^{2}  \geq \delta_p^2$.

When $A = \bigl[ \delta \mid e_1 \mid e_2 \dots \mid e_{d-1} \bigr]^{\top} \in \mathbb{R}^{d \times p}$ and $B = \bigl[e_1 \mid e_2 \mid \dots \mid e_d \bigr]^{\top} \in \mathbb{R}^{d \times p}$ are the linear transformation for \Lol~and PCA of $X$ into $\mathbb{R}^{d}$, we have
\begin{gather*}
C(F_0^{(A)}, F_1^{(A)}) = \frac{(\sum_{i=d}^{p} \delta_i^2)^2}{\sum_{i=d}^{p} \delta_i^2 \lambda_i} + \sum_{i=1}^{d-1} \frac{\delta_i^{2}}{\lambda_i};  \quad
C(F_0^{(B)}, F_1^{(B)}) = \sum_{i=1}^{d} \frac{\delta_i^2}{\lambda_i}.
\end{gather*}

This can be seen as follows. Let $\xi_{d-1} = \bigl[e_1 \mid e_2 \mid \dots \mid e_{d-1} \bigr] \in \mathbb{R}^{p \times (d-1)}$ and $\zeta_{d-1} = (\lambda_1 \delta_1, \lambda_2 \delta_2, \dots, \lambda_{d-1} \delta_{d-1})^{\top} \in \mathbb{R}^{d-1}$. Then
\begin{equation}
A \Sigma A^{\top} = \bigl[ \delta \mid \xi_{d-1} \bigr]^{\top} \, \Sigma \, \bigl[ \delta \mid \xi_{d-1} \bigr] = \begin{bmatrix} \delta^{\top} \Sigma \delta & \delta^{\top} \Sigma \xi_{d-1} \\ \xi_{d-1}^{\top} \Sigma \delta & \xi_{d-1}^{\top} \Sigma \xi_{d-1} \end{bmatrix} =
\begin{bmatrix} \sum_{i=1}^{p} \delta_i^{2} \lambda_i & \zeta_{d-1}^{\top} \\ \zeta_{d-1} & \Sigma_{d-1} \end{bmatrix}
\end{equation}
where $\Sigma_{d-1} = \mathrm{diag}(\lambda_1, \lambda_2, \dots, \lambda_{d-1})$ is the submatrix of $\Sigma$ corresponding to the eigenvalues $\lambda_1, \lambda_2, \dots, \lambda_{d-1}$. Using a formula for the inverse of a partitioned matrix, we have
\begin{equation}
\begin{split}
(A \Sigma A^{\top})^{-1} &= \begin{bmatrix} \sum_{i=1}^{p} \delta_i^{2} \lambda_i & \zeta_{d-1}^{\top} \\ \zeta_{d-1} & \Sigma_{d-1} \end{bmatrix}^{-1}
\\ & = \begin{bmatrix} \bigl(\sum_{i=1}^{p} \delta_i^{2} \lambda_i - \zeta_{d-1}^{\top} \Sigma_{d-1}^{-1} \zeta_{d-1}\bigr)^{-1} &
- \bigl(\sum_{i} \delta_i^{2} \lambda_i - \zeta_{d-1}^{\top} \Sigma_{d-1}^{-1} \zeta_{d-1}\bigr)^{-1} \zeta_{d-1}^{\top} \Sigma_{d-1}^{-1}
\\ - \Sigma_{d-1}^{-1} \zeta_{d-1} \bigl(\sum_{i} \delta_i^{2} \lambda_i - \zeta_{d-1}^{\top} \Sigma_{d-1}^{-1} \zeta_{d-1}\bigr)^{-1}
& \Sigma_{d-1}^{-1} + \Sigma_{d-1}^{-1} \zeta_{d-1} \bigl(\sum_{i} \delta_i^{2} \lambda_i - \zeta_{d-1}^{\top} \Sigma_{d-1}^{-1} \zeta_{d-1}\bigr)^{-1} \zeta_{d-1}^{\top} \Sigma_{d-1}^{-1}
\end{bmatrix}
\end{split}
\end{equation}
 Now, $\sum_{i=1}^{p} \delta_i^{2} \lambda_i - \zeta_{d-1}^{\top} \Sigma_{d-1}^{-1} \zeta_{d-1} = \sum_{i=1}^{p} \delta_i^{2} - \sum_{i=1}^{d-1} \delta_i^{2} \lambda_i = \sum_{i=d}^{p} \delta_i^{2} \lambda_i$.
%In addition, by the Sherman-Morrison-Woodbury formula, we have
% \begin{equation*}
% \begin{split}
% (\Sigma_{d-1} - \frac{\zeta_{d-1} \zeta_{d-1}^{\top}}{\sum_{i=1}^{p} \delta_i^{2} \lambda_i}\bigr)^{-1} &= \Sigma_{d-1}^{-1} + \frac{\Sigma_{d-1}^{-1} \zeta_{d-1} \zeta_{d-1}^{\top} \Sigma_{d-1}^{-1}/ \bigl(\sum_{i=1}^{p} \delta_i^{2} \lambda_i\bigr)}{1 - \zeta_{d-1}^{\top} \Sigma_{d-1}^{-1} \zeta_{d-1}/\bigl(\sum_{i=1}^{p} \delta_i^{2} \lambda_i\bigr)} \\
% &= \Sigma_{d-1}^{-1} + \frac{\Sigma_{d-1}^{-1} \zeta_{d-1} \zeta_{d-1}^{\top} \Sigma_{d-1}^{-1}}{\sum_{i=1}^{p} \delta_i^{2} \lambda_i - \zeta_{d-1}^{\top} \Sigma_{d-1}^{-1} \zeta_{d-1}} \\
% &= \Sigma_{d-1}^{-1} + \frac{\Sigma_{d-1}^{-1} \zeta_{d-1} \zeta_{d-1}^{\top} \Sigma_{d-1}^{-1}}{\sum_{i=1}^{p} \delta_i^{2} \lambda_i - \sum_{i=1}^{d-1} \delta_i^{2} \lambda_i} = \Sigma_{d-1}^{-1} + \Sigma_{d-1}^{-1} \zeta_{d-1} (\sum_{i} \delta_i^{2} \lambda_i - \zeta_{d-1}^{\top} \Sigma_{d-1}^{-1} \zeta_{d-1}\bigr)^{-1} \zeta_{d-1}^{\top} \Sigma_{d-1}^{-1}}.
% \end{split}
% \end{equation*}
Therefore,
% \begin{equation*}
% \begin{split}
% \bigl(\sum_{i=1}^{p} \delta_i^{2} \lambda_i\bigr)^{-1} \zeta_{d-1}^{\top} \bigl(\Sigma_{d-1}^{-1} + \frac{\Sigma_{d-1}^{-1} \zeta_{d-1} \zeta_{d-1}^{\top} \Sigma_{d-1}^{-1}}{\sum_{i=d}^{p} \delta_i^{2} \lambda_i} \bigr) &= \bigl(\sum_{i=1}^{p} \delta_i^{2} \lambda_i\bigr)^{-1} \Bigl(\zeta_{d-1}^{\top} \Sigma_{d-1}^{-1} + \frac{\zeta_{d-1}^{\top} \Sigma_{d-1}^{-1} \zeta_{d-1} \zeta_{d-1}^{\top} \Sigma_{d-1}^{-1}}{\sum_{i=d}^{p} \delta_i^{2} \lambda_i} \Bigr) \\
% &= \bigl(\sum_{i=1}^{p} \delta_i^{2} \lambda_i\bigr)^{-1} \Bigl(\zeta_{d-1}^{\top} \Sigma_{d-1}^{-1} + \frac{(\sum_{i=1}^{d-1} \delta_i^{2} \lambda_i)
% \zeta_{d-1}^{\top} \Sigma_{d-1}^{-1}}{\sum_{i=d}^{p} \delta_i^{2} \lambda_i}\Bigr) \\
% %&= \bigl(\sum_{i=1}^{p} \delta_i^{2} \lambda_i\bigr)^{-1} \zeta_{d-1}^{\top} \Sigma_{d-1}^{-1} \frac{\sum_{i=d}^{p} \delta_i^{2} \lambda_i + \sum_{i=1}^{d-1} \delta_i^{2} \lambda_i}{\sum_{i=d}^{p} \delta_i^{2} \lambda_i} \\
% & = \frac{\zeta_{d-1}^{\top} \Sigma_{d-1}^{-1}}{\sum_{i=d}^{p} \delta_i^{2} \lambda_i}.
% \end{split}
% \end{equation*}

% Combining the above, we have
\begin{equation}
\begin{split}
(A \Sigma A^{\top})^{-1} &= \begin{bmatrix} \bigl(\sum_{i=d}^{p} \delta_i^{2} \lambda_i\bigr)^{-1} & - \frac{\zeta_{d-1}^{\top} \Sigma_{d-1}^{-1}}{\sum_{i=d}^{p} \delta_i^{2} \lambda_i} \\
 - \frac{\Sigma_{d-1}^{-1} \zeta_{d-1}}{\sum_{i=d}^{p} \delta_i^{2} \lambda_i}
  & \Sigma_{d-1}^{-1} + \frac{\Sigma_{d-1}^{-1} \zeta_{d-1} \zeta_{d-1}^{\top} \Sigma_{d-1}^{-1}}{\sum_{i=d}^{p} \delta_i^{2} \lambda_i}
\end{bmatrix}.
\end{split}
\end{equation}
In addition, $A(\mu_1 - \mu_0) = (\delta^{\top} \delta, \delta_1, \delta_2, \dots, \delta_{d-1})^{\top} = (\delta^{\top} \delta, \zeta_{d-1}^{\top} \Sigma_{d-1}^{-1})^{\top} \in \mathbb{R}^{d}$. Hence
\begin{equation*}
\begin{split}
(\mu_1 - \mu_0)^{\top} A^{\top} (A \Sigma A^{\top})^{-1} A (\mu_1 - \mu_0) &=
\bigl[\delta^{\top} \delta \mid \zeta_{d-1}^{\top} \Sigma_{d-1}^{-1} \bigr]
\begin{bmatrix} \bigl(\sum_{i=d}^{p} \delta_i^{2} \lambda_i\bigr)^{-1} & - \frac{\zeta_{d-1}^{\top} \Sigma_{d-1}^{-1}}{\sum_{i=d}^{p} \delta_i^{2} \lambda_i} \\
 - \frac{\Sigma_{d-1}^{-1} \zeta_{d-1}}{\sum_{i=d}^{p} \delta_i^{2} \lambda_i}
  & \Sigma_{d-1}^{-1} + \frac{\Sigma_{d-1}^{-1} \zeta_{d-1} \zeta_{d-1}^{\top} \Sigma_{d-1}^{-1}}{\sum_{i=d}^{p} \delta_i^{2} \lambda_i} \end{bmatrix} \begin{bmatrix} \delta^{\top} \delta \\ \Sigma_{d-1}^{-1} \zeta_{d-1} \end{bmatrix} \\
  &= \frac{(\delta^{\top} \delta)^{2}}{\sum_{i=d}^{p} \delta_i^{2} \lambda_i} - 2 \delta^{\top} \delta \frac{\zeta_{d-1}^{\top} \Sigma_{d-1}^{-2} \zeta_{d-1}}{\sum_{i=d}^{p} \delta_i^2 \lambda_i} + \Bigl(\zeta_{d-1}^{\top} \Sigma_{d-1}^{-3} \zeta_{d-1} + \frac{(\zeta_{d-1}^{\top} \Sigma_{d-1}^{-2} \zeta_{d-1})^{2}}{\sum_{i=d}^{p} \delta_i^{2} \lambda_i} \Bigr) \\
  &= \frac{(\delta^{\top} \delta - \zeta_{d-1}^{\top} \Sigma_{d-1}^{-2} \zeta_{d-1})^{2}}{\sum_{i=d}^{p} \delta_i^{2} \lambda_i} + \zeta_{d-1}^{\top} \Sigma_{d-1}^{-3} \zeta_{d-1} \\
  &= \frac{\bigl(\sum_{i=1}^{p} \delta_i^{2} - \sum_{i=1}^{d-1} \delta_i^{2}\bigr)^{2}}{\sum_{i=d}^{p} \delta_i^{2} \lambda_i} + \sum_{i=1}^{d-1} \frac{\delta_i^{2}}{\lambda_i} = \frac{\bigl(\sum_{i=d}^{p} \delta_i^{2}\bigr)^{2}}{\sum_{i=d}^{p} \delta_i^{2} \lambda_i} + \sum_{i=1}^{d-1} \frac{\delta_i^{2}}{\lambda_i}.
\end{split}
\end{equation*}
The derivation of $C(F_0^{(B)}, F_1^{(B)})$ is straightforward and will be omitted.
We therefore have
$$C(F_0^{(A)}, F_1^{(A)}) - C(F_0^{(B)}, F_1^{(B)}) = \frac{\bigl(\sum_{i=d}^{p} \delta_i^{2}\bigr)^{2}}{\sum_{i=d}^{p} \delta_i^{2} \lambda_i} - \frac{\delta_{d}^{2}}{\lambda_d}.$$
From our assumption that $\lambda_d \geq \lambda_{d+1} \geq \dots \geq \lambda_{p}$, we have $\sum_{i=d}^{p} \delta_i^{2} \lambda_i \leq \lambda_{d} \sum_{i=d}^{p} \delta_{i}^{2}$
and hence
$$ \frac{\bigl(\sum_{i=d}^{p} \delta_i^{2}\bigr)^{2}}{\sum_{i=d}^{p} \delta_i^{2} \lambda_i} \geq \frac{\bigl(\sum_{i=d}^{p} \delta_{i}^{2}\bigr)^{2}}{\lambda_d \sum_{i=d}^{p} \delta_i^{2}} = \frac{1}{\lambda_d} \sum_{i=d}^{p} \delta_i^{2} \geq \frac{\delta_{d}^2}{\lambda_d} $$
and hence $C(F_0^{(A)}, F_1^{(A)}) \geq C(F_0^{(B)}, F_1^{(B)})$ always, and the inequality is strict provided that $\sum_{d+1}^{p} \delta_d^2 > 0$.

Finally we consider the case where $\Sigma$ is an arbitrary covariance matrix. Let $\lambda_1 \geq \lambda_2 \geq \dots \geq \lambda_{p}$ be the eigenvalues of $\Sigma$ and let $u_1, u_2, \dots, u_p$ be the corresponding eigenvectors. For $d \leq p$,  let $U_{d-1} = \bigl[ u_1 \mid u_2 \mid \dots \mid u_{d-1}] \in \mathbb{R}^{p \times (d-1)}$ be the matrix whose columns are the eigenvectors $u_1, u_2, \dots, u_{d-1}$ of $\Sigma$. Then the \Lol~projection matrix into $\mathbb{R}^{d}$ is given by $A = \bigl[ \delta | U_{d-1} \bigr]^{\top}$. We first have
\begin{equation*}
A \Sigma A^{\top} = \bigl[ \delta | U_{d-1} \bigr]^{\top}\, \Sigma \, \bigl[ \delta | U_{d-1} \bigr] = \begin{bmatrix} \delta^{\top} \Sigma & \delta^{\top} \Sigma U_{d-1} \\ U_{d-1}^{\top} \Sigma \delta & U_{d-1}^{\top} \Sigma U_{d-1} \end{bmatrix} = \begin{bmatrix} \delta^{\top} \Sigma & \delta^{\top} \Sigma U_{d-1} \\ U_{d-1}^{\top} \Sigma \delta & \Lambda_{d-1} \end{bmatrix}
\end{equation*}
where $\Lambda_{d-1} = \mathrm{diag}(\lambda_1, \lambda_2, \dots, \lambda_{d-1})$ is the $(d-1) \times (d-1)$ diagonal matrix formed by the eigenvalues $\lambda_1, \lambda_2, \dots, \lambda_{d-1}$.
Therefore, letting $\gamma = \delta^{\top} \Sigma \delta - \delta^{\top} \Sigma U_{d-1} \Lambda_{d-1}^{-1} U_{d-1}^{\top} \Sigma \delta$, we have
\begin{equation*}
\begin{split}
(A \Sigma A^{\top})^{-1} &= \begin{bmatrix} \delta^{\top} \Sigma \delta & \delta^{\top} \Sigma U_{d-1} \\ U_{d-1}^{\top} \Sigma \delta & U_{d-1}^{\top} \Sigma U_{d-1} \end{bmatrix}^{-1} \\
&= \begin{bmatrix} \gamma^{-1} & - \delta^{\top} \Sigma U_{d-1} \Lambda_{d-1}^{-1} \gamma^{-1} \\
- \Lambda_{d-1}^{-1} U_{d-1}^{\top} \Sigma \delta \gamma^{-1} & \bigl(\Lambda_{d-1} - \frac{U_{d-1}^{\top} \Sigma \delta \delta^{\top} \Sigma U_{d-1}}{\delta^{\top} \Sigma \delta} \bigr)^{-1} \end{bmatrix}.
\end{split}
\end{equation*}
The Sherman-Morrison-Woodbury formula then implies
\begin{equation*}
\begin{split}
\Bigl(\Lambda_{d-1} - \frac{U_{d-1}^{\top} \Sigma \delta \delta^{\top} \Sigma U_{d-1}}{\delta^{\top} \Sigma \delta} \Bigr)^{-1} & = \Lambda_{d-1}^{-1} + \frac{\Lambda_{d-1}^{-1} U_{d-1}^{\top} \Sigma \delta \delta^{\top} \Sigma U_{d-1} \Lambda_{d-1}^{-1} /(\delta^{\top} \Sigma \delta)}{1 - \delta^{\top} \Sigma U_{d-1} \Lambda_{d-1}^{-1} U_{d-1}^{\top} \Sigma \delta/(\delta^{\top} \Sigma \delta)} \\
&= \Lambda_{d-1}^{-1} + \frac{\Lambda_{d-1}^{-1} U_{d-1}^{\top} \Sigma \delta \delta^{\top} \Sigma U_{d-1} \Lambda_{d-1}^{-1}}{\delta^{\top} \Sigma \delta - \delta^{\top} \Sigma U_{d-1} \Lambda_{d-1}^{-1} U_{d-1}^{\top} \Sigma \delta} \\
&= \Lambda_{d-1}^{-1} + \gamma^{-1} \Lambda_{d-1}^{-1} U_{d-1}^{\top} \Sigma \delta \delta^{\top} \Sigma U_{d-1} \Lambda_{d-1}^{-1}
\end{split}
\end{equation*}

We note that $\Sigma U_{d-1} = U_{d-1} \Lambda_{d-1}$ and $U_{d-1}^{\top} \Sigma = \Lambda_{d-1} U_{d-1}^{\top}$ and hence
\begin{equation*}
\begin{split}
 \gamma = \delta^{\top} \Sigma \delta - \delta^{\top} \Sigma U_{d-1} \Lambda_{d-1}^{-1} U_{d-1}^{\top} \Sigma \delta &= \delta^{\top} \Sigma \delta - \delta^{\top} U_{d-1} \Lambda_{d-1} \Lambda_{d-1}^{-1} \Lambda_{d-1} U_{d-1}^{\top} \delta \\ & =  \delta^{\top} \Sigma \delta - \delta^{\top} U_{d-1} \Lambda_{d-1} U_{d-1}^{\top} \delta = \delta^{\top} (\Sigma - \Sigma_{d-1}) \delta
 \end{split}
 \end{equation*}
 where $\Sigma_{d-1} = U_{d-1} \Lambda_{d-1} U_{d-1}^{\top}$ is the best rank $d-1$ approximation to $\Sigma$ with respect to any unitaritly invariant norm. In addition,
 $$ \Lambda_{d-1}^{-1} U_{d-1}^{\top} \Sigma \delta \delta^{\top} \Sigma U_{d-1} \Lambda_{d-1}^{-1} = \Lambda_{d-1}^{-1} \Lambda_{d-1} U_{d-1}^{\top} \delta \delta^{\top} U_{d-1} \Lambda_{d-1} \Lambda_{d-1}^{-1} = U_{d-1}^{\top} \delta \delta^{\top} U_{d-1}.$$
 We thus have
 \begin{equation*}
 (A \Sigma A^{\top})^{-1} = \begin{bmatrix} \gamma^{-1} & - \delta^{\top} \Sigma U_{d-1} \Lambda_{d-1}^{-1} \gamma^{-1} \\
- \Lambda_{d-1}^{-1} U_{d-1}^{\top} \Sigma \delta \gamma^{-1} & \bigl(\Lambda_{d-1} - \frac{U_{d-1}^{\top} \Sigma \delta \delta^{\top} \Sigma U_{d-1}}{\delta^{\top} \Sigma \delta} \bigr)^{-1} \end{bmatrix} = \begin{bmatrix} \gamma^{-1} & - \gamma^{-1} \delta^{\top} U_{d-1} \\ - \gamma^{-1} U_{d-1}^{\top} \delta & \Lambda_{d-1}^{-1} + \gamma^{-1}  U_{d-1}^{\top} \delta \delta^{\top} U_{d-1} \end{bmatrix}.
 \end{equation*}
 Therefore,
 \begin{equation*}
 \begin{split}
 \delta^{\top} A^{\top} (A \Sigma A^{\top})^{-1} A \delta & = \delta^{\top} \bigl[ \delta \mid U_{d-1} \bigr] \begin{bmatrix} \gamma^{-1} & - \gamma^{-1} \delta^{\top} U_{d-1} \\ - \gamma^{-1} U_{d-1}^{\top} \delta & \Lambda_{d-1}^{-1} + \gamma^{-1}  U_{d-1}^{\top} \delta \delta^{\top} U_{d-1} \end{bmatrix} \bigl[ \delta | U_{d-1} \bigr]^{\top} \delta \\
 &= \bigl[\delta^{\top} \delta \mid \delta^{\top} U_{d-1} \bigr] \begin{bmatrix} \gamma^{-1} & - \gamma^{-1} \delta^{\top} U_{d-1} \\ - \gamma^{-1} U_{d-1}^{\top} \delta & \Lambda_{d-1}^{-1} + \gamma^{-1}  U_{d-1}^{\top} \delta \delta^{\top} U_{d-1} \end{bmatrix} \begin{bmatrix} \delta^{\top} \delta \\ U_{d-1}^{\top} \delta \end{bmatrix} \\
 &= \gamma^{-1} (\delta^{\top} \delta)^{2} - 2 \gamma^{-1} \delta^{\top} \delta \delta^{\top} U_{d-1} U_{d-1}^{\top} \delta + \delta^{\top} U_{d-1} (\Lambda_{d-1}^{-1} + \gamma^{-1} U_{d-1}^{\top} \delta \delta^{\top} U_{d-1}) U_{d-1}^{\top} \delta \\
 &= \gamma^{-1} (\delta^{\top} \delta - \delta^{\top} U_{d-1} U_{d-1}^{\top} \delta)^{2} + \delta^{\top} U_{d-1} \Lambda_{d-1}^{-1} U_{d-1}^{\top} \delta \\
 &= \gamma^{-1} (\delta^{\top} (I - U_{d-1} U_{d-1}^{\top}) \delta)^{2} + \delta^{\top} \Sigma_{d-1}^{\dagger} \delta
 \end{split}
 \end{equation*}
 where $\Sigma_{d-1}^{\dagger}$ is the Moore-Penrose pseudo-inverse of $\Sigma_{d-1}$. The PCA projection matrix into $\mathbb{R}^{d}$ is given by $B = U_{d}^{\top}$ and hence
 \begin{equation}
 \delta^{\top} B^{\top} (B \Sigma B^{\top})^{-1} B \delta = \delta^{\top} U_{d} \Lambda_d^{-1} U_d^{\top} \delta = \delta^{\top} \Sigma_{d}^{\dagger} \delta.
 \end{equation}
 We thus have
\begin{equation*}
\begin{split}
C(F_0^{(A)}, F_1^{(A)}) - C(F_0^{(B)}, F_1^{(B)}) &= \gamma^{-1} (\delta^{\top} (I - U_{d-1} U_{d-1}^{\top}) \delta)^{2} - \delta^{\top} (\Sigma_{d}^{\dagger} - \Sigma_{d-1}^{\dagger}) \delta \\ &= \frac{(\delta^{\top} (I - U_{d-1} U_{d-1}^{\top}) \delta)^{2}}{\delta^{\top} (\Sigma - \Sigma_{d-1}) \delta} - \delta^{\top} (\Sigma_{d}^{\dagger} - \Sigma_{d-1}^{\dagger}) \delta \\ &
\geq \frac{(\delta^{\top} (I - U_{d-1} U_{d-1}^{\top}) \delta)^{2}}{\lambda_d \delta^{\top} (I - U_{d-1}U_{d-1}^{\top}) \delta} - \frac{1}{\lambda_{d}} \delta^{\top} u_{d} u_{d}^{\top} \delta
\\ &= \frac{1}{\lambda_d} \delta^{\top} (I - U_{d-1}U_{d-1}) \delta - \frac{1}{\lambda_d} \delta^{\top} (U_{d} U_{d}^{\top} - U_{d-1} U_{d-1}^{\top}) \delta \geq 0
\end{split}
\end{equation*}
where we recall that $u_{d}$ is the $d$-th column of $U_{d}$. Thus $C(F_0^{(A)}, F_1^{(A)}) \geq C(F_0^{(B)}, F_1^{(B)})$ always, and the inequality is strict whenever $\delta^{\top} (I - U_{d} U_{d}^{\top}) \delta > 0$.

 Next we consider the case when $A = \bigl[ \delta | U_{d-1} \bigr]^{\top}$ and $B = U_{d}$ where $U_{d}$ is an arbitrary $p \times d$ matrix with $U_d^{\top} U_d = I$, i.e., $U_d$ has $d$ orthonornal columns, and $U_{d-1}$ is the first $d-1$ columns of $U_d$. A similar derivation to the above yields

 \begin{gather}
 C(F_0^{(A)}, F_1^{(A)}) = \frac{(\delta^{\top} \Sigma^{-1/2} (I - V_{d-1} V_{d-1}^{\top}) \Sigma^{1/2} \delta)^2}{\delta^{\top} \Sigma^{1/2} (I - V_{d-1} V_{d-1}^{\top}) \Sigma^{1/2} \delta} + \delta^{\top} \Sigma^{-1/2} V_{d-1} V_{d-1}^{\top} \Sigma^{-1/2} \delta \\
 C(F_0^{(B)}, F_1^{(B)}) = \delta^{\top} \Sigma^{-1/2} V_d V_d^{\top} \Sigma^{-1/2} \delta
 \end{gather}
 where $V_{d} V_{d-1}^{\top} = \Sigma^{1/2} U_d (U_d^{\top} \Sigma U_d)^{-1} U_d^{\top} \Sigma^{1/2}$ is the orthogonal projection onto the column space of $\Sigma^{1/2} U_d$. Hence $C(F_0^{(A)}, F_1^{(A)}) > C(F_0^{(B)}, F_1^{(B)})$ if and only if
 $$ \frac{(\delta^{\top} \Sigma^{-1/2} (I - V_{d-1} V_{d-1}^{\top}) \Sigma^{1/2} \delta)^2}{\delta^{\top} \Sigma^{1/2} (I - V_{d-1} V_{d-1}^{\top}) \Sigma^{1/2} \delta} > \delta^{\top} \Sigma^{-1/2} (V_d V_d^{\top} - V_{d-1} V_{d-1}^{\top}) \Sigma^{-1/2} \delta. $$


Let $C(F_0^{(A)}, F_1^{(A)})$ and $C(F_0^{(B)}, F_1^{(B)})$ be the Chernoff informations when $A = [\delta \mid U_{d-1}]^{\top}$ and $B = [\delta \mid U_d]^{\top}$ where $U_{d-1}$ and $U_d$ contain the eignevectors of the population covariance matrices $\Sigma$; similarly, let $\hat{C}(F_0^{(A)}, F_1^{(A)})$ and $\hat{C}(F_0^{(B)}, F_1^{(B)})$ be the Chernoff informations when $A = [\delta \mid \hat{U}_{d-1}]$ and $B = [\delta \mid \hat{U}_d]$ where $\hat{U}_{d-1}$ and $\hat{U}_d$ contain the eigenvectors of the {\emph sample} covariance matrices $\hat{\Sigma}$. Suppose furthermore that $C(F_0^{(A)}, F_1^{(A)}) > C(F_0^{(B)}, F_1^{(B)})$. Then for sufficiently large $n$, with high probability, $\hat{C}(F_0^{(A)}, F_1^{(A)}) > \hat{C}(F_0^{(B)}, F_1^{(B)})$.

Finally we consider the case when $B = \tilde{U}_d^{\top}$ where $\tilde{U}_d$ is the $p \times d$ matrix whose columns are the $d$ largest eigenvectors of the {\em pooled} covariance matrix $\tilde{\Sigma} = \mathbb{E}[(X - \tfrac{\mu_0 + \mu_1}{2})(X - \tfrac{\mu_0 + \mu_1}{2})^{\top}]$. Assume, without loss of generality, that $\mu_1 = -\mu_0 = \mu$. We then have

$$\tilde{\Sigma} = \mathbb{E}[X X^{\top}] = \pi \Sigma + \pi \mu_0 \mu_0^{\top} + (1 - \pi) \Sigma + (1 - \pi) \mu_1 \mu_1^{\top} = \Sigma + \mu \mu^{\top} = \Sigma + \tfrac{1}{4} \delta \delta^{\top}. $$

Therefore
$$ (B \Sigma B^{\top})^{-1} = \bigl(\tilde{U}_{d}^{\top} \Sigma \tilde{U}_d \bigr)^{-1} = \bigl( \tilde{U}_d^{\top} (\tilde{\Sigma} - \tfrac{1}{4} \delta \delta^{\top}) \tilde{U}_d\bigr)^{-1} = \bigl(\tilde{S}_d - \tfrac{1}{4} \tilde{U}_d^{\top} \delta \delta^{\top} \tilde{U}_d \bigr)^{-1} = \tilde{S}_d^{-1} + \frac{\tilde{S}_d^{-1} \tilde{U}_d^{\top} \delta \delta^{\top} \tilde{U}_d \tilde{S}_d^{-1}}{4 - \delta^{\top} \tilde{U}_d \tilde{S}_d^{-1} \tilde{U}_d^{\top} \delta} $$
where $\tilde{S}_d$ is the diagonal matrix containing the $d$ largest eigenvalues of $\tilde{\Sigma}$. Hence
\begin{equation}
\begin{split}
C(F_0^{(B)}, F_1^{(B)}) = \delta^{\top} B^{\top} (B \Sigma B^{\top})^{-1} B \delta & = \delta^{\top} \tilde{U}_d \Bigl(\tilde{S}_d^{-1} + \frac{\tilde{S}_d^{-1} \tilde{U}_d^{\top} \delta \delta^{\top} \tilde{U}_d \tilde{S}_d^{-1}}{4 - \delta^{\top} \tilde{U}_d \tilde{S}_d^{-1} \tilde{U}_d^{\top} \delta} \Bigr) \tilde{U}_d^{\top} \delta \\ &= \delta^{\top} \tilde{U}_d \tilde{S}_d^{-1} \tilde{U}_d^{\top} \delta + \frac{(\delta^{\top} \tilde{U}_d \tilde{S}_d^{-1} \tilde{U}_d^{\top} \delta)^{2}}{4 - \delta^{\top} \tilde{U}_d \tilde{S}_d^{-1} \tilde{U}_d^{\top} \delta} \\
&= \delta^{\top} \tilde{\Sigma}_d^{\dagger} \delta + \frac{(\delta^{\top} \tilde{\Sigma}_d^{\dagger} \delta)^2}{4 - \delta^{\top} \tilde{\Sigma}_d^{\dagger} \delta} = \frac{4 \delta^{\top} \tilde{\Sigma}_d^{\dagger} \delta}{4 - \delta^{\top} \tilde{\Sigma}_d^{\dagger} \delta}.
\end{split}
\end{equation}
where $\tilde{\Sigma}_d = \tilde{U}_d \tilde{S}_d \tilde{U}_d^{\top}$ is the best rank $d$ approximation to $\tilde{\Sigma} = \Sigma + \tfrac{1}{4} \delta \delta^{\top}$.

We recall that the \Lol~projection $A = [\delta \mid U_{d-1}]^{\top}$ yields
$$ C(F_0^{(A)}, F_1^{(A)}) = \frac{(\delta^{\top} (I - U_{d-1} U_{d-1}^{\top}) \delta)^{2}}{\delta^{\top} (\Sigma - \Sigma_{d-1}) \delta} + \delta^{\top} \Sigma_{d-1}^{\dagger} \delta. $$

To illustrate the difference between the \Lol~projection and that based on the eigenvectors of the {\em pooled} covariance matrix, consider the following simple example. Let $\Sigma = \mathrm{diag}(\lambda_1, \lambda_2, \dots, \lambda_p)$ be a diagonal matrix with $\lambda_1 \geq \lambda_2 \geq \dots \geq \lambda_p$. Also let $\delta = (0,0,\dots,0,s)$. Suppose furthermore that $\lambda_p + s^{2}/4 < \lambda_d$. Then we have $\tilde{\Sigma}_d = \mathrm{diag}(\lambda_1, \lambda_2, \dots, \lambda_d, 0, 0, \dots, 0)$. Thus $\tilde{\Sigma}_d^{\dagger} = \mathrm{diag}(1/\lambda_1, 1/\lambda_2, \dots, 1/\lambda_d, 0,0, \dots, 0)$ and $\delta^{\dagger} \tilde{\Sigma}_d^{\dagger} \delta = 0$. Therefore, $C(F_0^{(B)}, F_1^{(B)}) = 0$.

On the other hand, we have
$$ C(F_0^{(A)}, F_1^{(A)}) = \frac{(\delta^{\top} (I - U_{d-1} U_{d-1}^{\top}) \delta)^{2}}{\delta^{\top} (\Sigma - \Sigma_{d-1}) \delta} + \delta^{\top} \Sigma_{d-1}^{\dagger} \delta = \frac{s^4}{s^2 \lambda_p} + 0 = s^{2}/\lambda_p.$$


We can generalize the previous example as follows. Let $\Sigma$ be a $p \times p$ covariance matrix of the form
$$ \Sigma = \begin{bmatrix} \Sigma_{d} & 0 \\ 0 & \Sigma_{d}^{\perp} \end{bmatrix} $$ where $\Sigma_{d}$ is a $d \times d$ matrix.
Let $\lambda_1 \geq \lambda_2 \geq \dots \geq \lambda_p$ be the eigenvalues of $\Sigma$ and suppose furthermore that the eigenvalues of $\Sigma_{d}$ are $\lambda_1, \lambda_2, \dots, \lambda_d$. Let $\gamma = \lambda_{d} - \lambda_{d+1}$.
We now assume that $\delta$ are generated randomly as follows. The entries of $\delta$ are i.i.d. random variable sampled according to the following distribution. Given an index $i$, with probability $\epsilon$, $\delta_i = 0$ and with probability $1 - \epsilon$, $\delta_i$ is distributed according to a normal distribution with mean $\tau > 0$ and variance $\sigma^2$. Then with probability at least $\epsilon^{d}$, the covariance matrix for $\tilde{\Sigma}$ is of the form
$$ \tilde{\Sigma} = \begin{bmatrix} \Sigma_{d} & 0 \\ 0 & \Sigma_{d}^{\perp} + \tfrac{1}{4} (\tilde{\delta} \tilde{\delta}^{\top}) \end{bmatrix} $$
where $\tilde{\delta} \in \mathbb{R}^{p - d}$ is formed by excluding the first $d$ elements of $\delta$.
Now, if $\lambda_{d+1} + \tfrac{1}{4} \|\tilde{\delta}\|^{2} < \lambda_d$, then
the $d$ largest eigenvalues of $\tilde{\Sigma}$ is still $\lambda_1, \lambda_2, \dots, \lambda_d$, and thus the eigenvectors corresponding to the $d$ largest eigenvalues of $\tilde{\Sigma}$ is the same as that for the $d$ largest eigenvalues of $\Sigma$. That is to say, $$ \lambda_{d+1} + \tfrac{1}{4} \|\tilde{\delta}\|^{2} < \lambda_d \Longrightarrow \tilde{\Sigma}_{d}^{\dagger} = \Sigma_{d}^{\dagger} \Longrightarrow \delta^{\top} \tilde{\Sigma}_{d}^{\dagger} \delta = 0 \Longrightarrow C(F_0^{(B)}, F_1^{(B)}) = 0.$$

We now compute the probability that $\lambda_{d+1} + \tfrac{1}{4} \|\tilde{\delta}\|^{2} < \lambda_d$. Suppose for the moment that $\epsilon > 0$ is fixed and do not varies with $p$. We then have
$$ \frac{\sum_{i=d+1}^{p} \delta_i^{2} - (p - d) (1 - \epsilon) \tau^2}{\sqrt{(p - d)(2 (1 - \epsilon) (2 \tau^2 \sigma^2 + \sigma^4) + \epsilon (1 - \epsilon) (\tau^4 + 2 \tau^2 \sigma^2 + \sigma^4))}} \overset{\mathrm{d}}{\longrightarrow} N(0,1).$$

Thus, as $p \rightarrow \infty$, the probability that $\lambda_{d+1} + \tfrac{1}{4} \|\tilde{\delta}\|^{2} < \lambda_d$ converges to that of
$$\Phi\Bigl(\frac{4(\lambda_d - \lambda_{d+1}) - (p - d) (1 - \epsilon) \tau^2}{\sqrt{(p - d)(2 (1 - \epsilon) (2 \tau^2 \sigma^2 + \sigma^4) + \epsilon (1 - \epsilon) (\tau^4 + 2 \tau^2 \sigma^2 + \sigma^4))}} \Bigr).$$ This probability can be made arbitrarily close to $1$ provided that $\lambda_{d} - \lambda_{d+1} \geq Cp(1 - \epsilon) \tau^2$ for all sufficiently large $p$ and for some constant $C > 1/4$. Since the probability that $\delta_1 = \delta_2 = \dots = \delta_d$ is at least $\epsilon^{d}$, we thus conclude that for sufficiently large $p$, with probability at least $\epsilon^{d}$,
$$C(F_0^{(B)}, F_1^{(B)}) = 0 < C(F_0^{(A)}, F_1^{(A)}).$$

In the case where $\epsilon = \epsilon(p) \rightarrow 1$ as $p \rightarrow \infty$ such that $p(1 - \epsilon) \rightarrow \theta$ for some constant $K$, then the probability that $\lambda_{d+1} + \tfrac{1}{4} \|\tilde{\delta}\|^{2} < \lambda_d$ converges to the probability that
$$\frac{1}{4} \sum_{i=1}^{K} \sigma^2 \chi_{1}^{2}(\tau) \geq \lambda_{d} - \lambda_{d+1} $$
where $K$ is Poisson distributed with mean parameter $\theta$ and $\chi_{i}^{2}(\tau)$ is the non-central chi-square distribution with one degree of freedom and non-centrality parameter $\tau$. Thus if $\lambda_{d} - \lambda_{d+1} \geq C \theta \tau^2 \log{p}$ for sufficiently large $p$ and for some constant $C$, then this probability can also be made arbitrarily close to $1$.

\subsection{Finite Sample Performance}
We now consider the finite sample performance of \Lol~and PCA-based classifiers in the high-dimensional setting with small or moderate sample sizes, e.g., when $p$ is comparable to $n$ or when $p \gg n$. Once again we assume that $X | Y = i \sim \mathcal{N}(\mu_i, \Sigma)$ for $i = 0, 1$. Furthermore, we also assume that $\Sigma$ belongs to the class $\Theta(p,r,k,\tau,\lambda)$ as defined below.

{\bf Definition} Let $\lambda > 0$, $\tau \geq 1$ and $k \leq p$ be given. Denote by $\Theta(p,r,k,\tau,\lambda,\sigma^2)$ the collection of matrices $\Sigma$ such that

$$ \Sigma = V \Lambda V^{\top} + \sigma^2 I $$
where $V$ is a $p \times r$  matrix with orthonormal columns and $\Lambda$ is a $r \times r$ diagonal matrix whose diagonal entries $\lambda_1, \lambda_2, \dots, \lambda_r$ satisfy $\lambda \geq \lambda_1 \geq \lambda_2 \geq \dots \geq \lambda_r \geq \lambda/\tau$. In addition, assume also that $|\mathrm{supp}(V)| \leq k$ where $\mathrm{supp}(V)$ denote the non-zero rows of $V$, i.e., $\mathrm{supp}(V)$ is the subset of $\{1,2,\dots,p\}$ such that $V_j \not = 0$ if and only if $j \in \mathrm{supp}(V)$.

We note that in general $r \leq k \ll p$ and $\lambda/\tau \gg \sigma^2$. We then have the following result.

{\bf Theorem \cite{cai-pca-1}}
Suppose there exists constants $M_0$ and $M_1$ such that $M_1 \log p \geq \log n \geq M_0 \log \lambda$. Then there exists a constant $c_0 = c_0(M_0, M_1)$ depending on $M_0$ and $M_1$ such that for all $n$ and $p$ for which
$$ \frac{\tau k}{n} \log \frac{e p}{k} \leq c_0, $$ there exists an estimate $\hat{V}$ of $V$ such that

$$ \sup_{\Sigma \in \Theta(p,r,k,\tau,\lambda,\sigma^2)} \mathbb{E} \|\hat{V} \hat{V}^{\top} - V V^{\top} \|^{2} \leq \frac{C k (\sigma \lambda + \sigma^2)}{n \lambda^{2}} \log \frac{e p}{k}$$
where $C$ is a universal constant not depending on $p,r,k,\tau,\lambda$ and $\sigma^2$.

We can therefore show that provided that $M_0$ and $M_1$ is large enough, and $n$ and $p$ satisfies the condition in the preceding theorem, then provided that the Chernoff information of the population version of \Lol~is larger than the Chernoff information of the population version of PCA, the expected Chernoff information for the sample version of \Lol~is also larger than the expected Chernoff information of the sample version of PCA. We emphasize that it is necessary that the \Lol~and the PCA version both projected into the top $d \leq r$ dimension of the sample covariance matrices.





\clearpage
\section{The R implementation of LOL}

Figure \ref{Rimpl} shows the R implementation of \Lol~for binary classification
using FlashMatrix \cite{FlashMatrix}. The implementation takes a $D \times I$
matrix, where each column is a training instance and each instance has D
features, and outputs a $D \times k$ projection matrix.

\clearpage
\begin{figure}[t]
\begin{lstlisting}
\Lol~<- function(m, labels, k) {
	counts <- fm.table(labels)
	num.labels <- length(counts$val)
	num.features <- dim(m)[1]
	nv <- k - (num.labels - 1)
	gr.sum <- fm.groupby(m, 1, fm.as.factor(labels, 2), fm.bo.add)
	gr.mean <- fm.mapply.row(gr.sum, counts$Freq, fm.bo.div, FALSE)
	diff <- fm.get.cols(gr.mean, 1) - fm.get.cols(gr.mean, 2)
	svd <- fm.svd(m, nv=0, nu=nv)
	fm.cbind(diff, svd$u)
}
\end{lstlisting}
\caption{The R implementation of LOL.}
\label{Rimpl}
\end{figure}


% First note that

% \begin{lem}
% $\bP_A = \bP_{WA}$, when $\bW \T \bW = \bI$.
% \end{lem}


% \begin{proof}
% blah
% \end{proof}

\clearpage
% \bibliography{biblol}
\bibliography{biblol.bib}
\bibliographystyle{IEEEtran}


\end{document}
