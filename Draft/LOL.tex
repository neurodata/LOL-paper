\documentclass[10pt]{article}
\input{preamble.tex}

\title{\vspace{-50pt}\db{Big Supervised Manifold Learning by Low-Rank Optimal Linear Discriminant Analysis (LOL)}}
\author{Joshua T.~Vogelstein, Brett Mensh, \\ Minh Tang, Da Zheng, Randal Burns, Mauro Maggioni}
\date{}

% Research Articles (up to ~4500 words, including references, notes and captions, or ~5 printed pages) are expected to present a major advance. Research Articles include an abstract, an introduction, up to six figures or tables, sections with brief subheadings, and about 40 references. Materials and Methods should be included in supplementary materials, which should also include information needed to support the paper's conclusions.

% intro: 1500
% results

\begin{document}
\maketitle

%%% 1 idea per sentence, have a compound sentence when requiring gestalt meaning

%%% 1st sentence should carry the meaning of the whole paragraph

\begin{abstract}
In the 21st century high-dimensional observations abound.  In classical statistics, it is well known that as the dimensionality of a dataset increase, the number of samples required to estimate properties thereof increases even faster.  A number of techniques have been developed to stave off this ``curse of dimensionality'', including dimensionality reduction and regularization.  Mitigating this curse is especially demanding in supervised learning problems, such as classification. The challenge in finding a low-dimensional discriminant boundary in the face of high-dimensional observations is that direct searches are computationally infeasible.  Therefore, previous art have taken one of two approaches. First, one can ignore the discriminant problem, and try to reduce the dimensionality of the data---for example, using principal components analysis or manifold learning techniques---in hopes that one has not discarded the discriminant information.  Second, one directly solve the problem after making a number of overly restrictive assumptions of the data, such as independence and sparsity.  Here, we demonstrate via a simple geometric argument that one can algebraically compute a low-dimensional embedding that is approximately optimal with high probability in many settings.  The intuition is that we can  project the data both in the directions of maximal variance (ignoring the classification task) and the linear direction of maximal discrimination (ignoring the variance).  The result is an extremely computationally efficient supervised manifold learning method that we call ``Linear Optimal Low-Rank'' (LOL). LOL outperforms state-of-the-art algorithms in a wide variety of settings, including both those for which theoretical arguments predict it should, and in much more general settings for which we are unable as yet to obtain solid theoretical footing.  This includes both simulated settings, where our intuition enables us to characterize a number of simple variants designed for more complex settings, as well as several benchmark problems.  In particular, LOL outperforms several reference algorithms in terms of both efficiency and accuracy on said benchmarks. We therefore believe that LOL could be useful in myriad applications, and many extensions are readily available.
\end{abstract}

\vspace{15pt}

\clearpage
\linenumbers

% intro: we get 64, we have 157.  need to reduce by 2x

Supervised learning (SL)---the art and science of estimating statistical relationships using labeled training data---is a crucial tool in scientific discovery.  SL has been enabled a wide variety of basic and applied findings, ranging from discovering biomarkers in omics data \cite{Vogelstein2014a}, to object recognition from images \cite{Krizhevsky2012}, and includes celebrated methods such as support vector machines, random forests, and deep networks \cite{Hastie2004}.  A special case of SL is classification; a classifier  predicts the 'class' of a novel observation via partitioning the space of observations into different classes. 
(for example, predicting male vs.~female from MRI scans).  
In this big data age,  the ambient (or observed) dimensionality of the observations is quickly ballooning, and with it, the ambient dimensionality of the discriminant boundary.  While historical data may have consisted of only a few dimensions (e.g., height and weight), modern scientific datasets often consist of hundreds, thousands, or even millions of dimensions (e.g. genetics, neuroscience, omics). 
Regardless of the dimensionality, when  scientists or analysts obtain new datasets, they must make a number of decisions, by either implicitly or explicitly weighting considerations across several levels of analysis, including both statistical and computational considerations. 

As suggested above, the goal of such problems is to estimate a discriminant boundary that  partitions the space as optimally and efficiently  as possible. If we knew the ``true data generating mechanism'', then writing down the optimal discriminant boundary would be relatively trivial.  However, in all real data science problems, at least some parts of the true data generating mechanism remain unknown, because the ``Truth'' is likely infinitely complex.  Because we do not know the truth, we seek tools that will estimate boundaries as accurately and efficiencly as possible given the training data available to us.  To do so, one must make a series of decisions. These decisions have a variety of ramifications; the art of supervised learning is making choices that balance those ramifications appropriately for the problem at hand.  The choices can reasonably be organized into four levels of analysis: (i) model, (ii) algorithm, (iii) implementation, and (iv) platform, each have different implications, both quantitative and qualitative.  The quantitative impact can be further subdivided into statistical and computational implications, with statistical implications largely determined by the top two levels, and computational implications governed by the bottom two.   
Below, we list in italics the \emph{choices} one must make, and in bold the \textbf{design considerations} associated with each.  These considerations directly determine the design goals of our machine learning system, as will be elaborated upon below.


% trade-offs:
% stats: bias-variance trade-off
% comp: We make design tradeoffs to balance the opposing forces of minimizing memory usage and maximizing CPU cycles spent computing in parallel. The design trade-off is cache freshness for reduced cache maintenance overhead. 


First, one must choose a \emph{discriminant model} (often called feasible region or action space in other contexts, and often this choice is implicit in the algorithm choice). 
This modeling assumptions determine geometric constraints on the shape of the estimated boundary.
For example, a linear boundary is a potential discriminant model, and the particular feasiable optimal boundary for a given problem is then specified by (a potentially multidimensional) parameter: the slope and intercept.  \textbf{Model bias}  is the minimum distance between the optimal discriminant boundary in the discriminant model and true boundary (determined by the data generating mechanism).  As mentioned above, in real data problems, we always expect some model bias unless the discriminant model is ``non-parametric'', meaning that it can account for any discriminant boundary.

Second, one must choose an \emph{algorithm} to estimate the boundary, such as Fisher’s Linear Discriminant (\Fld), linear Support Vector Machines (\Svm), or logistic regression.  Each such algorithm has different convergence properties as sample size increases.  Importantly, for different data generating mechanisms, different algorithms may converge on different solutions.  The additional distance between the expected estimated boundary and the optimal boundary is called the \textbf{estimator bias}, which we also want to minimize by choosing an algorithm that will select an estimate as close as possible to the truth.  Different algorithms will also differ in their \textbf{estimator variance}, which is a function both of the size of the discriminant model, and the idiosyncrasies of the algorithm.  Of note, in certain circumstances, the two bias terms and the variance together determine expected mean square error, so one also desires to minimize the variance of the estimated boundary. 

In real data problems, there are two additional quantitative considerations impacted by the algorithm choice: \textbf{hyper-parameter searches} and \textbf{robustness}. As mentioned above, we desire to minimize both the bias and the variance; alas, decreasing one often comes at the cost increasing the other.  For any given data generating mechanism and dataset, the optimal trade-off is unknown. Fortunately, many algorithms are equiped with a hyper-parameter, that enables the practictioner to ``tune'' the bias vs.~variance to obtain an ``optimal'' fit in terms of expected error on other data.  The ease and efficiency with which those hyper-parameters can be tuned can substantially impact performance.   The robustness of an algorithm quantifies its ability to estimate nearly optimal boundaries when there are outliers. This is another consideration that plays a crucial role in real data.  


The third and fourths choices impact computational considerations.  Third, one must choose an \emph{implementation}.  First, implementations should be \textbf{numerically stable}, over the range  of the number of samples and dimensions under consideration.  Second, one desires an implementation that minimizes computational complexity, both in terms of  \textbf{space} and \textbf{time}.  
Fourth, one must choose a \emph{platform} upon which the implementation operates,  including both software dependencies and hardware choice.  In the big data age, both \textbf{scale-up} and \textbf{scale-out} are crucial design considerations.  Scale-up, generically speaking, refers to the ability of utilizing all available computational resources, rather than allowing computers to sit idly.  Scale-out refers to the ability to incorporate additional available resources.  Together, these terms determine the scalability and resource efficiency of a system, which can have dramatic consequences in real world big data problems.


All of the above quantitative considerations can be evaluated \textbf{theoretically}, but typically only under (overly) restrictive assumptions.  Even when the assumptions are met, theory often merely provides bounds, without concrete numbers in any particular situations. \textbf{Simulated experiments} can therefore buttress the theoretical results. By knowing the truth, one  can  compare the performance of different approaches to provide insight into when the theories are working as expected, including in settings that extend beyond the theoretical constraints.  Moreover, \textbf{real data performance}, provide further insight into different methodologies in real data settings.


The design or choice of a machine learning system for solving a given real world problem reflects each of the above design considerations.   Regardless of the problem, we want a discriminant model that exhibits low bias for that problem, an algorithm with low estimator bias and variance, with easy hyper-parameter searches that is robust to outliers, an implementation that is numerically stable with small space and time requirements, deployed on a platform that scales both up and out.  And we want all the properties demonstrated via theory, buttressed by simulations, and corroborated by real data experiments.  Obviously, it is quite a challenge to optimize along all of the above dimensions for any particular kind of exploitation task.  We therefore focus  on the simplest supervised learning task we can think of: a high-dimensional two-class classification problem.  To our knowledge, there does not yet exist a machine learning system that performs well along each of the above design considerations for even this problem.


The first century of statistical pattern recognition focused on elegant SL approaches for low-dimensional problems (e.g., Fisher's Linear Discriminant (FLD) \cite{Fisher}), and dimensionality reduction techniques for high-dimensional problems (e.g., Principal Components Analysis (PCA) \cite{PCA}), but did not seriously investigate high-dimensional SL approaches.  More recent work has focused on high-dimensional SL approaches, including sparse methods (e.g., Lasso \cite{Lasso}) and supervised dictionary learning techniques \cite{Mairal2009}.  However, theoretical guarantees for sparse methods require very strict assumptions that are often far from accurate \cite{??}, meaning that sparse methods tend to exhibit significant bias.  Moreover, their suseptibility to correlations of the predictor variables can lead to substantial variance, insofar that which variables are selector can be highly noise dependent \cite{Meinshausen2010c}.  Dictionary learning, on the other hand, has scant theoretical guarantees in any setting \cite{??}.  Moreover, because dictionary learning tends to consider a large complex discriminant model, its resulting estimates are highly dependent on initialization choice.  Finally, computationally, both are problematic for big data, in that they require large amounts of time, and implementations on scalable platforms are currently unavailable.   

In complementary work, generalizations of principal components to include various kinds of nonlinear dimensionality reduction techniques, collectively referred to as  ``manifold learning'', have been developing recently \cite{??}. These tools are predominantly ``unsupervised'', and therefore, do not explicitly search for a discriminant boundary, and therefore may add significant bias to the problem at hand.  Adding supervision to these techniques so far has been difficult; methods proposed thus far embed the data in an unsupervised fashion first, again, potentially adding bias \cite{Belkin2004a}, and require substantial computations.   Thus, a supervised manifold learning algorithm designed for one of the simplest possible high-dimensional classification problems remains amiss.


The computer science community has been developing tools for big data for several decades now. Specifically, algorithm complexity has lowered significantly, and ability to scale-up and scale-out has increased significantly.  This is true both for simple statistical primitives, such as computing a mean, and more complicated primitives like PCA \cite{??}.  More recently, a large number of distributed machine learning libraries have become available, including Apache Spark's mllib,  H20, Dato, and Vowpal Wabbit \cite{??}.  These focus almost entirely on large sample size and low dimensionality regime, whereas the motivating problems of interest for this work are small sample size and high-dimensionality. Moreover, they utilize a distributed platform that is susceptible to low bandwidth communication between nodes, meaning that as one adds resources, the computations cannot scale out optimally.



% , where each class is sampled from a Gaussian distribution and they only differ in their means.  Such a situation is a big data problem when either the dimensionality $p$ or sample size $n$ is large; we focus  on the setting where $p$ is large, and in fact, when $p$ is much larger than $n$, that is, $p>n$.  In this setting, much of classical statistics that relies on asymptotic results with $n$ increasing and $p$ fixed, is not appropriate.  Moreover, methods developed on the basis of the increasing sample size assumption suffer along many of the above considerations; in particular, they exhibit unacceptably large variance and numerical instability. 

% Fisher’s Linear Discriminant (\Fld), while asymptotically optimal (zero bias and variance as n goes to infinity), is numerically unstable when $n < p$, and therefore will not even run.  

% To address the above statistical considerations in such high-dimensional settings, a number of strategies have been developed recently in the literature.  A very natural “fix” to this is to first ``pre-process'' the data via a truncated Principal Components Analysis (tPCA), and then apply \Fld (an approach called Fisherfaces \cite{}.  While first running tPCA greatly reduces the variance relative to directly running FLD, and therefore makes the algorithm numerically stable, it adds estimator bias because the dimension reduction performed by PCA does not consider the class labels, only the predictor variables, and the discriminant boundary is determined by the relationship between the class labels and predictor variables. 

% Other ``manifold learning techniques'', such as isomap \cite{Tenenbaum2000a}, local linear embedding \cite{Roweis2000a}, Laplacian eigenmaps \cite{Belkin2003a}, Diffusion maps \cite{Coifman2006a} and others are similarly ``unsupervised'', and therefore, do not explicitly search for a discriminant boundary, and therefore may add significant bias.  Adding supervision to these techniques so far has been difficult, methods proposed thus far embed the data in an unsupervised fashion first, again, potentially adding bias \cite{Belkin2004a}, and require substantial computations. Sparse methods (e.g., \cite{}) can use both the class labels and predictor variables, but they come with substantial costs. In particular, if certain very restrictive assumptions of the data are not satisfied, such as the restrictive isometry property, sparse methods add both significant estimator bias and variance.  Moreover, they require much greater computational time than Fisherfaces.  

% To address the above computational considerations, a number of strategies have been developed recently in the literature.  Most such approaches have focused on the large $n$ setting, including sub-sampling \cite{}, random sketching \cite{Woodruff14a},  and distributed learning \cite{Guha12a,Agarwal2014a,Meng15a,}.  

% To date, we are unaware of any machine learning library designed specifically to address ``wide data'' (where $p>n$), rather they all focus on ``tall data'' (where $n>p$).  The computational considerations and implementation details for wide and tall data are sufficiently different to demand different implementations for optimized efficiency.  


Thus, a gap remains in the literature for a statistical tool that, under relatively general assumptions, achieves both small bias and variance, can be made robust, and has easily tunable hyper-parameters.  And complementarily, an implementation of that algorithm that is numerically stable, requires minimal space and time, is highly scalable for high-dimensional problems. Importantly, the method should perform well both on simulated and real data experiments.  And ideally, the software is intuitive, free and open source, and easy to use.

Here, we describe a novel solution, called Low-Rank Optimal Linear Discriminant Analysis (LOL), that satisfies all of the above desiderata. We developed LOL by considering the above described simple scenario, and realizing that the optimal dimension reduction for such a setting would have to combine both the means and the covariances, because together those two parameters determine the optimal decision boundary.  While other methods have previous had a similar insight (in particular the ``First Two Moment'' (F2M) methods \cite{Li1991a, Tishby1999a, Globerson2003a, Cook2005a,Fukumizu2004a}), F2M methods only operate in low-dimensional settings.  Therefore, we decided to combine the means and covariances differently, essentially by first projecting the data onto the means, and then effectively computing PCA on the ``residuals''.  Running FLD after projecting the data onto the subspace spanned jointly by the means and the top principal components comprises the LOL method.  In addition to MATLAB and R, we also implemented LOL using FlashX, a recently developed big data analytics library \cite{??}.  We were therefore able to take advantage of matrix parallelizations optimized for machines with solid state drives.  

We demonstrate LOL outperforms---or at least performs no worse than---several reference benchmarks along every statistical consideration mentioned above.  This includes proving the improvement over several standards, along both statistical considerations (bias and variance) for the simple Gaussian setting mentioned above. Moreover, we provide a distributed implementation that improves on previous computational capabilities in terms of: stability, time, space, and scalability under more general settings.  Moreover, numerical experiments quantitatively demonstrate the improvement of LOL over reference methods on a wide range of simulated experiments that satisfy our theoretical assumptions. By virtue of LOL being built upon geometric intuition, it is easy to extend it along various dimensions.  Additional numerical experiments demonstrate that practice matches our intuition, in particular, generalizations of LOL can easily be constructed to match different geometric assumptions of the data.  Indeed, even when data are sampled from distributions outside our theory and intuition, we see that LOL achieves smaller error than its competitors. In fact, we can extend our intuition outside of classification problems to other problems, including wide regression and hypothesis testing.  A trivial modification of LOL leads to regression and testing procedures that also outperform the natural competitors.  Computationally, LOL is always numerically stable, and requires no more computational space, and smaller computational time, than both Fisherfaces and sparse methods.  Moreover, we provide a highly scalable implementation that efficiently runs on terabyte datasets with hundreds of thousands or millions of features. Finally, we test LOL against a set of standard methods on four different benchmark real datasets.  For any of the four different settings, there was not a single method that outperformed LOL on both accuracy and time.   

In addition to the above quantitative properties, LOL is intuitive, only has a single hyper-parameter which is exceedingly easy to tune, and we provide free and open source implementations, in MATLAB, R, and FlashX, as well as pre-configured environments that will enable anybody to run LOL on commodity machines, including laptops, workstations, and cloud instances.  LOL is not likely to necessarily be the best algorithm on any given new high-dimensional challenge problem.  Rather, the arguments and methodology developed herein provide a framework for developing big supervised manifold learning algorithms to tackle the data science problems of the next century.




\section*{Results}
% get 192 lines, have 240, need to remove 50.

\subsection*{An Illustrative Real Data Example of Supervised Linear Manifold Learning}



% \begin{wrapfigure}{R}{0.7\textwidth}%[h!]
\begin{figure}
\centering % l b r t
\includegraphics[width=0.7\linewidth,trim=1cm 0cm 0cm 4.0cm,clip=true]{../Figs/mnist}
\caption{
Illustrating three different classifiers---\sct{Lasso}~(top), \sct{Fld$\circ$Pca}~(middle), and \Lol~(bottom)---for embedding images of the digits 3, 7, and 8 (from MNIST), each of which is 28 $\times$ 28 = 784 dimensional.
\textbf{(A)} A subset of training samples  (boundary colors are only for visualization purposes).
\textbf{(B)} The first four projection matrices learned by the three different approaches on 300 training samples. Note that \sct{Lasso}~is sparse and supervised, \Pca~is dense and unsupervised, and \Lol~is dense and supervised.
\textbf{(C)} Embedding 500 test samples into the top 2 dimensions using each approach.  Digits color coded as in A.
\textbf{(D)}  The estimated posterior distribution of test samples after
projecting data into 5-dimensions via each method.
We show only 3 vs.~8 for simplicity.
The vertical line shows the classification threshold.
The filled area is the estimated error rate; the goal of any classification algorithm is to minimize that area.
Clearly, \Lol~exhibits the best separation after embedding, which results in the best classification performance.
}
\label{f:mnist}
% \end{wrapfigure}
\end{figure}

A general strategy for high-dimensional classification proceeds as  schematized in Figure \ref{f:mnist}:
A obtain/select $n$ training samples of the data,
B learn a low dimensional projection,
C project $m$ testing samples onto the lower dimensional space,
D classify the embedded testing samples using some classifier.
We consider  three different linear dimensionality reduction methods---\sct{Lasso}, \Pca, and \Lol---each of which we compose with a classifier to form high-dimensional classifiers.\footnote{Although \sct{Lasso}~is not a 2-step method (where embedding is learned first, and then a classifier is applied), adaptive lasso \cite{Zou2006a} and its variants improve on lasso's theoretical and empirical properties, so we consider such an approach here.}

To demonstrate the utility of \Lol, we  first consider one of the most popular benchmark datasets ever, the MNIST dataset \cite{mnist}.  This dataset consists of $n=60$,$000$ examples of images of the digits 0 through 9.  Each such image is represented by a 28$\times$28 matrix, which means that the observed (or ambient) dimensionality of the data is $p=28^2=784$.  Because we are motivated by the $n \ll p$ scenario, we subsample the data to select $n=300$ examples of the numbers $3$, $7$, and $8$. We then apply all three approaches to this subsample of the MNIST dataset, learning a projection, and embedding $m=500$ testing samples, and classifying the resulting embedded data.

% Figure \ref{f:mnist}A shows several examples of digit, the border of each digit is colored for visualization purposes only.
% Figure \ref{f:mnist}B shows the estimated projection matrices for each approach (see Methods for details).
% These are the matrices that transform the data into a low-dimensional representation.



% Figure \ref{f:mnist}C shows n'=500 randomly selected training samples from the digits 3, 7, and 8, embedded into the first two dimensions using each of the three approaches. The colors of the embedded points correspond to those in the leftcolumn, that is, 3 is blue, 7 is red, and 8 is green.
% FIgure \ref{f:mnist}D shows a smoothed histogram of the posterior probabilities of each test samples after classifying.
\sct{Lasso}, by virtue of being a sparse method, finds the pixels that most discriminate the 3 classes.  The resulting embeddings mostly live along the boundaries, because these images are close to binary, and therefore, images either have or do not have a particular pixel. Indeed, although the images themselves are nearly sparse (over 80\% of the pixels in the dataset have intensity $\leq 0.05$),  a low-dimensional discriminant boundary does not seem to be sparse.  \Pca, on the other hand, finds the linear combinations of training samples that maximize the variance.  This unsupervised linear manifold learning method results in projection matrices that indeed look like linear combinations of the three different digits.  The goal here, however, is separating classes, not maximizing variability.  The resulting embeddings are not particularly well separated, suggesting the the directions of discriminability are not the same as the directions of maximum variance.  \Lol~is our newly proposed supervised linear manifold learning method (see below for details).  The projection matrices it learns look qualitatively much like those of \Pca. This is not surprising, as both are linear combinations of the training examples.  The resulting embeddings however, look quite different.  The three different classes are very clearly separated by even the first two dimensions.  The result of these embeddings yields classifiers whose performance is obvious from looking at the embeddings: \Lol~achieves significantly smaller error than the other two approaches.
This numerical experiment justifies the use of supervised linear manifold learning, we next build a geometric intuition for these methods in simple simulated examples, to better illustrate when we can expect \Lol~to outperform other methods, and perhaps more importantly, when we expect  \Lol~to fail.

% \clearpage
\subsection*{Linear Gaussian Intuition}

The above real data example suggests the geometric intuition for when \Lol~outperforms its sparse and unsupervised counterparts.  To further investigate, both theoretically and numerically, we consider the simplest setting that illustrates the relevant geometry.  In particular, we consider a two-class classification problem, where both classes are distributed according to a multivariate normal distribution, where the two classes have the same covariance matrix. so that the only difference between the classes is their means (we call this the Linear Discriminant Analysis (\Lda) model; see Methods for details).




% To motivate \Lol, and the following simulations, lets c
Consider what the optimal projection would be in this scenario. The optimal low-dimensional projection is analytically available as the dot product of the difference of means and the inverse covariance matrix, $\mb{A}_*=\bdel\T \bSig^{-1}$ \cite{Bickel2004a} (see Methods for derivation).
\Pca, the dominant unsupervised manifold learning method, utilizes only the covariance structure of the data, and ignores the difference between the means (because \Pca~for classification operates on the class-conditionally centered data, see Methods for details).
In particular, \Pca~would project the data on the top d eigenvectors of the  covariance matrix.
\textbf{The key insight of our work is the following: we can combine the difference of the means and the covariance matrix in a simple fashion, rather than just the covariance matrix, to find a low dimensional projection.}
Na\"ively, this should typically improve performance, because in this stylized scenario, both are important. The \sct{F2M} literature has a similar insight, but a different construction that requires the dimensionality to be smaller than the sample size \cite{Li1991a, Tishby1999a, Globerson2003a, Cook2005a,Fukumizu2004a}. We implement our idea by simply concatenating the difference of the means with the top d eigenvectors of the  covariance matrix.
This is equivalent to first projecting onto the difference of the means vector, and then projecting the residuals onto the first d principle components.
Thus, it requires almost no additional computational time or complexity over that of \Pca, rather, merely estimates the difference of the means.
In this sense, \Lol~can be thought of as a very simple  ``supervised \Pca''.



\begin{figure}%{R}{0.7\textwidth}%[h!]
% \begin{SCfigure}
\centering
\includegraphics[width=0.8\linewidth,trim=0in 0in 1.5in 0in,clip=true]{../Figs/cigars_est}%l b r t
\caption{
\Lol~achieves near optimal performance for a wide variety of distributions.
Each point is sampled from a multivariate Gaussian;
the three columns correspond to different simulation parameters (see Methods for details).
In each of 3 simulations, we sample $n=100$ points in $p=1000$ dimensions.  And for each approach, we embed into the top 20 dimensions. Note that we use the sample estimates, rather than the true population values of the parameters.  The five columns show (in decreasing order): 
\textbf{Row 1}: A scatter plot of the first two dimensions of the sampled points, with class 0 and 1 as black and gray dots, respectively.
\textbf{Row 2} \sct{Fld $\circ$ Pca}.
\textbf{Row 3} \Road, a sparse method designed specifically for this model \cite{Fan2012a}.
\textbf{Row 4} \Lol, our newly proposed method.
\textbf{Row 5} the Bayes optimal classifier, which is what all classifiers strive to achieve.
\textbf{(A)} The mean difference vector is aligned with the direction of maximal variance, maxing it ideal for both \Pca~to discover the discriminant direction and a sparse solution. In this setting, the results are  similar for all methods, and essentially optimal.
\textbf{(B)} The mean difference vector is orthogonal to the direction of maximal variance, making \Pca~fail, but sparse methods and \Lol~can still recover the correct dimensions, achieving nearly optimal performance. 
\textbf{(C)} Same as B, but the data are rotated, in this case, only \Lol~performs well.
Note that \Lol~is closest to Bayes optimal in all three settings.
}
\label{f:cigars}
\end{figure}
% \vspace{-30pt}


Figure \ref{f:cigars} shows three different examples of data sampled from the \Lda~model to geometrically illustrate this intuition.
In each, we sample $n=100$ training samples in $p=1000$ dimensional space, so $n \ll p$.
Figure \ref{f:cigars}A shows an example we call  ``stacked cigars''.
In this example the covariance matrix is diagonal, so all ambient dimensions are independent of one another.
Moreover, the difference between the means and direction of maximum variance are both large along the same dimensions (they are highly correlated with one another).
This is an idealized setting for \Pca, because \Pca~finds the direction of maximal variance, which happens to correspond to the direction of maximal separation.
However, in the face of high-dimensional data, \Pca~does not weight the discriminant directions sufficiently, and therefore performs only moderately well.\footnote{When having to estimate the eigenvector from the data, \Pca~performs even worse.  This is because when $n \ll p$, \Pca~is an inconsistent estimator with large variance \cite{Baik2006a,Paul2007a}.}
Because all dimensions are independent, this is a good scenario for sparse methods.
Indeed,  \Road, a sparse classifier designed for precisely this scenario,  does an excellent job finding the most useful ambient dimensions.
\Lol~does the best of all three approaches, by using both the difference of the means and the covariance. This is obvious upon comparing each approach to the Bayes optimal solution for this setting, which is analytically available due to its simplicity.


Figure \ref{f:cigars}B shows an example which is a worst case scenario for using \Pca~to find the optimal projection for classification.
In particular, the variance is getting larger for subsequent dimensions, $\sigma_1 < \sigma_2 < \cdots < \sigma_p$, while the magnitudes of the difference between the means are decreasing with dimension, $\delta_1 > \delta_2 < \cdots > \delta_p$.
Thus, for any truncation level,  \Pca~finds exactly the \emph{wrong} directions.
\Road~is not hampered by this problem, it is also able to find the directions of maximal discrimination, rather than those of maximal variance.
Again, \Lol, by using both the means and the covariance, does extremely well.


Figure \ref{f:cigars}C is exactly the same as B, except the data have been randomly rotated in all 1000 dimensions.  This means that none of the original coordinates have much information, rather, linear combinations of them do.
This is evidenced by observing the scatter plot, which shows that the first two dimensions  fail to disambiguate the two classes.
\Pca, being rotationally invariant, fails in this scenario as it did in B.
Now, there is no small number of ambient dimensions that separate the data well, so \Road~also fails.
 \Lol~is unperturbed by this rotation; in particular, it is able to ``unrotate'' the data, to find dimensions that optimally separate the two classes.


\subsection*{Statistical Theory}

The above numerical experiments provide the intuition to guide our theoretical developments.
\begin{thm} \label{t:LDA}
\Lol~better than (or as good as) \Pca, under the \Lda~model when the parameters are provided.  
% This is true for all possible observed dimensionality of the data, and number of dimensions into which we embed. 
\end{thm}
In words, it is better to incorporate the mean difference vector into the projection matrix.  The \emph{degree} of improvement is a function of the embedding dimension d, the ambient dimensionality p, and the parameters (see Methods for details and proof), but the \emph{existence} of an improvement, or at least no worse performance, is independent of those factors.  Note that better here includes both bias and variance.  In the population version of these two methods, there is no variance, so only the bias is important.  Moreover, we know the conditions under which \Lol~is strictly better than \Pca, they are formally stated in Assumption \ref{a:2} (in Methods).  Informally, \Lol~is better than \Pca~whenever the angle between the mean vector and the top $d$ principal components is large enough. Under reasonable assumptions, in the appendix, we show that this happens $84\%$ of the time.  
% Note that Theorem \ref{t:LDA} is true regardless of the truncation dimension and ambient (observed) dimension.  



\subsection*{Numerical Experiments Extending Our Theoretical Results}

% In the above numerical and theoretical investigations, we fixed $d$, the number of dimensions to embed into.  Much unsupervised manifold learning theory typically focuses on finding the ``true'' intrinsic dimensionality of the data.   The analogous question for supervised manifold learning would be to find the true intrinsic dimensionality of the discriminant boundary.  However, in real data problems, typically, their is no perfect low dimensional representation because of noise.
% , rather, the more data we obtain, the higher-dimensional discriminant boundary we can estimate, and the close to Bayes optimal we can perform.

% Thus, in all the following simulations, the true ambient dimensionality of the data is equal to the dimensionality of the optimal discriminant boundary (given infinite data).  In other words, there does not exist a discriminant space that is lower dimensional than the ambient space, so we cannot find the ``intrinsic dimension'' of the data or the discriminant boundary.  Rather, we face a trade-off: keeping more dimensions reduces bias, but increases variance.  The optimal bias/variance trade-off depends on the distribution of the data, as well as the sample size \cite{Trunk1979a}.

% We formalize this notion for the \Lda model and proof the following:
% \begin{thm} \label{t:n}
% Under the \Lda~ model, estimated \Lol~is better than \Pca.
% \end{thm}
% Note that the degree of improvement is a function of the number of samples $n$, in addition to the embedding dimension $d$, the ambient dimensionality $p$, and the parameters (see Methods for details and proof).

In the previous section, we proved that \Lol~is often better than \Pca, and never worse under the \Lda~model, regardless of observed dimensionality, embedded dimensionality, and parameters, assuming the parameters are given.  In this section, we numerically investigate what happens when the parameters must be estimated from training data, and in particular, from $n=100$ training samples, when the observed dimensionality is $p=100$.  These results strengthen our theoretical results by providing exact magnitudes of performance, rather than merely better than guarantees, in addition to operating in a finite sample setting, more akin to real data scenarios. 

\para{\Lda~Model} We begin by investigating three scenarios that satisfy the \Lda~model assumptions required by our proofs. First, consider  the rotated trunk example from Figure \ref{f:cigars}C as well as a ``Toeplitz'' example, as depicted in Figures \ref{f:properties}A and B, respectively.  In both cases, we compare \Lol~to \FoP~and \Road.  In both scenarios, for all dimensions, \Lol~achieves a lower error rate than either of its competitors. 


\para{Multiple Classes} \Lol~can trivially be extended to $>2$ class situations.  Nai\"vely it may seem like we would need to keep all pairwise differences between means.  However, given $k$ classes, the set of all $k^2$ differences is only rank $k-1$.  In other words, we can equivalently compute the distance of each mean to a single mean (in practice, for minimizing variance reasons, we use the class which has the maximum number of samples (breaking ties randomly).  Figure \ref{f:properties}C shows a 3-class generalization of the rotated Trunk example from Figure \ref{f:properties}A.  While \Lol~uses the additional class naturally,  many previously proposed high-dimensional classifiers, such as \Road, natively only work for 2-classes. Thus, we only compare \Lol~to \PoF~here. As before, \Lol~significantly outperforms \PoF~for all embedding dimensions.



% the data are sampled from the \Lda~ model, and in both cases, the optimal dimensionality under finite samples depends on the particular approach, but is never the true dimensionality.  Moreover, \Lol~dominates the other approaches, regardless of the number of dimensions used.
%

% Indeed, we can generalize Theorem \ref{t:n} to include ``sub-Gaussian'' data, rather than just Gaussian:
% \begin{thm} \label{t:FAT}
% Under a sub-Gaussian generalization of the \Lda~ model, \Lol~is still better than \Pca.
% \end{thm}




\begin{figure}[h!]
\centering
\includegraphics[width=1\linewidth]{../Figs/properties}
\caption{
Seven simulations demonstrating \Lol~achieves superior finite sample performance over competitors both in settings for which we have asymptotic theoretical guarantees, and those for which we do not. 
% that even when the true discriminant boundary is high-dimensional, \Lol~can find a low-dimensional projection that wins the bias-variance trade-off against competing methods.
For the first three, the top panels depict the means (top), the shared covariance matrix (middle).  For the next three, the top panels depict a 2D scatter plot (left), mean and level set of one standard deviation of covariance matrix (right).  For all seven simulations, the bottom panel shows misclassification rate as a function of the number of embedded dimensions, for several different classifiers.  The simulations settings are as follows:
\textbf{(A)} Rotated Trunk: same as Figure \ref{f:cigars}C.
\textbf{(B)} Toeplitz: another setting where mean difference is not well correlated with any eigenvector, and no ambient coordinate is particularly useful on its own.
\textbf{(C)} 3 Class variant of the rotated Trunk example to demonstrate that \Lol~naturally adapts, and excels in, multi-class problems.
\textbf{(D)} Fat Tails: a common phenomenon in real data that is more general than our theory supports.
\textbf{(E)} QDA: \Qoq, a variant of \Lol~when each class has a unique covariance, outperforms \Lol, as expected, when the true discriminant boundary is a quadratic, rather than linear, function.
\textbf{(F)} Outliers: adding high-dimensional outliers degrades performance of standard eigensolvers, but those can easily be replaced in \Lol~for a robust variants (called \Lrl).
\textbf{(G)} XOR: a high-dimensional stochastic generalization of XOR, demonstrating that \Qoq~works even in scenarios that are quite distinct from the original motivating problems.
In all 7 cases, \Lol, or the appropriate generalization thereof, outperforms unsupervised or sparse methods.  Moreover, the optimal embedding dimension is never the true discriminant dimension, but rather, a smaller number jointly determined by parameter settings and sample size.
}
\label{f:properties}
\end{figure}



\subsection*{Generalizations of \Lol}

% The simple geometric intuition which led to the development of \Lol~suggests that we can easily generalize \Lol~to be more appropriate for more complicated settings. We consider three additional scenarios:

The above simulations were all under the \Lda~model, for which we have theoretical guarantees that \Lol~should outperform \PoF.  In this section, we explore simulation settings that extend beyond the geometric constraints we required for our proofs, to develop an even deeper understanding of \Lol.

\para{Fat Tails} Figure \ref{f:properties}D shows a sparse example with ``fat tails'' to mirror real data settings better. More specifically, each class is the sum of multiple Gaussians, with the same mean, but different covariances (see Methods for details). The qualitative results are consistent with those of A and B, even though the setting is no longer exactly the setting under which we have theoretical confirmation. More specifically, \Lol~outperforms \PoF~and \Road~for all dimensions.  


\para{QDA} Sometimes, it makes more sense to model each class as having a unique covariance matrix, rather than a shared covariance matrix.  Assuming everything is Gaussian, the optimal classifier in this scenario is called  Quadratic Discriminant Analysis (QDA) \cite{Hastie2004}.  Intuitively then, we can modify \Lol~to compute the eigenvectors separately for each class, and concatenate them (sorting them according to their singular values).  Moreover, rather than classifying the projected data with \Lda, we can then classify the projected data with QDA.  Indeed, simulating data according to such a model (Figure \ref{f:properties}E), \Lol~performs slightly better than chance, regardless of the number of dimensions we use to project, whereas QOQ (for Quadratic Optimal QDA) performs significantly better regardless of how many dimensions it keeps. This demonstrates a  straightforward generalization of \Lol, available to us because of the simplicity and intuitiveness of \Lol.

\para{Outliers}  Outliers persist in many real data sets.  Finding outliers, especially in high-dimensional data, is both tedious and difficult.  Therefore, it is often advantageous to have estimators that are robust to certain kinds of outliers \cite{Huber1981a,Rousseeuw1999a,Ferrari2010a}.  \Pca~and eigenvector computation are particularly sensitive to outliers \cite{Candes2009b}.  Because \Lol~is so simple and modular, we can replace typical eigenvector computation with a robust variant thereof, such as the geometric median subspace embedding \cite{Zhang2014a}.  Figure \ref{f:properties}F shows an example where we generated  $n/2$ training samples according to the simple \Lda~model, but then added another $n/2$ training samples from a noise model.  \Lrl~(Linear Robust Low-Rank), performs better than \Lol~regardless of the number of dimensions we keep. This simulation setting further demonstrates the flexibility of the \Lol~framework to be extensible to other, more complicated scenarios.

\para{XOR}  XOR is perhaps the simplest nonlinear problem, the problem that led to the demise of the perceptron, prior to its resurgence after the development of multi-layer perceptrons \cite{Bishop2006a}.  Thus, in our opinion, it is warranted to check whether any new classification method can perform well in this scenario.  The classical (two-dimensional) XOR problem is quite simple: the output of a classifier is zero if both inputs are the same (00 or 11), and the output is one if the inputs differ (01 or 10).  Figure \ref{f:properties}G shows a high dimensional and stochastic variant of XOR (see Methods for details).  This simulation was designed such that standard classifiers, such as support vector machines and random forests, achieve chance levels (not shown).  \Lol, performs moderately better than chance, and QOQ performs significantly better than chance, regardless of the chosen dimensionality.  This demonstrates that our classifiers developed herein, though quite simple and intuition, can perform well even in settings where the data are badly modeled by our underlying assumptions.  This mirrors previous findings where the so-called ``idiots's Bayes'' classifier outperforms more sophisticated classifiers \cite{Bickel2004a}.  In fact, we think of our work as finding intermediate points between idiot's Bayes (or na\"ive Bayes) and \Fld, by enabling degrees of regularization by changing the dimensionality used.


\subsection*{Computational Efficiency}

In many applications, the main quantifiable consideration in whether to use a particular method, other than accuracy, is computational efficiency.  Because implementing \Lol~requires only highly optimized linear algebraic routines---including computing moments and singular value decomposition---rather than the costly iterative programming techniques currently required for sparse or dictionary learning type problems.  Moreover, because \Lol~only requires computing means and covariances, we can leverage recent advances in parallel matrix operations \cite{FlashMatrix} on fast solid-state drives (SSDs). We implement LOL with the R interface of FlashX \cite{FlashGraph, FlashMatrix, FlashEigen}.
%More specifically, we modified FlashX to be able to perform all the computational primitives required for \Lol~on matrices that are short and fat, rather than tall and skinny.
Given the FlashX programming framework, we can run \Lol~on essentially arbitrarily large data.  For these examples, we simulated data satisfying the \Lda~model; specifically, we used a spherically symmetric covariance matrix, with means just $\pm$ the one vector (see Methods for simulation details).

Figure \ref{f:speed}A demonstrates both the in memory and semi-external memory \cite{SEM_SpMM} computing models of our implementation.  For the in memory implementation (light green line), we see that the run time increases linearly with the number of dimensions, requiring about 11 minutes to run \Lol~on a $p=32$,$000$,$000$ dimensional problem with $n=2000$ samples.   This linear increase is the optimal scale up according to Ahmdel's Law \cite{Amdahl1967}.  This example already requires 477 gigabyte (GB) to store the data, but $<$1 GB to perform the computations.  For larger data, we developed a semi-external memory implementation, which stores the data matrix on solid state drives, and the low-rank estimates in RAM \cite{abello1998functional}.  This strategy allows us to run \Lol~as long as the data can fit on disk, and the low-rank approximation can fit in RAM.  The semi-external memory implementation (dark green line) achieves the same performance as the in memory implementation whenever the data are small enough for in memory, and continues scaling optimally (linearly) as the dimensionality further increases to $p=128,000,000$ dimensions, a multi-terabyte scenario.   

Another option for making \Lol~scale up better is to utilize randomized algorithms.  In particular, random projections---for which the data are multiplied by a lower-dimensional random matrix---have been shown to provide excellent approximation eigenvectors \cite{Candes2006b}.  Moreover, very sparse random projections, in which the elements of the matrix are mostly zero, with $\pm 1$ randomly distributed, have been shown to be effective, and have significant computational benefits \cite{Hastie2006}. We therefore further modified FlashX to incorporate random projections and very sparse random projections, and let \Lal~denote Linear Approximate Low-rank.  Figure \ref{f:speed}A shows an order of magnitude improvement in both the in-memory and semi-external memory implementations of \Lal.


Figure \ref{f:speed}B shows the error for \Lol~and \PoF~in this scenario, simply to demonstrate that even in this extremely high dimensional setting, \Lol~still outperforms \PoF. As expected, \Lal~performs as well as \Lol for the low dimensional settings.

% we also implement a randomized variant of \Lol, that uses a novel FlashX implementation of random projection (in memory).  In this simulation setting, \Lfl~(short for Linear Fast Low-rank) performs just as well as \Lol, though at a substantially reduced computational burden. 


% To quantify the computational efficiency of \Lol~and its variants, Figure \ref{f:speed} shows the wall time it takes to run each method on the stacked cigars problem, varying the ambient dimensionality, embedded dimensionality, and sample size.  Note that for completeness, we include two additional variants of \Lol: \Lal~and \Lfl.  \Lfl~(short for Linear Fast Low-rank) replaces the standard \Svd~algorithm with a randomized variant, which can be much faster in certain situations \cite{Halko2011a}.  \Lal~(short for Linear Approximate Low-rank) goes even one step further, replacing \Svd~with random projections \cite{Candes2006a}.  This variant of \Lol~is the fastest, its runtime is  least sensitive to $(p,d,n)$, and its accuracy is often commensurate (or better) than other variants of \Lol.  The runtime of all the variants of \Lol~are quite similar to \sct{Fld $\circ$ Pca}.  Given, given \Lol's improved accuracy, and nearly identical simplicity, it seems there is very little reason to not use \Lol~instead of \sct{Fld $\circ$ Pca}.

\begin{figure}[h!]
\centering
% \includegraphics[width=1\linewidth]{../Figs/speed_test}
\includegraphics[width=1\linewidth]{../Figs/scalability}
\caption{
Computational efficiency of various low-dimensional projection methods. In all cases, $n=2000$, and we used spherically symmetric  simulation parameters (see Methods for details).   We compare \Pca~with the projection step of \Lol~(light green for in memory, dark green for semi-external memory ) and \Lfl~(light orange for in-memory, dark orange for semi-external memory) for different observed dimensions ($p$). \textbf{(A)} \Lol~exhibits optimal (linear) scale up and scale out, requiring only 46 minutes to find the embedding on a 2TB dataset, and only 3 minutes using \Lal (the sparse constant of sparse random projection $c=\dfrac{1}{\sqrt{p}}$). \textbf{(B)} Error for \Lal~is the same as \Lol~in this setting, and both are significantly better than \PoF~for all choices of embedding dimension.  
% from \Lol, \Qoq, \Lrl, \Lfl, and \Lal, for different values of $(p,d)$.  The addition of the mean difference vector is essentially negligible.  Moreover, for small $d$, the \Lfl~is advantageous.  \Lal~is always fastest, and its performance is often comparable to other methods (not shown).
}
\label{f:speed}
\end{figure}

\para{Computational Theory}

We reify the above computational experiments with theoretical statements.  Computing the mean for each class requires $\mc{O}(n_j p)$ floating point operations (flops), where $n_j$ is the number of samples per class. Subtracting the means requires $\mc{O}((J-1) p)$ flops, where $J$ is the total number of classes.  Computing the first $d$ singular triples (left and right singular vectors, and singular value) requires $\mc{O}(n p d)$ flops.
The sparse random projection however only requires $\mc{O}(n p d / c)$, where $c1$ is the sparsity of the sparse random projection matrix.
%The randomized version of \Svd~however only requires $\mc{O}(d^2 (n + p))+ (q+1) 2 d M$, where $M$ is the flop count required for matrix vector multiply.
Since, $d \ll n,p$, this can substantially reduce computational complexity.  Regardless, \Lol~computation is clearly bottlenecked by the \Pca~computation or random projection for single threaded operations.

Our parallel in memory implementation scales-up optimally (linearly) with the number of threads, requiring $\mc{O}(n p d / T)$ flops for \Lol, 
and $\mc{O}(n p d / c T)$ flops for \Lal, 
where $T$ is the number of threads.  Our parallel semi-external memory implementation scales-out optimally (linearly) as well.  This is in agreement with the linear fit of the lines in Figure \ref{f:speed}A.


% first computing a dot product, which requires $\mc{O}(n p d/T)$ space and time. Given that dot product, the result is an $n \times n$ matrix, so computing the first $d$ singular triples only requires $\mc{O}( n^2 d / T)$ space and time.  Therefore, the most computationally taxing operation is the dot product.  This is, in fact, the motivation for developing a randomized version of \Lol.  Specifically, if we employ a very sparse random projection matrix \cite{??} with only $z$ non-zeros, then the dot product operation reduces to $\mc{O}(n z p/T)$, which can yield a significant computational savings. 



\subsection*{Benchmark Real Data Applications}



\begin{figure}
\centering
\includegraphics[width=1\linewidth]{../Figs/realdata}
\caption{
For four standard datasets, we benchmark \Lol~(green circles) versus standard classification methods, including support vector machines (blue up triangles), \Road~(cyan down triangles), \sct{Lasso}~(magenta pluses), and random forest (orange diamonds).
Top panels show error rate as a function of log$_2$ number of embedded dimensions (for \Lol, \Road, and \sct{Lasso}) or cost (for SVM).
% \Lol~consistently achieves lower error for fewer dimensions.
Bottom panels show the minimum error rate achieved by each of the five algorithms versus time.
The lower left dark gray (upper right light gray) rectangle is the area in which any algorithm is \emph{better}  (worse) than \Lol~in terms of both accuracy and efficiency.
\textbf{(A)} Prostate: a standard sparse dataset.  1-dimensional \Lol~does very well, although keeping $2^5$ ambient coordinates slightly improves performance, at a significant cost of compute time (two orders of magnitude), with minimal additional interpretability.
\textbf{(B)} Colon: another standard sparse dataset.  Here, 2-4 dimensions of \Lol~outperforms all other approaches considered regardless of how many dimensions they keep.
\textbf{(C)} MNIST: 10 image categories, so \Road~is not possible.  \Lol~does very well regardless of the number of dimensions kept.  SVN marginally improves on \Lol~accuracy, at a significant cost in computation (two orders of magnitude).
\textbf{(D)} CIFAR-10: a higher dimensional and newer 10 category image classification problem.  Results are qualitatively similar to C.
%
Note that, for all four of the problems, there is no algorithmperforming better and faster than \Lol; rather, most algorithms typically perform worse and slower (though some are more accurate and much more computationally expensive).
% This suggests that regardless of how one subjectively weights computational efficiency versus accuracy, \Lol~is the best default algorithm in a variety of real data settings.
}
\label{f:realdata}
\end{figure}

Although \Lol~both statistically and computationally outperforms its natural competitors both in theory and in a variety of simulation settings, real data often tell a different story. 
% 
% To more comprehensively understand the relative advantages and disadvantages of \Lol~with respect to other high-dimensional classification approaches, in addition to evaluating its performance in theory, and in a variety of numerical simulations, it is important to evaluate it also on benchmark datasets.  
We have  therefore selected four commonly used high-dimensional datasets to compare \Lol~to several state-of-the-art algorithms (see Methods for details).  For each dataset, we compare \Lol~to (i) support vector machines (SVM), (ii) \Road, (iii) lasso, (iv) and random forest (RF).  Because in practice all these approaches have ``hyperparameters'' to tune, we consider several possible values for  SVM, lasso, and \Lol~(but not RF, as its runtime was too high).  Figure \ref{f:realdata} shows the results for all four datasets. For each, we use MATLAB implementations, partially because \Road~provides MATLAB code, and also because of MATLAB's ubiquity in certain communities.  Because we also have R and FlashX implementations of \Lol, comparisons using other languages would be straightforward.





Qualitatively, the results are similar across datasets: \Lol~achieves high accuracy and computational efficiency as compared to the other methodologies.  Considering Figure \ref{f:realdata}A and B, two popular sparse settings, we find that \Lol~can find very low dimensional projections with very good accuracy. For the prostate data, with a sufficiently non-sparse solution for \Road, it slightly outperforms \Lol, but at substantial computational cost; in particular, \Road~takes about 100 times longer to run on this dataset.   Figure \ref{f:realdata}C and D are 10-class problems, so \Road~is no longer possible.  Here, SVM can again slightly outperform \Lol, but again, requiring 100 fold additional computational time.  In all cases, the beloved random forest classifier performs subpar. In all four scenarios, there is never an algorithm that achieves smaller error in less time, as indicated by the dark gray box being empty in the lower panels of Figure \ref{f:realdata}A-D.


\subsection*{Extensions to Other Supervised Learning Problems}

The utility of incorporating the mean difference vector into supervised machine learning for big and wide data extends beyond merely classification.  In particular, hypothesis testing can be considered as a special case of classification, with a particular loss function.  Therefore we apply the same idea to a hypothesis testing scenario.  The multivariate generalization of the t-test, called Hotelling's Test, suffers from the same problem as does the classification problem; namely, it requires inverting an estimate of the covariance matrix whose estimate would be low-rank and therefore singular, in the high-dimensional setting. To mitigate this issue in the hypothesis testing scenario, prior art applied similar tricks as they have done in the classification setting. One particularly nice and related example is that of  Lopes et al. \cite{Lopes2011a}, who addresses this dilemma by using random projections to obtain a low-dimensional representation, following by applying Hotelling's Test in the lower dimensional subspace.  Figure \ref{f:generalizations}A and B shows the power of their test alongside the power of the same approach, but using the \Lol~projection rather than random projections.  The two different simulations include the simulated settings considered in their manuscript (see Methods for details).  The results make it clear that the \Lol~test has higher power for essentially all scenarios.  Moreover, it is not merely the replacing random projections with \Pca~(solid magenta line), nor simply incorporating the mean difference vector (dashed green line), but rather, it appears that \Lol~for testing uses both modifications to improve performance.

High-dimensional  regression is another supervised learning method that can utilize the \Lol~idea. Linear regression, like classification and Hotelling's Test, requires inverting a  matrix as well.  By projecting the data only a lower dimensional subspace first, followed by linear regression on the low-dimensional data, we can mitigate the curse of high-dimensions.  To choose the projection matrix, we partition the data into K partitions, based on the percentile of the target variable, we obtain a K class classification problem.  Then, we can apply \Lol~to learn the embedding.  Figure \ref{f:generalizations}C shows an example of this approach, contrasted with \Lasso~and partial least squares, in a sparse simulation setting (see Methods for details). \Lol~is able to find a better low-dimensional projection than \Lasso, and performs significantly better than partial least squares, for essentially all choices of number of dimensions to embed into.




\begin{figure}
% \begin{wrapfigure}{R}{0.7\textwidth} %[h!]
\centering
\includegraphics[width=1\linewidth]{../Figs/regression_power}
\caption{
The intuition of including the mean difference vector is equally useful for other supervised manifold learning problems, including testing and regression.
\textbf{(A)} and \textbf{(B)} show two different high-dimensional testing settings, as described in Methods.  Power is plotted against the decay rate of the spectrum, which approximates the effective number of dimensions.  \Lol~composed with Hotelling's test outperforms the random projections variants described in \cite{Lopes2011a}, as well as several other variants.
\textbf{(C)} A sparse high-dimensional regression setting, as described in Methods, designed for sparse methods to perform well.  Log$_{10}$ mean squared error is plotted against the number of embedded dimensions.
\Lol~composed with linear regression outperforms \sct{Lasso}~(cyan), the classic sparse regression method, as well as partial least squares (PLS; black).
% In the legend, 'A' denote either 'linear regression' (in C), or 'Hotelling' (in A and B).
These three simulation settings therefore demonstrate the generality of this technique.
}
\label{f:generalizations}
% \end{wrapfigure}
\end{figure}
% \clearpage


\section*{Discussion}
% get 64 lines, only using 20.

We have introduced a very simple, yet new, methodology to improve performance on supervised learning problems with big and wide data.  In particular, we have proposed a supervised manifold learning procedure that utilizes both the difference of the means, and the covariance matrices, and proved that it performs better than first applying \Pca~to the data under reasonable assumptions.  This is in stark contrast to most previous approaches, which only utilize the covariance matrices (or kernel variants thereof), or try to solve a difficult optimization theoretic problem.  In addition to demonstrating the statistical accuracy and computational efficiency of \Lol~on simulated and real classification problems, we also demonstrate how the same idea can also be used for other kinds of supervised learning problems, including regression and hypothesis testing. Theoretical guarantees suggest that this line of research is promising and can be extended to other more general settings and tasks.


% \subsection*{Qualitative Design Considerations}

% Finally, in addition to the above mentioned quantitative criteria, there are also many qualitative criteria that merit consideration at each level of analysis.  For the problem specification level,  the model choices also determine the  \textbf{appropriateness}.  Indeed, real decision criteria are often quite difficult to specify precisely, and so our problem specification is almost always related to but not exactly the problem we hope to achieve.  For example, while we might want to parse a scene, we often tackle a simpler problem of checking whether an object of a particular kind is present because it is easier.  For the algorithm level, the \textbf{interpretability} of the resulting estimator is often an important consideration.  For example, when developing biomarkers, it is important that the doctors and insurance companies can understand the decision making process of the algorithm, otherwise they might not believe the results. For the implementation level,  how \textbf{accessible} is the implementation is crucial.  In particular as more people outside the ivory academic tower---such as citizen scientists and even academics in the developing world---are contributing to data science and scientific discovery in general, access to the implementation is increasingly important.  Finally, at the platform level, the \textbf{ease of use} of the platform is increasingly important.  For example, requiring tedious installations of many different software libraries, or specialized hardware, can make the platform difficult to employ.  

\para{Related Work} 
% 
One of the first publications to compose \Fld~with an unsupervised learning method was the celebrated Fisherfaces paper \cite{Belhumeur1997a}.  The authors showed via a sequence of numerical experiments the utility of embedding with \Pca~prior to classifying with \Fld.  We extend this work by adding a supervised component to the initial embedding.  Moreover, we provide the geometric intuition for why and when this is advantageous, as well as show numerous examples demonstrating its superiority.  %Finally, we have matrix concentration inequalities proving the advantages of \Lol~over Fisherfaces.
We also prove that Fisherfaces can be implemented very efficiently (see Methods), while shedding light on why it is performing relatively well in general. 

% Most manifold learning methods, while exhibiting both strong theoretical \cite{Eckart1936a,deSilva2003, Allard2012} and empirical performance, are fully unsupervised.  Thus, in classification problems, they discover a low-dimensional representation of the data, ignoring the labels.  This can be highly problematic when the discriminant dimensions and the directions of maximal variance in the learned manifold are not aligned (see Figure \ref{f:mnist} for an example).  Supervised dimensionality reduction techniques, therefore, combine the best of both worlds, explicitly searching for low-dimensional discriminant boundaries.  

% A set of methods from the statistics community is collectively referred to as   ``sufficient dimensionality reduction'' (SIR) or ``first two moments'' (F2M) methods  \cite{Li1991a, Tishby1999a, Globerson2003a, Cook2005a,Fukumizu2004a}.  These methods are theoretically elegant, but typically require the sample size to be larger than the number of observed dimensions (although see \cite{Cook2013} for some promising work).  Other approaches formulate an optimization problem, such as projection pursuit \cite{Huber1985a}, empirical risk minimization \cite{Belkin2006a}, or supervised dictionary learning \cite{Mairal2009}.  These methods are limited because they are prone to fall into local minima, they require costly iterative algorithms, and lack any theoretical guarantees \cite{Belkin2006a}.   Thus, there remains a gap in the literature: a supervised learning method with theoretical convergence guarantees appropriate when the dimensionality is orders of magnitude larger than the sample size.



\para{Next Steps}
% 
The \Lol~idea, appending the mean difference vector to convert unsupervised manifold learning to supervised manifold learning, has many potential applications.  We have presented the first few.  Incorporating additional nonlinearities via kernel methods \cite{Mika1999a}, ensemble methods such as random forests \cite{Breiman2001a}, and multiscale methods \cite{Allard2012}
% ,  and more scalable implementations \cite{Chang2011a}, 
are all of immediate interest. MATLAB, R, and FlashR code for the experiments performed in this manuscript is available from \url{http://docs.neurodata.io/LOL/}.

\clearpage
\appendix

\section{Simulations}

% \subsection{Simulations}

For most simulation settings, each class is Guassian:
$f_{x|y} = \mc{N}(\bmu_y,\mb{\Sigma_y})$,
$f_y = \mc{B}(\pi)$.  
We typically assume that both classes are equally like, $\pi=0.5$, and the covariance matrices are the same, $\bSig_0=\bSig_1=\bSig$. Under such assumptions, we merely specify $\bth=\{\bmu_0,\bmu_1, \bSig\}$.


\paragraph*{Stacked Cigars} 
\begin{compactitem}
\item $\bmu_0=\mb{0}$, 
\item $\bmu_1=(a, b, a, \ldots, a)$, 
\item $\bSig$ is a diagonal matrix, with diagonal vector, $\bd=(1,b,1\ldots,1)$,
\end{compactitem}
where $a=0.15$ and $b=4$.


\paragraph*{Trunk} 
\begin{compactitem}
\item $\bmu_0=b/\sqrt{(1, 3, 5, \ldots, 2p)}$, 
\item $\bmu_1=-\bmu_0$, 
\item $\bSig$ is a diagonal matrix, with diagonal vector, $\bd=100/\sqrt{(p, p-1, p-2, \ldots, 1)}$,
\end{compactitem}
where $b=4$.

% if ~isfield(task,'b'), b=4; else b=task.b; end
% mu1=b./sqrt(1:2:2*D)';
% mu0=-mu1;

% Sigma=eye(D);
% Sigma(1:D+1:end)=100./sqrt(D:-1:1);

\paragraph*{Rotated Trunk} Same as Trunk, but the data are randomly rotated, that is, we sample $\mb{Q}$ uniformly from the set of p-dimensional rotation matrices, and then set:
\begin{compactitem}
\item $\bmu_0 \leftarrow \mb{Q} \bmu_0$, 
\item $\bmu_1 \leftarrow \mb{Q} \bmu_1$, 
\item $\bSig \leftarrow \mb{Q} \bSig \mb{Q}\T$.
\end{compactitem}


\paragraph*{Toeplitz} 
\begin{compactitem}
\item $\bmu_0=b \times (1,-1,1,-1,\ldots,1)$, 
\item $\bmu_1= - \bmu_0$, 
\item $\bSig$ is a Toeplitz matrix, where the top row is $\rho^{(0,1,2,\ldots,p-1)}$,
\end{compactitem}
where $b$ is a function of the Toeplitz matrix such that the noise stays constant as dimensionality increases, and $\rho=0.5$.

% D1=10;
% rho=0.5;
% c=rho.^(0:D1-1);
% A = toeplitz(c);
% K1=sum(A(:));

% c=rho.^(0:D-1);
% A = toeplitz(c);
% K=sum(A(:));

% mudelt=(K1*b^2/K)^0.5/2;
% mu0 = ones(D,1);
% mu0(2:2:end)=-1;
% mu0=mudelt*mu0;
% mu1=-mu0;

% Sigma=A;


\paragraph*{3 Classes} Same as Trunk, but with a third mean equal to the zero vector, $\mu_2=\mb{0}$.


\paragraph*{Fat Tails} For this setting, each class is actually a mixture of two Gaussians with the same mean (the two classes have the same covariances): 
\begin{compactitem}
\item $\bmu_0=\mb{0}$, 
\item $\bmu_1=(0,\ldots,0,1,\ldots,1)$, where the first $s=10$ elements are zero, 
\item $\bSig_0$ is a matrix with one's on the diagonal, and $0.2$ on the off diagonal,
\item $\bSig_1 = 15 \times \bSig_0$,
\end{compactitem}
and then we randomly rotated as in the rotated Trunk example.




\paragraph*{QDA} A generalization of the Toeplitz setting, where the two classes have two different covariance matrices, meaning that the optimal discriminant boundary is quadratic.
\begin{compactitem}
\item $\bmu_0=b \times (1,-1,1,-1,\ldots,1)$, 
\item $\bmu_1=-\mb{Q} \times (\bmu_0+0.1)$, 
\item $\bSig_0$ is the same Toeplitz matrix as described above, and 
\item $\bSig_1 = \mb{Q} \bSig_0 \mb{Q}\T$.
\end{compactitem}



% if ~isfield(task,'b'), b=0.4; else b=task.b; end
% D1=10;

% rho=0.5;
% c=rho.^(0:D1-1);
% A = toeplitz(c);
% K1=sum(A(:));

% c=rho.^(0:D-1);
% A = toeplitz(c);
% K=sum(A(:));
% Sigma=A;

% mudelt=(K1*b^2/K)^0.5/2;
% mu0 = ones(D,1);
% mu0(2:2:end)=-1;
% mu0=mudelt*mu0;


% [Q, ~] = qr(randn(D));
% if det(Q)<-.99
%     Q(:,1)=-Q(:,1);
% end

% th=pi/4;
% Q(1:2,1:2)=[cos(th) -sin(th); sin(th) cos(th)];
% Q(1,3:end)=0;
% Q(2,3:end)=0;
% Q(3:end,1)=0;
% Q(3:end,2)=0;

% Sigma(:,:,2)=Q*Sigma*Q';
% mu1=-Q*(mu0+0.1);



\paragraph*{Outliers} In this dataset, we generate $n/2$ samples from an inlier model, and the remaining $n/2$ samples from an outlier model.  For the inlier model, we first generate a random $d \times p$ dimensional orthonormal matrix, $V$, where $d=p/10$.  Then, the first half of the inlier points are generated by $f_{x|0}$, the next half by $f_{x|1}$, and the remaining points generated by $f_{x | \emptyset}$. For the outliers, we sampled their class randomly from a fair Bernoulli distribution:   
\begin{compactitem}
\item $f_{x|0} = \mc{N}_d(0,\sigma^2) \times V\T$,
\item $f_{x|1} = \mc{N}_d(0,\sigma^2) \times V\T + b$,
\item $f_{x|\emptyset} = \mc{N}_p(\mb{0},\sigma^2\mb{I})$,
\item $f_{\emptyset} = \mc{B}(0.5)$.
\end{compactitem}
where we set $\sigma=0.1$ and $b=0.5$.  

% Ninlier=task.n/2; 
% n0=Ninlier/2;
% n1=n0;
% Noutlier=task.n-Ninlier;
% task.D=200;
% d=round(task.D/10);
% noise=0.1;
% offset=0.5;
% V=orth(rand(task.D,d));
% X0=randn(n0,d)*V';
% X1=randn(n1,d)*V'+offset;
% X=[X0;X1; randn(Noutlier,task.D)];
% X=X+randn(size(X))*noise;

% Y=[zeros(n0,1); ones(n1,1); rand(Noutlier,1)>0.5]+1;




\clearpage

\begin{figure}
\centering
\includegraphics[width=1\linewidth,trim=0.5in 4.5in 0.5in 0.5in,clip=true]{../Figs/table} %l b r t
\caption{Table of algorithms and their properties for high-dimensional data. Gray elements indicate that results are demonstrated in the Figure labeled in the bottom row. 'X' denotes relatively good performance for a given setting, or has the particular property.
}
\label{f:table}
\end{figure}



\clearpage





\section[theory]{Theoretical Background}


\subsection{The Classification Problem}

Let $(\bX,Y)$ be a pair of random variables, jointly sampled from $F :=F_{\bX,Y}=F_{\bX|Y}F_{Y}$.
Let $\bX$ be a multivariate vector-valued random variable, such that its realizations live in p dimensional Euclidean space, $\bx \in \Real^p$.  Let $Y$ be a categorical random variable, whose realizations are discrete,  $y \in \{0,1,\ldots C\}$.  The goal of a classification problem is to find a function $g(\bx)$ such that its output tends to be the true class label $y$:
\begin{align*} %\label{eq:bayes}
g^*(\bx) := \argmax_{g \in \mc{G}} \PP[g(\bx) = y].
\end{align*}
When the joint distribution of the data is known, then the Bayes optimal solution is:
\begin{align}  \label{eq:R}
g^*(\bx) := \argmax_y f_{y|\bx} = \argmax_y f_{\bx|y}f_y =\argmax_y \{\log f_{\bx|y} + \log f_y \}
\end{align}
Denote expected misclassification rate of classifier $g$ for a given joint distribution $F$,
\begin{align*}
L^F_g := \EE[g(\bx) \neq y] := \int \PP[g(\bx) \neq y] f_{\bx,y} d\bx dy,
\end{align*}
where $\EE$ is the expectation, which in this case, is with respect to $F_{XY}$.
For brevity, we often simply write $L_g$, and we define $L_* := L_{g^*}$.


\subsection{Linear Discriminant Analysis (\Lda)}

Linear Discriminant Analysis (\Lda) is an approach to classification that uses a linear function of the first two moments of the distribution of the data.  More specifically, let $\mu_j=\EE[F_{X|Y=j}]$ denote the class conditional mean, and let $\bSig=\EE[F_{X}^2]$ denote the joint covariance matrix, and $\pi_j=\PP[Y=j]$.   Using this notation, we can define the \Lda~classifier:
\begin{align*}
g_{\Lda}(\bx)&:=\argmin_y \frac{1}{2} (\bx-\bmu_0)\T \bSig^{-1}(\bx-\bmu_0) + \II\{Y=y\}  \log \pi_y,
\end{align*}
where $\II\{ \cdot\}$ is one when its argument is true, and zero otherwise.
Let $L_{\Lda}^F$ be the misclassification rate of the above classifier for distribution $F$.
%
Assuming equal class prior and centered means,  $\pi_0=\pi_1$ and $(\bmu_0+\bmu1)/2=\mb{0}$, re-arranging a bit, we obtain
\begin{align*}
g_{\Lda}(\bx) :=  \argmin_y \bx\T \bSig^{-1} \bmu_y.
\end{align*}
In words, the  \Lda~classifier chooses the class for whom the projection of an input vector $\bx$, onto $\bSig^{-1} \bmu_y$, is maximized.
%
When there are only two classes, this further simplies to
\begin{align*}
g_{2-\Lda}(\bx) :=  \II\{ \bx\T \bSig^{-1} \bdel > 0 \},
\end{align*}
where $\bdel=\bmu_0-\bmu_1$.   Note that the equal class prior and centered means assumptions merely changes the threshold constant from $0$ to something else.

\subsection{\Lda~Model}

A statistical model is  a family of distributions indexed by a parameter $\bth \in \bTh$, $\mc{F}_{\bth}=\{F_{\bth} : \bth \in \bTh \}$.
Consider the special case of the above where $F_{\bX|Y=y}$ is a multivariate Gaussian distribution,
$\mc{N}(\bmu_y,\bSig)$, where each class has its own mean, but all classes have the same covariance.
We refer to this model as the \Lda~model.
Let $\bth=(\bpi,\bmu,\bSig)$, and let $\bTh_{C-\Lda}=( \triangle_C, \Real^{p \times C},\Real_{\succ 0}^{p \times p})$, where $\bmu=(\bmu_1,\ldots, \bmu_C)$, $\triangle_C$ is the $C$ dimensional simplex, that is $\triangle_C = \{ \bx : x_i \geq 0 \forall i, \sum_i x_i = 1\}$, and $\Real_{\succ 0}^{p \times p}$ is the set of positive definite  $p \times p$ matrices. Denote
$\mc{F}_{\Lda}=\{F_{\bth} : \bth \in \bTh_{\Lda}\}$, dropping the superscript $C$ for brevity where appropriate.
The following lemma is well known:
\begin{lem}
$L_{\Lda}^F=L_*^F$ for any $F \in \mc{F}_{\Lda}$.
\end{lem}

\begin{proof}
Under the \Lda~model, the Bayes optimal classifier is available by plugging the explicit distributions into Eq.~\eqref{eq:R}.
\end{proof}




\section[projections]{Projection Based Classifiers}


Let $\bA \in \Real^{d \times p}$ be an orthonormal matrix, that is, a matrix that projects p dimensional data into a d dimensional subspace, where $\bA\bA\T$ is the $d \times d$ identity matrix, and $\bA\T \bA$ skis symmetric $p \times p$ matrix with rank d.   The question that motivated this work is: what is the best projection matrix that we can estimate, to use to ``pre-process'' the data prior to applying \Lda.
% \begin{lem}
% $g_{\Lda}^F(\bA \bx)= \II \{ (\bA \bx)\T \bSig^{-1}_A \bdel_A > 0\}$.
% \end{lem}
Projecting the data $\bx$ onto a low-dimensional subspace, and the classifying via \Lda~in that subspace is equivalent to redefining the parameters in the low-dimensional subspace,
$\bSig_A=\bA \bSig \bA\T \in \Real^{d \times d}$ and $\bdel_A = \bA \bdel \in \Real^d$, and then using $g_{\Lda}$.  When $C=2$, $\pi_0=\pi_1$, and $(\mu_0+\mu_1)/2=\mb{0}$, this amounts to:
\begin{align} \label{eq:g_A}
g^d_A(x) := \II \{ (\bA \bx)\T \bSig^{-1}_A \bdel_A > 0\}, \text{ where } \bA \in \Real^{d \times p}.
\end{align}
Let $L^d_A :=\int \PP[g_A(\bx)=y] f_{\bx,y} d\bx dy$.
Our goal therefore is to be able to choose $A$ for a given parameter setting $\bth=(\bpi, \bdel,\bSig)$, such that $L_A$ is as small as possible (note that $L_A$ will never be smaller than $L_*$).

Formally, we seek to solve the following optimization problem:
% \begin{align} \label{eq:A}
% \bA_* = \argmin_{\bA \in \Real^{p \times d}} L_A.
% \end{align}
\begin{equation} \label{eq:A}
\begin{aligned}
& \underset{\bA}{\text{minimize}}
& & \EE [ \II \{ \bx\T \bA\T \bSig^{-1}_A \bdel_A > 0\} \neq y] \\
& \text{subject to} & & \bA \in \Real^{p \times d}, \quad \bA \bA\T = \bI_{d \times d},
\end{aligned}
\end{equation}
where $\bI_{u \times v}$ is the $u \times v$ identity matrix identity, that is, $\bI(i,j)=1$ for all $i=j \leq \min(u,v)$, and zero otherwise.
Let $\mc{A}^d=\{\bA : \bA \in \Real^{d \times p}, \bA \bA\T = \bI_{d \times d}\}$, and let $\mc{A}_* \subset \mc{A}$ be the set of $\bA$  that minimize Eq.~\eqref{eq:A}, and let $\bA_* \in \mc{A}_*$ (where we dropped the superscript $d$ for brevity).   Let $L_{\bA}^*=L_{\bA_*}$ be the misclassification rate for any $\bA \in \mc{A}_*$, that is, $L_{\bA}^*$ is the Bayes optimal misclassification rate for the classifier that composes $\bA$ with \Lda.


In our opinion, Eq.~\eqref{eq:A} is the simplest supervised manifold learning problem there is: a two-class classification problem, where the data are multivariate Gaussians with shared covariances, the manifold is linear, and the classification is done via \Lda.
Nonetheless, solving Eq.~\eqref{eq:A} is difficult, because we do not know how to evaluate the integral analytically, and we do not know any algorithms that are guaranteed to find the global optimum in finite time.  This has led to previous work using a surrogate function \cite{not sure who}.
We proceed by studying a few natural choices for $\bA$.





\subsection{Bayes Optimal Projection}

% Let $\mb{\bA}\T=\bSig^{-1} \bdel$.
\begin{lem}
$\bdel\T  \bSig^{-1} \in \mc{A}_*$
\end{lem}

\begin{proof}
Let $\bB = (\bSig^{-1} \bdel)\T = \bdel\T (\bSig^{-1})\T = \bdel\T \bSig^{-1}$, so that $\bB\T = \bSig^{-1} \bdel$,
% = \bdel\T \bOm\T
and plugging this in to Eq.~\eqref{eq:g_A}, we obtain
\begin{align*}
g_{B}(x) &= \II \{ \bx \bB\T  \bSig^{-1}_{B} \bdel_{B} > 0\} &
\\&= \II \{ \bx\T \bSig^{-1} \bdel \times (\bSig^{-1}_{B} \bdel_{B}) > 0\} & \text{plugging in $\bB$}
\\&= \II \{ \bx\T \bSig^{-1} \bdel k > 0\} & \text{because $\bSig^{-1}_{B} \bdel_{B} > 0$}.
\end{align*}
In other words, letting $\bB$ be the Bayes optimal projection recovers the Bayes classifier, as it should.
Or, more formally, for any $F \in \mc{F}_{\Lda}$, $L_{\bdel\T \bSig^{-1}} = L_*$
\end{proof}

\subsection[PCA]{Principle Components Analysis (\Pca) Projection}

Principle Components Analysis (\Pca) finds the directions of maximal variance in a dataset.  \Pca~is closely related to eigendecompositions and singular value decompositions (\Svd).  In particular, the top principle component of a matrix $\bX \in \Real^{p \times n}$, whose columns are centered, is the eigenvector with the largest corresponding eigenvalue of the centered covariance matrix $\bX \bX\T$.  \Svd~enables one to estimate this eigenvector without ever forming the outer product matrix, because \Svd~factorizes a matrix $\bX$ into $\bU \bS \bV\T$, where  $\bU$ and $\bV$ are orthonormal  ${p \times n}$ matrices, and $\bS$ is a diagonal matrix, whose diagonal values are decreasing,  $s_1 \geq s_2 \geq \cdots > s_n$.  Defining $\bU =[\bu_1, \bu_2, \ldots, \bu_n]$, where each $\bu_i \in \Real^p$, then $\bu_i$ is the $i^{th}$ eigenvector, and $s_i$ is the square root of the $i^{th}$ eigenvalue of $\bX \bX\T$.  Let $\bA^{\Pca}_d =[\bu_1, \ldots , \bu_d]$ be the truncated \Pca~orthonormal matrix.

The \Pca~matrix is perhaps the most obvious choice of a orthonormal matrix for several reasons.  First, truncated \Pca~minimizes the squared error loss between the original data matrix and all possible rank d representations:
\begin{align*}
\argmin_{A \in \Real^{d \times p} : \bA \bA\T = \bI_{d \times d}} \norm{ \bX - \bA^T \bA }_F^2.
\end{align*}
Second, the ubiquity of \Pca~has led to a large number of highly optimized numerical libraries for computing \Pca~(for example, LAPACK \cite{Anderson1999a}).

Moreover, let $\bU_d=[\bu_1,\ldots,\bu_d] \in \Real^{p \times d}$, and note that $\bU_d\T \bU_d = \bI_{d \times p}$ and $\bU_d\T \bU_d  = \bI_{p \times d}$.  Similarly, let $\bU \bS \bU\T = \bSig$, and $\bU \bS^{-1} \bU\T = \bSig^{-1}$.  Let $\bS_d$ be the matrix whose diagonal entries are the eigenvalues, up to the $d^{th}$ one, that is $\bS_d(i,j)=s_i$ for $i=j \leq d$ and zero otherwise.  Similarly, $\bSig_d=\bU \bS_d \bU\T=\bU_d \bS_d \bU_d\T$.

Let $g_{\Pca}^d:=g_{A_{\Pca}^d}$, and let $L_{\Pca}^d:=L_{A_{\Pca}^d}$.
And let $g_{\Lda}^d := \II \{ x \bSig_d^{-1} \bdel > 0\}$ be the regularized \Lda~classifier, that is, the \Lda~classifier, but sets the bottom $p-d$ eigenvalues to zero.

\begin{lem}
$L_{\Pca}^d = L_{\Lda}^d$.
\end{lem}

\begin{proof}
Plugging $\bU_d$ into Eq.~\eqref{eq:g_A} for $\bA$, and considering only the left side of the operand, we have
\begin{align*}
(\bA \bx)\T \bSig^{-1}_A \bdel_A &= \bx\T \bA\T \bA \bSig^{-1} \bA\T \bA \bdel,
\\&= \bx\T  \bU_d\bU_d\T \bSig^{-1} \bU_d\bU_d\T \bdel,
\\&= \bx\T  \bU_d \bU_d\T \bU \bS^{-1} \bU \bU_d\bU_d\T \bdel,
\\&= \bx\T  \bU_d \bI_{d \times p} \bS^{-1} \bI_{p \times d} \bU_d\T \bdel,
\\&= \bx\T  \bU_d \bS^{-1}_d  \bU_d\T \bdel ,
\\&= \bx\T  \bSig^{-1}_d  \bdel.
\end{align*}
\end{proof}

The implication of this lemma is that if one desires to implement Fisherfaces, rather than first learning the eigenvectors and then learning \Lda, one can instead directly implement regularized \Lda~by setting the bottom $p-d$ eigenvalues to zero.




\subsection[LOL]{Linear Optimal Low-Rank (\Lol) Projection}


The basic idea of \Lol~is to use both $\bdel$ and the top $d$ eigenvectors.  Most na\"ively, we could simply concatenate the two, $\bA_{\Lol}^d=[\bdel,\bA_{\Pca}^{d-1}]$.
Recall that eigenvectors are orthonormal.  To maintain orthonormality, we could easily apply Gram-Schmidt,  $\bA_{\Lol}^d=$ \sct{Orth}$([\bdel, \bA_{\Pca}^{d-1}])$.
Both in practice and in theory (as will be shown below), this orthogonalization step does not matter much.

to ensure that they are balanced appropriately, we normalize $\bdel$

each vector in $\bdel$ to have norm unity.  Formally, let $\mt{\bdel}_j = \bdel_j / \norm{\bdel_j}$, where $\bdel_j$ is the $j^{th}$ difference of the mean vector (remember, the number of vectors is equal to $C-1$, where $C$ is the total number of classes), and let  $\bA_{\Lol}^d=[\mt{\bdel}, \bA_{\Pca}^{d-(C-1)}]$.
The eigenvectors are all normalized and orthogonal to one another; to impose orthogonality between $\mt{\bdel}$ and the eigenvectors, we could use any number of numerically optimized algorithms.  However, in practice, orthogonalizing does not matter very much, so we do not bother. We formally demonstrate this below.




\section[LDA]{Theoretical Properties of LDA based Classifiers}


\subsection{\Lda~is rotationally invariant}

For certain classification tasks, the ambient coordinates have intrinsic value, for example, when simple interpretability is desired.  However, in many other contexts, interpretability is less important \cite{Breiman2001b}.  When the exploitation task at hand is invariant to rotations, then we have no reason to restrict our search space to be sparse in the ambient coordinates, rather, for example, we can consider sparsity in the eigenvector basis.  Fisherfaces is one example of a rotationally invariant classifier, under certain model assumptions.
Let  $\bW$ be a rotation matrix, that is $\bW \in \mc{W}=\{\bW : \bW\T = \bW^{-1}$ and det$(\bW)=1\}$.
Moreover, let $\bW \circ F$ denote the distribution $F$ after transformation by an operator $\bW$.  For example, if $F=\mc{N}(\bmu,\bSig)$ then $\bW \circ F=\mc{N}(\bW  \bmu, \bW \bSig \bW\T)$.

\begin{defi}
A rotationally invariant classifier has the following property:
$$L_g^F = L_g^{W \circ F}, \qquad F \in \mc{F}.$$
In words, the Bayes risk of using classifier $g$ on distribution $F$ is unchanged if $F$ is first rotated, for any $F \in \mc{F}$.
\end{defi}


Now, we can state the main lemma of this subsection:  \Lda~is rotationally invariant.
\begin{lem} \label{l:rot}
$L_{\Lda}^F = L_{\Lda}^{W \circ F}$, for any $F \in \mc{F}$.
\end{lem}

\begin{proof}
\Lda~simply becomes thresholding $\bx\T \bSig^{-1} \bdel$.  Thus, we can demonstrate rotational invariance by demonstrating that $\bx\T \bSig^{-1} \bdel$ is rotationally invariant.

% First, note that for any distribution $F \in \mc{F}_{\Lda}$, we can reparameterize it such that $\bSig$ is diagonal.  This follows because for any $\bSig$, we can represent it as $\bSig=\bU \bS \bU$, and there exists a $\bW$ such that
% from the following:
% \begin{align}
% \bSig = \bU \bS \bU =
% \end{align}

\begin{align*}
% \bx\T \bSig^{-1} \bdel &=
(\bW \bx) \T  (\bW \bSig \bW\T )^{-1} \bW \bdel  %& \text{from Lemma \ref{l:rot}}\\
&= \bx\T \bW\T  (\bW \bU \bS \bU\T \bW\T)^{-1} \bW \bdel & \text{by substituting $\bU \bS \bU\T$ for $\bSig$} \\
&= \bx\T \bW\T  (\mt{\bU} \bS \mt{\bU}\T)^{-1} \bW \bdel & \text{by letting $\mt{\bU}=\bW \bU$} \\
&= \bx\T \bW\T  (\mt{\bU} \bS^{-1} \mt{\bU}\T) \bW \bdel & \text{by the laws of matrix inverse} \\
&= \bx\T \bW\T  \bW \bU \bS^{-1}  \bU\T \bW\T \bW \bdel & \text{by un-substituting $\bW \bU=\mt{\bU}$} \\
&= \bx\T  \bU \bS^{-1}  \bU\T  \bdel  & \text{because $\bW\T \bW = \bI$} \\
&= \bx\T   \bSig^{-1} \bdel & \text{by un-substituting $\bU \bS^{-1} \bU\T = \bSig$}
\end{align*}
\end{proof}

One implication of this lemma is that we can reparameterize without loss of generality.  Specifically, defining $\bW := \bU\T$ yields a change of variables: $\bSig \mapsto \bS$ and $\bdel \mapsto \bU\T \bdel := \bdel''$, where $\bS$ is a diagonal covariance matrix.  Moreover, let $\bd=(\sigma_1,\ldots, \sigma_D)\T$ be the vector of eignevalues, then $\bS^{-1} {\bdel'}=\bd^{-1} \odot \mt{\bdel}$, where $\odot$ is the Hadamard (entrywise) product.  The \Lda~classifier may therefore be encoded by a unit vector, $\mt{\bd}:= \frac{1}{m} \bd^{-1} \odot \mt{\bdel'}$, and its magnitude, $m:=\norm{\bd^{-1} \odot \mt{\bdel}}$.
This will be useful later.




\subsection[]{Rotation of Projection Based Linear Classifiers $g_A$}

By a similar argument as above, one can easily show that:

\begin{align*}
(\bA  \bW \bx) \T  (\bA \bW  \bSig  \bW\T \bA\T)^{-1} \bA \bW \bdel
&= \bx\T (\bW\T \bA\T) (\bA \bW) \bSig^{-1} (\bW\T \bA\T) (\bA \bW) \bdel \\
&= \bx\T \bY\T \bY \bSig^{-1} \bY\T \bY \bdel \\
&= \bx\T \bZ \bSig^{-1} \bZ\T \bdel \\
&= \bx\T (\bZ \bSig \bZ\T)^{-1} \bdel = \bx\T \mt{\bSig}_d^{-1} \bdel,
% (\bA\T \bA \bx) \T  \bSig^{-1} \bA\T \bA \bdel = (\bA \bx)\T \bSig^{-1}_A \bdel_A.
\end{align*}
% \end{proof}
where $\bY = \bA \bW \in \Real^{d \times p}$ so that $\bZ=\bY\T \bY$ is a symmetric ${p \times p}$ matrix of rank $d$.  In other words, rotating and then projecting is equivalent to a change of basis.
The implications of the above is:
\begin{lem}
$g_A$ is rotationally invariant if and only if span($\bA$)=span($\bSig_d$).
In other words, \Pca~is the only rotationally invariant projection.
\end{lem}

\subsection{Chernoff information}
We now introduce the notion of the Chernoff information, which serves as our surrogate measure for the Bayes error of any classification procedure given the {\em projected} data -- in the context of this paper the projection is via LOL or PCA. Our discussion of the Chernoff information is under the context of decision rules for hypothesis testing, nevertheless, as evidenced by the fact that the Maximum A Posterior decision rule -- equivalently the Bayes classifier -- achieves the Chernoff information rate, this distinction between hypothesis testing and classification is mainly for ease of exposition.

Let $F_0$ and $F_1$ be two absolutely continuous multivariate distribution in $\Omega \subset \mathbb{R}^{d}$ with density function $f_0$ and $f_1$, respectively. Suppose that $Y_1, Y_2, \dots, Y_m$ are independent and identically distributed random variables, with $Y_i$ distributed either $F_0$ or $F_1$. We are interested in testing the simple null hypothesis $\mathbb{H}_0 \colon F = F_0$ against the simple alternative hypothesis $\mathbb{H}_1 \colon F = F_1$. A test $T$ is a sequence of mapping $T_m \colon \Omega^{m} \mapsto \{0,1\}$ such that given $Y_1 = y_1, Y_2 = y_2, \dots, Y_m = y_m$, the test rejects $\mathbb{H}_0$ in favor of $\mathbb{H}_1$ if $T_m(y_1, y_2, \dots, y_m) = 1$; similarly, the test rejects $\mathbb{H}_1$ in favor of $\mathbb{H}_0$ if $T_m(y_1, y_2, \dots, y_m) = 0$. 
The Neyman-Pearson lemma states that, given $Y_1 = y_1, Y_2 = y_2, \dots, Y_m = y_m$ and a threshold $\eta_m \in \mathbb{R}$, the likelihood ratio test which rejects $\mathbb{H}_0$ in favor of $\mathbb{H}_1$ whenever
$$ \Bigl(\sum_{i=1}^{m} \log{f_0(y_i)} - \sum_{i=1}^{m} \log{f_1(y_i)} \Bigr) \leq \eta_m $$
is the most powerful test at significance level $\alpha_m = \alpha(\eta_m)$, i.e., the likelihood ratio test minimizes the type-II error $\beta_m$ subject to the contrainst that the type-I error is at most $\alpha_m$. 

Assuming that $\pi \in (0,1)$ is a prior probability that $\mathbb{H}_0$ is true. Then, for a given $\alpha_m^{*} \in (0,1)$, let $\beta_m^{*} = \beta_m^{*}(\alpha_m^{*})$ be the type-II error associated with the likelihood ratio test when the type-I error is at most $\alpha_m^{*}$. The quantity $\inf_{\alpha_m^{*} \in (0,1)} \pi \alpha_m^{*} + (1 - \pi) \beta_m^{*}$ is then the Bayes risk in deciding between $\mathbb{H}_0$ and $\mathbb{H}_1$ given the $m$ independent random variables $Y_1, Y_2, \dots, Y_m$. A classical result of Chernoff \cite{chernoff_1952} states that the Bayes risk is intrinsically linked to a quantity known as the {\em Chernoff information}. More specifically, let $C(F_0, F_1)$ be the quantity
\begin{equation}
\label{eq:chernoff-defn}
\begin{split} C(F_0, F_1) & = - \log \, \Bigl[\, \inf_{t \in (0,1)} \int_{\mathbb{R}^{d}} f_0^{t}(\bm{x}) f_1^{1-t}(\bm{x}) \mathrm{d}\bm{x} \Bigr] \\
&= \sup_{t \in (0,1)} \Bigl[ - \log \int_{\mathbb{R}^{d}} f_0^{t}(\bm{x}) f_1^{1-t}(\bm{x}) \mathrm{d}\bm{x} \Bigr]
\end{split}
\end{equation}
Then we have
\begin{equation}
\label{eq:chernoff-binary}
\begin{split}
\lim_{m \rightarrow \infty} \frac{1}{m} \inf_{\alpha_m^{*} \in (0,1)} \log( \pi \alpha_m^{*} + (1 - \pi) \beta_m^{*}) & = - \, C(F_0, F_1).
\end{split}
\end{equation}
Thus $C(F_0, F_1)$ is the {\em exponential} rate at which the Bayes error $\inf_{\alpha_m^{*} \in (0,1)} \pi \alpha_m^{*} + (1 - \pi) \beta_m^{*}$ decreases as $m \rightarrow \infty$; we also note that the $C(F_0, F_1)$ is independent of $\pi$. We also define, for a given $t \in (0,1)$ the Chernoff divergence $C_t(F_0, F_1) $ between $F_0$ and $F_1$ by
$$ C_{t}(F_0,F_1) = - \log \int_{\mathbb{R}^{d}} f_0^{t}(\bm{x}) f_1^{1-t}(\bm{x}) \mathrm{d}\bm{x}. $$
The Chernoff divergence is an example of a $f$-divergence as defined in \cite{Csizar}. When $t = 1/2$, $C_t(F_0,F_1)$ is the Bhattacharyya distance between $F_0$ and $F_1$. 

The result of Eq.~\eqref{eq:chernoff-binary} can be extended to $K + 1 \geq 2$ hypothesis, with the exponential rate being the minimum of the Chernoff information between any pair of hypothesis. More specifically, let $F_0, F_1, \dots, F_{K}$ be distributions on $\mathbb{R}^{d}$ and let $Y_1, Y_2, \dots, Y_m$ be independent and identically distributed random variables with distribution $F \in \{F_0, F_1, \dots, F_K\}$. Our inference task is in determining the distribution of the $Y_i$ among the $K+1$ hypothesis $\mathbb{H}_0 \colon F = F_0, \dots, \mathbb{H}_{K} \colon F = F_K$. Suppose also that hypothesis $\mathbb{H}_k$ has {\em a priori} probabibility $\pi_k$. For any decision rule $\delta$, the risk of $\delta$ is $r(\delta) = \sum_{k} \pi_k \sum_{l \not = k} \alpha_{lk}(\delta) $ where $\alpha_{lk}(\delta)$ is the probability of accepting hypothesis $\mathbb{H}_l$ when hypothesis $\mathbb{H}_k$ is true. Then we have \cite{leang-johnson}
\begin{equation}
\label{eq:chernoff-multiple}
\inf_{\delta} \lim_{m \rightarrow \infty}  \frac{r(\delta)}{m} = - \min_{k \not = l} C(F_k, F_l).
\end{equation}
where the infimum is over all decision rule $\delta$, i.e., for any $\delta$, $r(\delta)$ decreases to $0$ as $m \rightarrow \infty$ at a rate no faster than $\exp(- m \min_{k \not = l} C(F_k, F_l))$. 
%It was also shown in \cite{leang-johnson} that the {\em Maximum A Posterior} decision rule achieves this rate. 

When the distributions $F_0$ and $F_1$ are multivariate normal, that is, $F_0 =  \mathcal{N}(\mu_0, \Sigma_0)$ and $F_1 = \mathcal{N}(\mu_1, \Sigma_1)$; then, denoting by $\Sigma_t = t \Sigma_0 + (1 - t) \Sigma_1$, we have 
\begin{equation*}
C(F_0, F_1) = \sup_{t \in (0,1)} \Bigl(\frac{t(1 - t)}{2} (\mu_1 - \mu_2)^{\top}\Sigma_t^{-1}(\mu_1 - \mu_2) + \frac{1}{2} \log \frac{|\Sigma_t|}{|\Sigma_0|^{t} |\Sigma_1|^{1 - t}}  \Bigr).
\end{equation*}

\subsection{Projecting data and Chernoff information}
We now discuss how the Chernoff information characterizes the effect a linear transformation $A$ of the data has on classification accuracy.
We start with the following simple result whose proof follows directly from Eq.~\eqref{eq:chernoff-multiple}. 
\begin{lem}
\label{lem:chernoff-1}
Let $F_0 = \mathcal{N}(\mu_0, \Sigma)$ and $F_1 \sim \mathcal{N}(\mu_1, \Sigma)$ be two multivariate normals with equal covariance matrices. For any linear transformation $A$, let $F_0^{(A)}$ and $F_1^{(A)}$ denotes the distribution of $AX$ when $X \sim F_0$ and $X \sim F_1$, respectively. We then have
\begin{equation}
\begin{split}
C(F_0^{(A)}, F_1^{(A)}) &= \frac{1}{8} (\mu_1 - \mu_0)^{\top} A^{\top} (A \Sigma A^{\top})^{-1} A (\mu_1 - \mu_0) \\ & = \frac{1}{8} (\mu_1 - \mu_0)^{\top} \Sigma^{-1/2} \Sigma^{1/2} A^{\top} (A \Sigma A^{\top})^{-1} A \Sigma^{1/2} \Sigma^{-1/2} (\mu_1 - \mu_0) \\
&= \frac{1}{8} \|P_{\Sigma^{1/2} A^{\top}} \Sigma^{-1/2} (\mu_1 - \mu_0) \|_{F}^{2}
\end{split}
\end{equation}
where $P_{Z} = Z(Z^{\top} Z)^{-1} Z^{\top}$ denotes the matrix corresponding to the orthogonal projection onto the columns of $Z$. 
\end{lem}

Thus for a classification problem where $X | Y = 0$ and $X | Y = 1$ are distributed multivariate normals with mean $\mu_0$ and $\mu_1$ and the same covariance matrix $\Sigma$, Lemma~\ref{lem:chernoff-1} then states that for any two linear transformations $A$ and $B$, the transformed data $AX$ is to be preferred over the transformed data $BX$ if
$$ (\mu_1 - \mu_0)^{\top} A^{\top} (A \Sigma A^{\top})^{-1} A (\mu_1 - \mu_0) > (\mu_1 - \mu_0)^{\top} B^{\top} (B \Sigma B^{\top})^{-1} B (\mu_1 - \mu_0). $$

As an example, suppose $\Sigma$ is diagonal with distinct eigenvalues where the diagonal entries of $\Sigma$ are in non-increasing order. 
Denote by $\delta = \mu_1 - \mu_0$ and let
$A = \delta^{\top}$ and $B = e_1^{\top} = (1,0,0,\dots,0)$ be the linear transformations for LOL and PCA of $X$ into $\mathbb{R}$. We then have  
\begin{gather*}
C(F_0^{(A)}, F_1^{(A)}) = \frac{(\delta^{\top} \delta)^2}{\delta^{\top} \Sigma \delta} = \frac{(\sum_{i} \delta_i^{2})^{2}}{\sum_{i} \delta_i^{2} \lambda_i}; \quad 
C(F_0^{(B)}, F_1^{(B)}) = \frac{\delta_1^2}{\lambda_1}
\end{gather*}
where $\lambda_1$ is the largest eigenvalue of $\Sigma$. Suppose furthermore that $\delta_1 \leq \delta_2 \leq \dots \leq \delta_p$ and $\lambda_1 > \lambda_2 > \dots > \lambda_p$. Then $C(F_0^{(A)}, F_1^{(A)})$ can be lower-bounded as
$$ C(F_0^{(A)}, F_1^{(A)}) = \frac{(\sum_{i} \delta_i^{2})^{2}}{\sum_{i} \delta_i^{2} \lambda_i} \geq \frac{(p \delta_1^2)^2}{p \delta_p^{2} \lambda_1} $$
and hence $C(F_0^{(A)}, F_1^{(A)}) > C(F_0^{(B)}, F_1^{(B)})$ provided $p \delta_1^{2}  \geq \delta_p^2$. 

When $A = \bigl[ \delta \mid e_1 \mid e_2 \dots \mid e_{d-1} \bigr]^{\top} \in \mathbb{R}^{d \times p}$ and $B = \bigl[e_1 \mid e_2 \mid \dots \mid e_d \bigr]^{\top} \in \mathbb{R}^{d \times p}$ are the linear transformation for LOL and PCA of $X$ into $\mathbb{R}^{d}$, we have
\begin{gather*}
C(F_0^{(A)}, F_1^{(A)}) = \frac{(\sum_{i=d}^{p} \delta_i^2)^2}{\sum_{i=d}^{p} \delta_i^2 \lambda_i} + \sum_{i=1}^{d-1} \frac{\delta_i^{2}}{\lambda_i};  \quad 
C(F_0^{(B)}, F_1^{(B)}) = \sum_{i=1}^{d} \frac{\delta_i^2}{\lambda_i}.
\end{gather*}

This can be seen as follows. Let $\xi_{d-1} = \bigl[e_1 \mid e_2 \mid \dots \mid e_{d-1} \bigr] \in \mathbb{R}^{p \times (d-1)}$ and $\zeta_{d-1} = (\lambda_1 \delta_1, \lambda_2 \delta_2, \dots, \lambda_{d-1} \delta_{d-1})^{\top} \in \mathbb{R}^{d-1}$. Then
\begin{equation}
A \Sigma A^{\top} = \bigl[ \delta \mid \xi_{d-1} \bigr]^{\top} \, \Sigma \, \bigl[ \delta \mid \xi_{d-1} \bigr] = \begin{bmatrix} \delta^{\top} \Sigma \delta & \delta^{\top} \Sigma \xi_{d-1} \\ \xi_{d-1}^{\top} \Sigma \delta & \xi_{d-1}^{\top} \Sigma \xi_{d-1} \end{bmatrix} = 
\begin{bmatrix} \sum_{i=1}^{p} \delta_i^{2} \lambda_i & \zeta_{d-1}^{\top} \\ \zeta_{d-1} & \Sigma_{d-1} \end{bmatrix}
\end{equation}
where $\Sigma_{d-1} = \mathrm{diag}(\lambda_1, \lambda_2, \dots, \lambda_{d-1})$ is the submatrix of $\Sigma$ corresponding to the eigenvalues $\lambda_1, \lambda_2, \dots, \lambda_{d-1}$. Using a formula for the inverse of a partitioned matrix, we have
\begin{equation}
\begin{split}
(A \Sigma A^{\top})^{-1} &= \begin{bmatrix} \sum_{i=1}^{p} \delta_i^{2} \lambda_i & \zeta_{d-1}^{\top} \\ \zeta_{d-1} & \Sigma_{d-1} \end{bmatrix}^{-1} 
\\ & = \begin{bmatrix} \bigl(\sum_{i=1}^{p} \delta_i^{2} \lambda_i - \zeta_{d-1}^{\top} \Sigma_{d-1}^{-1} \zeta_{d-1}\bigr)^{-1} & - \bigl(\sum_{i=1}^{p} \delta_i^{2} \lambda_i\bigr)^{-1} \zeta_{d-1}^{\top} \bigl(\Sigma_{d-1} - \frac{\zeta_{d-1} \zeta_{d-1}^{\top}}{\sum_{i=1}^{p} \delta_i^{2} \lambda_i}\bigr)^{-1} \\ - \bigl(\sum_{i=1}^{p} \delta_i^{2} \lambda_i\bigr)^{-1} \bigl(\Sigma_{d-1} - \frac{\zeta_{d-1} \zeta_{d-1}^{\top}}{\sum_{i=1}^{p} \delta_i^{2} \lambda_i}\bigr)^{-1} \zeta_{d-1} & \bigl(\Sigma_{d-1} - \frac{\zeta_{d-1} \zeta_{d-1}^{\top}}{\sum_{i=1}^{p} \delta_i^{2} \lambda_i}\bigr)^{-1}
\end{bmatrix}
\end{split}
\end{equation}
Now, $\sum_{i=1}^{p} \delta_i^{2} \lambda_i - \zeta_{d-1}^{\top} \Sigma_{d-1}^{-1} \zeta_{d-1} = \sum_{i=1}^{p} \delta_i^{2} - \sum_{i=1}^{d-1} \delta_i^{2} \lambda_i = \sum_{i=d}^{p} \delta_i^{2} \lambda_i$. In addition, by the Sherman-Morrison-Woodbury formula, we have
\begin{equation*}
\begin{split}
(\Sigma_{d-1} - \frac{\zeta_{d-1} \zeta_{d-1}^{\top}}{\sum_{i=1}^{p} \delta_i^{2} \lambda_i}\bigr)^{-1} &= \Sigma_{d-1}^{-1} + \frac{\Sigma_{d-1}^{-1} \zeta_{d-1} \zeta_{d-1}^{\top} \Sigma_{d-1}^{-1}/ \bigl(\sum_{i=1}^{p} \delta_i^{2} \lambda_i\bigr)}{1 - \zeta_{d-1}^{\top} \Sigma_{d-1}^{-1} \zeta_{d-1}/\bigl(\sum_{i=1}^{p} \delta_i^{2} \lambda_i\bigr)} \\
&= \Sigma_{d-1}^{-1} + \frac{\Sigma_{d-1}^{-1} \zeta_{d-1} \zeta_{d-1}^{\top} \Sigma_{d-1}^{-1}}{\sum_{i=1}^{p} \delta_i^{2} \lambda_i - \zeta_{d-1}^{\top} \Sigma_{d-1}^{-1} \zeta_{d-1}} \\
&= \Sigma_{d-1}^{-1} + \frac{\Sigma_{d-1}^{-1} \zeta_{d-1} \zeta_{d-1}^{\top} \Sigma_{d-1}^{-1}}{\sum_{i=1}^{p} \delta_i^{2} \lambda_i - \sum_{i=1}^{d-1} \delta_i^{2} \lambda_i} = \Sigma_{d-1}^{-1} + \frac{\Sigma_{d-1}^{-1} \zeta_{d-1} \zeta_{d-1}^{\top} \Sigma_{d-1}^{-1}}{\sum_{i=d}^{p} \delta_i^{2} \lambda_i}.
\end{split}
\end{equation*}
Therefore,
\begin{equation*}
\begin{split}
\bigl(\sum_{i=1}^{p} \delta_i^{2} \lambda_i\bigr)^{-1} \zeta_{d-1}^{\top} \bigl(\Sigma_{d-1}^{-1} + \frac{\Sigma_{d-1}^{-1} \zeta_{d-1} \zeta_{d-1}^{\top} \Sigma_{d-1}^{-1}}{\sum_{i=d}^{p} \delta_i^{2} \lambda_i} \bigr) &= \bigl(\sum_{i=1}^{p} \delta_i^{2} \lambda_i\bigr)^{-1} \Bigl(\zeta_{d-1}^{\top} \Sigma_{d-1}^{-1} + \frac{\zeta_{d-1}^{\top} \Sigma_{d-1}^{-1} \zeta_{d-1} \zeta_{d-1}^{\top} \Sigma_{d-1}^{-1}}{\sum_{i=d}^{p} \delta_i^{2} \lambda_i} \Bigr) \\
&= \bigl(\sum_{i=1}^{p} \delta_i^{2} \lambda_i\bigr)^{-1} \Bigl(\zeta_{d-1}^{\top} \Sigma_{d-1}^{-1} + \frac{(\sum_{i=1}^{d-1} \delta_i^{2} \lambda_i)
\zeta_{d-1}^{\top} \Sigma_{d-1}^{-1}}{\sum_{i=d}^{p} \delta_i^{2} \lambda_i}\Bigr) \\
%&= \bigl(\sum_{i=1}^{p} \delta_i^{2} \lambda_i\bigr)^{-1} \zeta_{d-1}^{\top} \Sigma_{d-1}^{-1} \frac{\sum_{i=d}^{p} \delta_i^{2} \lambda_i + \sum_{i=1}^{d-1} \delta_i^{2} \lambda_i}{\sum_{i=d}^{p} \delta_i^{2} \lambda_i} \\ 
& = \frac{\zeta_{d-1}^{\top} \Sigma_{d-1}^{-1}}{\sum_{i=d}^{p} \delta_i^{2} \lambda_i}
\end{split}
\end{equation*}

Combining the above, we have
\begin{equation}
\begin{split}
(A \Sigma A^{\top})^{-1} &= \begin{bmatrix} \bigl(\sum_{i=d}^{p} \delta_i^{2} \lambda_i\bigr)^{-1} & - \frac{\zeta_{d-1}^{\top} \Sigma_{d-1}^{-1}}{\sum_{i=d}^{p} \delta_i^{2} \lambda_i} \\
 - \frac{\Sigma_{d-1}^{-1} \zeta_{d-1}}{\sum_{i=d}^{p} \delta_i^{2} \lambda_i} 
  & \Sigma_{d-1}^{-1} + \frac{\Sigma_{d-1}^{-1} \zeta_{d-1} \zeta_{d-1}^{\top} \Sigma_{d-1}^{-1}}{\sum_{i=d}^{p} \delta_i^{2} \lambda_i}.
\end{bmatrix}
\end{split}
\end{equation}
In addition, $A(\mu_1 - \mu_0) = (\delta^{\top} \delta, \delta_1, \delta_2, \dots, \delta_{d-1})^{top} = (\delta^{\top} \delta, \zeta_{d-1}^{\top} \Sigma_{d-1}^{-1})^{\top} \in \mathbb{R}^{d}$. Hence
\begin{equation*} 
\begin{split}
(\mu_1 - \mu_0)^{\top} A^{\top} (A \Sigma A^{\top})^{-1} A (\mu_1 - \mu_0) &= 
\bigl[\delta^{\top} \delta \mid \zeta_{d-1}^{\top} \Sigma_{d-1}^{-1} \bigr] 
\begin{bmatrix} \bigl(\sum_{i=d}^{p} \delta_i^{2} \lambda_i\bigr)^{-1} & - \frac{\zeta_{d-1}^{\top} \Sigma_{d-1}^{-1}}{\sum_{i=d}^{p} \delta_i^{2} \lambda_i} \\
 - \frac{\Sigma_{d-1}^{-1} \zeta_{d-1}}{\sum_{i=d}^{p} \delta_i^{2} \lambda_i} 
  & \Sigma_{d-1}^{-1} + \frac{\Sigma_{d-1}^{-1} \zeta_{d-1} \zeta_{d-1}^{\top} \Sigma_{d-1}^{-1}}{\sum_{i=d}^{p} \delta_i^{2} \lambda_i} \end{bmatrix} \begin{bmatrix} \delta^{\top} \delta \\ \Sigma_{d-1}^{-1} \zeta_{d-1} \end{bmatrix} \\
  &= \frac{(\delta^{\top} \delta)^{2}}{\sum_{i=d}^{p} \delta_i^{2} \lambda_i} - 2 \delta^{\top} \delta \frac{\zeta_{d-1}^{\top} \Sigma_{d-1}^{-2} \zeta_{d-1}}{\sum_{i=d}^{p} \delta_i^2 \lambda_i} + \Bigl(\zeta_{d-1}^{\top} \Sigma_{d-1}^{-3} \zeta_{d-1} + \frac{(\zeta_{d-1}^{\top} \Sigma_{d-1}^{2} \zeta_{d-1})^{2}}{\sum_{i=d}^{p} \delta_i^{2} \lambda_i} \Bigr) \\
  &= \frac{(\delta^{\top} \delta - \zeta_{d-1}^{\top} \Sigma_{d-1}^{-2} \zeta_{d-1})^{2}}{\sum_{i=d}^{p} \delta_i^{2} \lambda_i} + \zeta_{d-1}^{\top} \Sigma_{d-1}^{-3} \zeta_{d-1} \\
  &= \frac{\bigl(\sum_{i=1}^{p} \delta_i^{2} - \sum_{i=1}^{d-1} \delta_i^{2}\bigr)^{2}}{\sum_{i=d}^{p} \delta_i^{2} \lambda_i} + \sum_{i=1}^{d-1} \frac{\delta_i^{2}}{\lambda_i} = \frac{\bigl(\sum_{i=d}^{p} \delta_i^{2}\bigr)^{2}}{\sum_{i=d}^{p} \delta_i^{2} \lambda_i} + \sum_{i=1}^{d-1} \frac{\delta_i^{2}}{\lambda_i}.
\end{split}
\end{equation*}
The deriviation of $C(F_0^{(B)}, F_1^{(B)})$ is straightforward and will be omitted. Thus a sufficient condition for $C(F_0^{(A)}, F_1^{(A)}) > C(F_0^{(B)}, F_1^{(B)})$ is that
$$ \frac{\bigl(\sum_{i=d}^{p} \delta_i^{2}\bigr)^{2}}{\sum_{i=d}^{p} \delta_i^{2} \lambda_i} > \frac{\delta_{d}^{2}}{\lambda_d}. $$
Once again, assuming $\delta_{d} \leq \delta_{d+1} \leq \dots \leq \delta_{p}$ and $\lambda_d > \lambda_{d+1} > \dots > \lambda_{p}$, the above is implied by the condition $(p - d) \delta_{d}^{2} > \delta_p^2$. 
 

\subsection{Simplifying the Objective Function}

Recalling Eq.~\eqref{eq:g_A}, a projection based classifier is effectively thresholding the dot product of $\bx$ with the linear projection operator $\bP_A :=\bA\T \bSig_A^{-1} \bdel_A \in \Real^p$, and let $\bP_*:=\bP_{\bA_*}$.  Unfortunately, the nonlinearity in in Eq.~\eqref{eq:A} makes analysis difficult.
However, because of the linear nature of the classifier and projection matrix operator, an objective function that is simpler to evaluate is available.
Define
$\angle(\bP,\bP') = \frac{ \bP\T \bP'}{||\bP||_2 ||\bP'||_2} \in (0,1)$, and consider

\begin{equation} \label{eq:angle}
\begin{aligned}
& \underset{\bA}{\text{minimize}}
& & -\angle(\bP_A,\bP_*),
\\ & \text{subject to} & & \bA \in \Real^{p \times d}, \quad \bA \bA\T = \bI_{d \times d}.
\end{aligned}
\end{equation}

\begin{lem} \label{l:angle}
The solution to Eq.~\eqref{eq:angle} is also the solution to Eq.~\eqref{eq:A} for any given $d$.
\end{lem}

\begin{proof}
The minimum of Eq.~\eqref{eq:angle} is clearly $\bA=\bSig^{-1} \bdel$, which is also the minimum of Eq.~\eqref{eq:A}.
\end{proof}


\begin{remark}
$\angle$ is merely the angle between two vectors, and is therefore scale invariant.  In other words, $\angle(\bP_A,\bP)=\angle(\bP_{c A},\bP)$, for any $c > 0$.
\end{remark}

Given the above, we can evaluate various choices of $\bA$ in terms of their induced projection operator $\bP_A$ and the angle between said projection operators and the Bayes optimal projection operator.
% Let $\bP_*=\bP_{A_*}=\bSig^{-1} \bdel$,
% and $\alpha^*_A=\angle(\bP_*,\bP_A)$.


\begin{lem}
$\angle(\bP_A, \bP_*) <1  \implies L_A > L_*$
\end{lem}


\begin{proof}
If $\angle(\bP_A, \bP_*) <1$, then there exists an $\bx$ such that $\II\{ \bx\T \bP_A >0 \} \neq \II\{ \bx\T \bP_* >0 \}$, and therefore, $L_A > L_*$.
\end{proof}



\begin{lem}
\label{q:a2}
$$\angle(\bP_A,\bP_*) \leq \angle(\bP_B,\bP_*) \implies L_A \leq L_B.$$
\end{lem}
% <<<<<<< HEAD
% \begin{proof}
% We recall from the definitions that $\bP_{\bA} = \bA\T (\bA \bSig \bA\T)^{-1} \bA \bdel$ and $\bP_{\bB} = \bB\T (\bB \bSig \bB\T)^{-1} \bB \bdel$. In addition, $\bP_{*} = \bSig^{-1} \bdel$. For ease of notation, let $\ba = \bP_{\bA}$ and $\mb{b} = \bP_{\bB}$. 
% Therefore,
% $$ 
% \frac{\bP_{\bA}\T \bP_{*}}{\|\bP_{\bA}\|_{F} \|\bP_{*}\|_{F}} = \frac{\ba\T \bSig^{-1} \bdel}{\|\ba\| \|\bP_{*}\|_{F}} \geq  \frac{\mb{b}\T \bSig^{-1} \bdel}{\|\mb{b}\| \|\bP_{*}\|_{F}} = \frac{\bP_{\bB}\T \bP_{*}}{\|\bP_{\bB}\|_{F} \|\bP_{*}\|_{F}}
% $$
% Without loss of generality, assume that $\ba\T \ba = 1$ and  $\mb{b}\T \mb{b} = 1$. Then the above condition reduced to
% $$ \ba\T \bSig^{-1} \bdel \geq \mb{b}\T \bSig^{-1} \bdel.$$

% We now analyze $L_{A}$. Let $C = \tfrac{1}{2}(2 \pi \mathrm{\det}(\bSig))^{-1/2}$. We have
% \begin{equation*}
% \begin{split}
% L_{A} &= \mathbb{E}[\mathbb{I}\{\mb{x}\T \ba > 0\} \not = y] \\ &=
% \int_{\mb{x}\T \ba \leq 0} C \exp\Bigl(-\tfrac{1}{2}(\mb{x} - \bmu)\T \bSig^{-1} (\mb{x} - \bmu)\Bigr) \mathrm{d} \mb{x} + \int_{\mb{x}\T \ba > 0} C \exp\Bigl(-\tfrac{1}{2}(\mb{x} + \bmu)\T \bSig^{-1} (\mb{x} + \bmu)\Bigr) \mathrm{d} \mb{x}
% \\ &= 0.5 - 
% \int_{\mb{x}\T \ba > 0} C \exp\Bigl(-\tfrac{1}{2}(\mb{x} - \bmu)\T \bSig^{-1} (\mb{x} - \bmu)\Bigr) \mathrm{d} \mb{x}
% + \int_{\mb{x}\T \ba > 0} C \exp\Bigl(-\tfrac{1}{2}(\mb{x} + \bmu)\T \bSig^{-1} (\mb{x} + \bmu)\Bigr) \mathrm{d} \mb{x}
% \\ &= 0.5 - C \int_{\mb{x}\T \ba > 0} \Bigl(\exp(-\tfrac{1}{2}(\mb{x} - \bmu)\T \bSig^{-1} (\mb{x} - \bmu)) - \exp(-\tfrac{1}{2}(\mb{x} + \bmu)\T \bSig^{-1} (\mb{x} + \bmu)) \Bigr) \mathrm{d} \mb{x}
% \\ &= 0.5 - C \int_{\mb{x}\T \ba > 0} h(\mb{x}) \mathrm{d} \mb{x}
% \end{split}
% \end{equation*}
% where $h(\mb{x}) = \exp(-\tfrac{1}{2}(\mb{x} - \bmu)\T \bSig^{-1} (\mb{x} - \bmu)) - \exp(-\tfrac{1}{2}(\mb{x} + \bmu)\T \bSig^{-1} (\mb{x} + \bmu))$.
% In particular, $L_*$ is given by
% $$L_* = 0.5 - C \int_{\mb{x}\T \bSig^{-1} \bdel > 0} h(\mb{x}) \mathrm{d} \mb{x}, $$
% where $h(\mb{x}) = \exp(-\tfrac{1}{2}(\mb{x} - \bmu)\T \bSig^{-1} (\mb{x} - \bmu)) - \exp(-\tfrac{1}{2}(\mb{x} + \bmu)\T \bSig^{-1} (\mb{x} + \bmu)$. 
% And since  $h(\mb{x}) \geq 0$ if and only if $\mb{x}\T \bSig^{-1} \bdel \geq 0$, $L_*$ is indeed the Bayes risk. 

% Thus, $L_A \leq L_B$ is equivalent to 
% \begin{equation*}
% \int_{\mb{x}\T \ba > 0} h(x) \mathrm{d} \mb{x} \geq \int_{\mb{x}\T \mb{b} > 0} h(x) \mathrm{d} \mb{x}.
% \end{equation*}

% We now analyze the set $\{\mb{x} \colon \mb{x}\T \ba > 0\}$. We write 
% $\mb{x}\T \ba = \mb{x}\T \bSig^{-1} \bdel + \mb{x}\T (\ba - \bSig^{-1} \bdel)$. 
% Without loss of generality, we can further assume that $\|(\bSig^{-1} \bdel)\T \| = 1$. Then, because $\ba\T \ba = 1$, we have that
% $\|\ba - \bSig^{-1} \bdel \|^{2} = 2 - 2 \ba\T \bSig^{-1} \bdel$. 
% Let $c_a = 2 - 2 \ba\T \bSig^{-1} \bdel$; we shall assume for the moment that $c_a$ is ``small''. Then for any $\eta > 0$, we have
% \begin{equation*}
% \{\mb{x} \colon \mb{x}\T \ba > 0 \} = \{ \mb{x} \colon \mb{x}\T \ba > 0 \} \cap \Bigl(\{\mb{x} \colon \mb{x}\T \bSig^{-1} \bdel > \eta \} \cup \{\mb{x} \colon \mb{x}\T \bSig^{-1} \bdel \in [-\eta, \eta] \} \cup \{ \mb{x} \colon \mb{x}\T \bSig^{-1} \bdel < -\eta \}\Bigr)
% \end{equation*}
% Now provided that $c_a$ is ``small'' we can choose $\eta = Kc_a$ for some constant $K$ such that 
% \begin{gather}
% \int_{\mb{x}\T \bSig^{-1} \bdel \in [-\eta, \eta]} h(\mb{x}) \mathrm{d} \mb{x} \approx 0
% \label{part:a}
% \\
% \int_{\substack{\mb{x} \colon \mb{x}\T \bSig^{-1} \bdel < -\eta \\ \mb{x}\T \ba > 0 }} h(\mb{x}) \mathrm{d}\mb{x} \approx 0 
% \label{part:b}
% \\
%  \int_{\substack{\mb{x} \colon \mb{x}\T \bSig^{-1} \bdel > \eta \\ \mb{x}\T \ba > 0 }} h(\mb{x}) \mathrm{d}\mb{x} \approx 
% \int_{\mb{x} \colon \mb{x}\T \bSig^{-1} \bdel > \eta } h(\mb{x}) \mathrm{d} \mb{x}
% \label{part:c}
% \end{gather}

% Therefore,
% \begin{equation*}
% \int_{\mb{x}\T \ba > 0} h(\mb{x}) \mathrm{d} \mb{x} \approx \int_{\mb{x}\T \bSig^{-1} \bdel > K c_a} h(\mb{x}) \mathrm{d} \mb{x}
% \end{equation*}

% We note that in general, the integrals in Eq.~\eqref{part:a} and Eq.~\eqref{part:b} are non-positive. Therefore, letting $\eta' = K c_b$ where $c_b = 2 - 2 \ba\T \bSig^{-1} \bdel$, we have
% \begin{equation*}
% \int_{\mb{x}\T \mb{b} > 0} h(\mb{x}) \mathrm{d} \mb{x} \leq \int_{\substack{\mb{x} \colon \mb{x}\T \bSig^{-1} \bdel > K c_b \\ \mb{x}\T \mb{b} > 0 }} h(\mb{x}) \mathrm{d} \mb{x} \leq
% \int_{\mb{x} \colon \mb{x}\T \bSig^{-1} \bdel > K c_b } h(\mb{x}) \mathrm{d} \mb{x}.
% \end{equation*}
% for any $\mb{b}$. 
% Therefore, because $h(\mb{x})$ is positive on the set $\mb{x}\T \bSig^{-1} \bdel > 0$, we can thus conclude that there exists some $\epsilon_2 > \epsilon_1 > 0$ such that
% \begin{equation*}
% \int_{\mb{x}\T \ba > 0} h(\mb{x}) \geq \int_{\mb{x}\T \mb{b} > 0} h(\mb{x}).
% \end{equation*}
% for any $\ba$ and $\mb{b}$ with $c_a < \epsilon_1$ and $c_b > \epsilon_2$. 
% =======
% \begin{proof}
% We recall from the definitions that $\bP_{\bA} = \bA\T (\bA \mb{\Sigma} \bA\T)^{-1} \bA \mb{\delta}$ and $\bP_{\mathbf{B}} = \mathbf{B}\T (\mathbf{B} \mb{\Sigma} \mathbf{B}\T)^{-1} \mathbf{B} \mb{\delta}$. In addition, $\bP_{*} = \mb{\Sigma}^{-1} \mb{\delta}$. For ease of notation, let $\mb{a} = \bP_{\bA}$ and $\mb{b} = \bP_{\mathbf{B}}$. 
% Therefore,
% $$ 
% \frac{\bP_{\bA}\T \bP_{*}}{\|\bP_{\bA}\|_{F} \|\bP_{*}\|_{F}} = \frac{\mb{a}\T \mb{\Sigma}^{-1} \mb{\delta}}{\|\mb{a}\| \|\bP_{*}\|_{F}} \geq  \frac{\mb{b}\T \mb{\Sigma}^{-1} \mb{\delta}}{\|\mb{b}\| \|\bP_{*}\|_{F}} = \frac{\bP_{\mathbf{B}}\T \bP_{*}}{\|\bP_{\mathbf{B}}\|_{F} \|\bP_{*}\|_{F}}
% $$
% Without loss of generality, assume that $\mb{a}\T \mb{a} = 1$ and  $\mb{b}\T \mb{b} = 1$. Then the above condition reduced to
% $$ \mb{a}\T \mb{\Sigma}^{-1} \mb{\delta} \geq \mb{b}\T \mb{\Sigma}^{-1} \mb{\delta}.$$

% We now analyze $L_{A}$. Let $C = \tfrac{1}{2}(2 \pi \mathrm{\det}(\mb{\Sigma}))^{-1/2}$. We have
% \begin{equation*}
% \begin{split}
% L_{A} &= \mathbb{E}[\mathbb{I}\{\mb{x}\T \mb{a} > 0\} \not = y] \\ &=
% \int_{\mb{x}\T \mb{a} \leq 0} C \exp\Bigl(-\tfrac{1}{2}(\mb{x} - \mb{\mu})\T \mb{\Sigma}^{-1} (\mb{x} - \mb{\mu})\Bigr) \mathrm{d} \mb{x} + \int_{\mb{x}\T \mb{a} > 0} C \exp\Bigl(-\tfrac{1}{2}(\mb{x} + \mb{\mu})\T \mb{\Sigma}^{-1} (\mb{x} + \mb{\mu})\Bigr) \mathrm{d} \mb{x}
% \\ &= 0.5 - 
% \int_{\mb{x}\T \mb{a} > 0} C \exp\Bigl(-\tfrac{1}{2}(\mb{x} - \mb{\mu})\T \mb{\Sigma}^{-1} (\mb{x} - \mb{\mu})\Bigr) \mathrm{d} \mb{x}
% + \int_{\mb{x}\T \mb{a} > 0} C \exp\Bigl(-\tfrac{1}{2}(\mb{x} + \mb{\mu})\T \mb{\Sigma}^{-1} (\mb{x} + \mb{\mu})\Bigr) \mathrm{d} \mb{x}
% \\ &= 0.5 - C \int_{\mb{x}\T \mb{a} > 0} \Bigl(\exp(-\tfrac{1}{2}(\mb{x} - \mb{\mu})\T \mb{\Sigma}^{-1} (\mb{x} - \mb{\mu})) - \exp(-\tfrac{1}{2}(\mb{x} + \mb{\mu})\T \mb{\Sigma}^{-1} (\mb{x} + \mb{\mu})) \Bigr) \mathrm{d} \mb{x}
% \\ &= 0.5 - C \int_{\mb{x}\T \mb{a} > 0} h(\mb{x}) \mathrm{d} \mb{x}
% \end{split}
% \end{equation*}
% where $h(\mb{x}) = \exp(-\tfrac{1}{2}(\mb{x} - \mb{\mu})\T \mb{\Sigma}^{-1} (\mb{x} - \mb{\mu})) - \exp(-\tfrac{1}{2}(\mb{x} + \mb{\mu})\T \mb{\Sigma}^{-1} (\mb{x} + \mb{\mu}))$.
% In particular, $L_*$ is given by
% $$L_* = 0.5 - C \int_{\mb{x}\T \mb{\Sigma}^{-1} \mb{\delta} > 0} h(\mb{x}) \mathrm{d} \mb{x}, $$
% where $h(\mb{x}) = \exp(-\tfrac{1}{2}(\mb{x} - \mb{\mu})\T \mb{\Sigma}^{-1} (\mb{x} - \mb{\mu})) - \exp(-\tfrac{1}{2}(\mb{x} + \mb{\mu})\T \mb{\Sigma}^{-1} (\mb{x} + \mb{\mu})$. 
% And since  $h(\mb{x}) \geq 0$ if and only if $\mb{x}\T \mb{\Sigma}^{-1} \mb{\delta} \geq 0$, $L_*$ is indeed the Bayes risk. 

% Thus, $L_A \leq L_B$ is equivalent to 
% \begin{equation*}
% \int_{\mb{x}\T \mb{a} > 0} h(x) \mathrm{d} \mb{x} \geq \int_{\mb{x}\T \mb{b} > 0} h(x) \mathrm{d} \mb{x}.
% \end{equation*}

% We now analyze the set $\{\mb{x} \colon \mb{x}\T \mb{a} > 0\}$. We write 
% $\mb{x}\T \mb{a} = \mb{x}\T \mb{\Sigma}^{-1} \mb{\delta} + \mb{x}\T (\mb{a} - \mb{\Sigma}^{-1} \mb{\delta})$. 
% Without loss of generality, we can further assume that $\|(\mb{\Sigma}^{-1} \mb{\delta})\T \| = 1$. Then, because $\mb{a}\T \mb{a} = 1$, we have that
% $\|\mb{a} - \mb{\Sigma}^{-1} \mb{\delta} \|^{2} = 2 - 2 \mb{a}\T \mb{\Sigma}^{-1} \mb{\delta}$. 
% Let $c_a = 2 - 2 \mb{a}\T \mb{\Sigma}^{-1} \mb{\delta}$; we shall assume for the moment that $c_a$ is ``small''. Then for any $\eta > 0$, we have
% \begin{equation*}
% \{\mb{x} \colon \mb{x}\T \mb{a} > 0 \} = \{ \mb{x} \colon \mb{x}\T \mb{a} > 0 \} \cap \Bigl(\{\mb{x} \colon \mb{x}\T \mb{\Sigma}^{-1} \mb{\delta} > \eta \} \cup \{\mb{x} \colon \mb{x}\T \mb{\Sigma}^{-1} \mb{\delta} \in [-\eta, \eta] \} \cup \{ \mb{x} \colon \mb{x}\T \mb{\Sigma}^{-1} \mb{\delta} < -\eta \}\Bigr)
% \end{equation*}
% Now provided that $c_a$ is ``small'' we can choose $\eta = Kc_a$ for some constant $K$ such that 
% \begin{gather}
% \int_{\mb{x}\T \mb{\Sigma}^{-1} \mb{\delta} \in [-\eta, \eta]} h(\mb{x}) \mathrm{d} \mb{x} \approx 0
% \label{part:a}
% \\
% \int_{\substack{\mb{x} \colon \mb{x}\T \mb{\Sigma}^{-1} \mb{\delta} < -\eta \\ \mb{x}\T \mb{a} > 0 }} h(\mb{x}) \mathrm{d}\mb{x} \approx 0 
% \label{part:b}
% \\
%  \int_{\substack{\mb{x} \colon \mb{x}\T \mb{\Sigma}^{-1} \mb{\delta} > \eta \\ \mb{x}\T \mb{a} > 0 }} h(\mb{x}) \mathrm{d}\mb{x} \approx 
% \int_{\mb{x} \colon \mb{x}\T \mb{\Sigma}^{-1} \mb{\delta} > \eta } h(\mb{x}) \mathrm{d} \mb{x}
% \label{part:c}
% \end{gather}

% Therefore,
% \begin{equation*}
% \int_{\mb{x}\T \mb{a} > 0} h(\mb{x}) \mathrm{d} \mb{x} \approx \int_{\mb{x}\T \mb{\Sigma}^{-1} \mb{\delta} > K c_a} h(\mb{x}) \mathrm{d} \mb{x}
% \end{equation*}

% We note that in general, the integrals in Eq.~\eqref{part:a} and Eq.~\eqref{part:b} are non-positive. Therefore, letting $\eta' = K c_b$ where $c_b = 2 - 2 \mb{a}\T \mb{\Sigma}^{-1} \mb{\delta}$, we have
% \begin{equation*}
% \int_{\mb{x}\T \mb{b} > 0} h(\mb{x}) \mathrm{d} \mb{x} \leq \int_{\substack{\mb{x} \colon \mb{x}\T \mb{\Sigma}^{-1} \mb{\delta} > K c_b \\ \mb{x}\T \mb{b} > 0 }} h(\mb{x}) \mathrm{d} \mb{x} \leq
% \int_{\mb{x} \colon \mb{x}\T \mb{\Sigma}^{-1} \mb{\delta} > K c_b } h(\mb{x}) \mathrm{d} \mb{x}.
% \end{equation*}
% for any $\mb{b}$. 
% Therefore, because $h(\mb{x})$ is positive on the set $\mb{x}\T \mb{\Sigma}^{-1} \mb{\delta} > 0$, we can thus conclude that there exists some $\epsilon_2 > \epsilon_1 > 0$ such that
% \begin{equation*}
% \int_{\mb{x}\T \mb{a} > 0} h(\mb{x}) \geq \int_{\mb{x}\T \mb{b} > 0} h(\mb{x}).
% \end{equation*}
% for any $\mb{a}$ and $\mb{b}$ with $c_a < \epsilon_1$ and $c_b > \epsilon_2$. 
>>>>>>> 10e9e0fe9df8fe199320671c68fa2ecc62bdc025

% \end{proof}



% Note that Conjecture \ref{q:a2} is a stronger statement than Conjecture \ref{q:a1}, and in particular, if Conjecture \ref{q:a2} is true, then so is Conjecture \ref{q:a1}.




\subsubsection{When $d=1$}

\begin{remark}
If $\bA \in \mc{A}_*$ and $\bB \notin \mc{A}_*$, then $\angle(\bA\T,\bA_*\T)=1$ and $\angle(\bB\T,\bP_*\T)<1$, and therefore $\angle(\bA\T,\bA_*\T) > \angle(\bB\T,\bA_*\T)$.
\end{remark}



\begin{lem} \label{q:angle}
When $d=1$:
% \begin{lem}
\label{q:a1}
$$\angle(\bA\T,\bA_*\T) \leq \angle(\bB\T,\bA_*\T) \implies \angle(\bP_A,\bP_*) \leq \angle(\bP_B,\bP_*).$$
% \end{lem}
\end{lem}

\begin{proof}
A little arithmetic shows that the left hand side means that:
$$\frac{\bA \bSig^{-1} \bdel}{\|\bA\| \|\bSig^{-1} \bdel\|} > \frac{\bB \bSig^{-1} \bdel.}{\|\bB\| \|\bSig^{-1} \bdel\|}. $$

Similarly, the right hand side means that:
$$\frac{\bdel\T \bA\T (\bA \bSig \bA\T)^{-1} \bA \bSig^{-1} \bdel}{\|\bdel\T \bA\T (\bA \bSig \bA\T)^{-1} \bA\| \|\bSig^{-1} \bdel\|} > \frac{\bdel\T \bB\T (\bB \bSig \bB\T)^{-1} \bB \bSig^{-1} \bdel}{\|\bdel\T \bB\T (\bB \bSig \bB\T)^{-1} \bB\| \|\bSig^{-1} \bdel \|}$$.

Furthermore, $\bA \bSig \bA\T \in \mathbb{R}$ and $\bB \bSig \bB\T \in \mathbb{R}$ as $\mb{A}$ and $\mb{B}$ are, in this case, both $1 \times p$ matrices. 
Similary, $\bdel\T \bA\T \in \mathbb{R}$ and $\bdel\T \bB\T \in \mathbb{R}$. Thus, the above display equation is equivalent to
$$\frac{\bA \bSig^{-1} \bdel}{\|\bA\|} \geq \frac{\bB \bSig^{-1} \bdel}{\|\bB\|}$$
which holds if and only if left hand side holds.
\end{proof}



\newpage
\subsection{\Pca~versus \Lol} \label{sec:pvl}


XXX this section needs revision XXX

We would like to prove that \Lol~is always better than \Pca, when using one or the other to project the data onto a low dimensional space, followed by classifying with \Lda, under a wide variety of of settings.  Formally, we would like to prove $\PP[L_{\Lol}^d \leq L_{\Pca}^d]$ is big.  To do so, we ask a sequence of increasingly sophisticated questions that have the following form:
\begin{compactenum}
\item Under which parameter settings is \Lol~better than \Pca?
\item How often do those parameter settings arise, under various statistical models of the parameters?
\end{compactenum}


Recall that for the $C$-class classification problem, the parameter from which we sample the data is $\bth_c=(\pi_c,\bmu_c,\bSig_c)$, where

\begin{compactitem}
\item the class probabilities are non-negative and sum to unity: $\bpi=(\pi_1,\ldots,\pi_C) \in \triangle_C := \sum_{c \in \mc{C}} \pi_c = 1$ and $\pi_c \geq 0 \forall c \in \mc{C}$,
\item the class means are $p$-dimensional vectors: $\bmu_c \in \Real^{p}$ is the class $c$ mean vector, and
\item the class covariances are positive definite $p \times p$ real matrices: $\bSig_c  \in \Real^{p \times p}_+$ is the class conditional covariance matrix (and $\Real^{p \times p}_+$ is the set of positive definite real $p\times p$ matrices).
\end{compactitem}

The fully unconstrained parameter space is therefore $\bTh=\{ \triangle_C \times (\Real^p, \Real^{p \times p}_+)^C \}$.

When using a projection based classifier, we also have the hyperparameter $d$ which specifies the the dimensionality of the low-dimensional projection.

We thus try to answer the above two questions for increasingly relaxed constraints on the parameter space and $d$. To start, we assume that:
\begin{compactenum}
\item we have only two classes, $C=2$;
\item each class has the same covariance matrix, $\bSig_0=\bSig_1$.
\end{compactenum}

For simplicity (but without loss of generality), we will also assume that the two classes have centered means and equal priors, that is, $(\bmu_0+\bmu_1)/2=0$ and $\pi_0=\pi_1=1/2$ (relaxing this assumption merely changes the threshold of the classifier).  Let the parameter space defined by these constraints be denoted $\bTh' = \{\Real^{2p}, \Real_+^{p \times p} \}$. In Section \ref{sec:pvl}, we will always assume $\bth \in \bTh'$, unless otherwise specified.

Given the above, we consider the following sequence of relaxations, first when $d=1$, and then when $d < p$:
\begin{compactenum}
\item the covariance is a scaled identity matrix, that is, $\bSig=k \mb{I}$;
\item the covariance is a diagonal matrix, that is, $\bSig=\bS$, where $\bS_{ij}=\sigma_i$ when $i=j$ and is zero otherwise;
\item the covariance is an arbitrary positive definite matrix, that is  $\bSig \in \Real^{p \times p}_\succ$.
\end{compactenum}

After those, extensions to the multiclass and/or different covariance matrix setting might then also be explored.








\subsubsection{$d=1$}

In this section, we will only consider $d=1$, meaning that
\begin{compactitem}
\item \Lol~is simply $\bdel$,
\item \Pca~is simply $\bu_1$, the eigenvector corresponding to the largest eigenvalue of $\bSig$.
\end{compactitem}



\paragraph{Scaled Identity Covariance Matrix}


\begin{lem}
\Lol~better than \Pca~almost always when $\bSig=k \mb{I}$.
\end{lem}

\begin{proof}
To prove this statement, we first show how to define $\bA_{\Lol}^1$ and $\bA_{\Pca}^1$ is this setting.

\begin{compactitem}
\item $\bA_{\Lol}^1$ is simply $\bdel$, regardless of the covariance matrices and priors.
\item When $\bSig=k \bI$, $\bA_{\Pca}^1$ is a random vector, because the first principal component of a scaled identity matrix can equally be defined as any basis vector.
\end{compactitem}

Thus, only when the first eigenvector of the covariance matrix is randomly assigned to $\bdel$ will \Pca~work as well as \Lol.
\end{proof}


Given the conditions under which \Lol~is better than \Pca, we next ask how often that happens, under a reasonable model assumption.

\begin{lem}
$\PP[L_{\Lol}^1 \leq L_{\Pca}^1]=1$ when $\bSig=k \bI$ and $\bdel \sim \mc{N}(\bmu_{\delta},\bSig_{\delta})$.
\end{lem}

\begin{proof}
When $\bdel \sim \mc{N}(\bmu_\delta,\bSig_\delta)$, the probability that it exactly equals any vector $\bx \in \Real^d$ is real, this includes, of course,  $\bu_1$, the first eigenvector of $k \mb{I}$.  Thus, $\PP[\bdel \propto \bu_1]=0$, no matter how $\bu_1$ is chosen (as long as it is not chosen using the knowledge of the value of $\bdel$).
This implies that $\PP[\angle(\bu_1\T,\bA_*\T) =1 ] = 0$, and therefore, $\PP[L_{\bu_1} \geq L_*] =1$.
\end{proof}


% \newpage
\paragraph{Diagonal covariance matrix}

XXX error in this section, must normalize properly XXX

Now consider a simple generalization of the above scenario, namely, $\bSig=\bS$ is diagonal.
We want to know under what conditions is \Lol~better than \Pca, and how often that happens under a reasonable model.


Let $\bs$ be the singular values of $\bSig$.  When $\bSig$ is diagonal, its diagonal elements are $s_1,\ldots,s_p$, and we call that matrix $\bS$.
Recalling that $s_i=\lambda_i^2$, where $\lambda_i$'s are eigenvalues, and when a matrix is diagonal, $s_i=\lambda_i^2$, we have  $\bA_*=\bS^{-1} \bdel=\bs^{-1} \odot \bdel := \mt{\bdel} =(\del_1/\lambda_1^2,\ldots,\del_p/\lambda_p^2)$.



\begin{lem}
\Lol~is better than \Pca~whenever $1-\delta_1 < {s_1}/{\delta_1} \sum_{i=2}^p {\delta_i^2}/{s_i}$.
A consequence of this is that \Lol~is always better than \Pca~whenever $\delta_1>1$.
This means that when the variables are independent
\end{lem}

% \begin{question}
% When is $\angle(\bdel,\mt{\bdel}) > \angle(\bu_1, \mt{\bdel})$?
% \end{question}


\begin{proof}
Recalling that $\bu_i=\be_i$, where $\be_i$ is a vector of all zeros except a one in the $i^{th}$ element, and let $\bu^i=\bu_i$ for simplicity here, we can show that:
$$
\angle(\bdel,\mt{\bdel}) =
\sum_i \delta_i \mt{\delta}_i =
\sum_i \delta_i s^{-1}_i \delta_i =
\sum_i \delta_i^2 / s_i
$$
and
$$
\angle(\bu^1, \mt{\bdel})  = \sum_i u^1_i \mt{\delta_i} =  \sum_i u^1_i s^{-1}_i {\delta_i}= \sum_i u^1_i {\delta_i} /s_i =  u^1_1 {\delta_1} /s_1=  {\delta_1} /s_1.
$$

Thus, $\angle(\bdel,\mt{\bdel}) > \angle(\bu_1, \mt{\bdel})$ any time that
$\sum_i \delta_i^2 / s_i > \delta_1 / s_1$.
Note that at a minimum, it must be that $\delta_1 < 1$.

Conjectures 1 and 2 demonstrate that if the angle is better, than Bayes error is also better, and this completes the proof.
\end{proof}

% By definition, $\bA_{\Lol}^1=\bdel$, and $\bA_{\Pca}^1=\bu_1$, where $\bu_1$ is the eigenvector associated with the largest eigenvalue of $\bS$.

% We consider a simple version of our question: how often is the angle between $\bA_{\Lol}^1$ and $\bA_*$ bigger than the angle between $\bA_{\Pca}^1$ and $\bA_*$.  Formally, we would like to prove:



If Conjecture 1 and 2 are true, then the above also implies that \Lol's Bayes error is also better than \Pca's, in this setting.
Numerical experiments suggest this is true, and moreover, that when randomly sampling $\bdel$ and $\bS$, \Lol~always does better than \Pca.

% It would be nice to be able to show, immediately, that because $\bdel$ is closer to $\mt{\delta}$ than $\bu_1$ is, that their corresponding $\bP$ matrices and classification performance are also closer to the Bayes optimal.  Numerical results suggest this is true, but I do not know how to prove it yet.

% \begin{conj}
% $\PP[ \angle(A_{\Lol}^1,\bA_*)  \geq  \angle(\bA_{\Pca}^1,\bA_*)]=1$ for any $\bth \in \bTh_{2-\Lda}$.
% \end{conj}

% \begin{proof}
% coming soon.  numerical experiments are convincing.
% \end{proof}


\begin{conj}
\Lol~is almost always better than \Pca~whenever $s_i \iid \mc{U}(0,1)$ and $\delta_i \iid \mc{N}(0,1)$ for all $i \in [p]$.
% When $\bdel \sim \mc{N}(\bmu_\delta, \bSig_\delta)$ and $\bs \sim \log \mc{N}(\bmu_s,\bSig_s)$, $\PP[\sum_i \delta_i^2 / s_i > \delta_1 / s_1]=1$.
\end{conj}

XXX the below proof is just silly and wrong. XXX

\begin{proof}
We start with the very simple scenario of $p=1$ and condition on $s$ to build intuition.
In this setting we have:
\begin{align}
\PP[ \delta^2 / s > \delta/s ] = \PP[ \delta^2 / s - \delta/s  > 0].
\end{align}
Let $z= x+y$, where $x= \frac{\delta^2}{s}$  and $y=\frac{\delta}{-s}$.
So, $x$ has a generalized chi-squared distribution and $y$ has a normal distribution.
A bit of algebra reveals the explicit form of this distribution, however, it includes an integral that we do not know how to evaluate.  Thus, instead of solving exactly for the above, we provide a bound.

Bennett's inequality states given $X_1,\ldots, X_n$, assume:
\begin{compactitem}
\item each $X_i$ is independent of all others
\item (without loss of generality) $\EE[X_i]=0 \forall i$
\item $|X_i|<a$ almost surely for all $i$
\item $\sigma^2 = \frac{1}{n} \sum_i \VV(X_i)$.
\end{compactitem}
Then, for any $t \geq 0$

% $$ \PP[ \sum_i X_i > t] \leq \exp \left{ -\frac{n\sigma^2}{a^2} h \left( \frac{at}{n\sigma^2}\right) \right}, $$
where $h(u)=(1+u) \log (1+u) - u$.

Recalling that $\VV(\sum_i X_i) = \sum_i \VV(X_i)$ for independent random variables, and that
\begin{compactenum}
\item  $\VV(\delta/s) = s^{-2}$
\item $\VV(\delta^2/s) = h(s)$
\end{compactenum}





where $\Phi$ is the cumulative distribution function for a standard normal random variable.

Note that re-introducing randomness into $s$


Coming soon.  ``almost always'' and ``better'' mean with probability 1 and better in the sense of the angle between $\bA_{\Lol}^1$ and the Bayes vector is larger than that of $\bA_{\Pca}^1$ and the Bayes vector.
I believe this also implies Bayes error is better, but we have not yet shown that.
\end{proof}




\paragraph{Arbitrary Covariance Matrix}


\subsubsection{$d \geq 1$}
\subsubsection{Scaled Identity Covariance Matrix}
\subsubsection{Diagonal Covariance Matrix}
\subsubsection{Arbitrary Covariance Matrix}




\newpage
\section{Asymptotic Theory}

In real data problems,  the true joint distribution is unknown. Instead, what is provided is a set of training data.  We therefore assume the existence of $n$ training samples, each of which has been sampled identically and independently from the same distribution, $(\bX_i,Y_i) \iid F_{\bX,Y}$, for $i =1,2,\ldots, n$.  We can use these training samples to then estimate $f_{x|y}$ and $f_y$.  Plugging these estimates in to Eq.~\eqref{eq:bayes}, we obtain the Bayes plugin classifier:
\begin{align} \label{eq:plugin}
\mh{g}^*_n(\bx) := \argmax_y \mh{f}_{\bx|y}\mh{f}_y.
\end{align}
Under suitable conditions, it is easy to show that this Bayes plugin classifiers performance is asymptotically optimal.
Formally, we know that:
% \begin{align} \label{eq:pc}
$L_{\mh{g}^*_n} \conv L_{g^*}$.
% \end{align}


When the parameters, and we want to use a linear approach, we can implement a Bayes plug-in \Lda, which we call Fisher's Discriminant Analysis (\Fld) \cite{Fisher1925a}.  Under the two-class, equal prior, and centered means assumption, we have
% $\bSig$ and $\bdel$ are unknown, as in real data scenarios, we can use the training samples to estimate them, and plug them in, as in Eq.~\eqref{eq:plugin}:
\begin{align}
\mh{g}^*_n(\bx) := \II\{ \bx\T \mh{\bSig}^{-1} \mh{\bdel} > 0 \},
\end{align}
and $L_{\mh{g}_n}$ is the misclassification rate for an estimated classifier, $\mh{g}_n$.
% This Bayes plugin classifier is called Fisher's Linear Discriminant (FLD; in contrast to \Lda, which uses the true---not estimated---parameters).
Unfortunately, when $p \gg n$, the estimate of the covariance matrix $\bSig$ will be low-rank, and therefore, not invertible (because an infinite number of solutions all fit equally well).  In such scenarios, we seek alternative methods, even in the \Lda~model.

We would like to prove:
\begin{lem}
$L_{\mh{\Lol}}^d \conv L_*$ for any $\bth \in \bTh'$.
\end{lem}


\begin{lem}
$\PP[L_{\mh{\Pca}}^d \conv L_*]>0$ for any $\bth \in \bTh'$
\end{lem}

\newpage
\section{Finite Sample Theory}

It would be awesome to prove something like:
\begin{thm}
\begin{align*}
\PP[ \mh{L}_{\Lol}^d - \mh{L}_{\Pca}^2  > 0  ] < f(\bth,d,n),
\end{align*}
\end{thm}
which would state that \Lol~is better than \Pca, again, under suitable assumptions.


\begin{thm}
\begin{align*}
\PP[ \bP_{\Pca} \T \bP_*  - \bP_{\Lol} \T \bP_*  > t \norm{\bP_A} \norm{\bP_*} ] < f(t,p,d),
\end{align*}
\end{thm}
which would state that \Lol~is better than \Pca, again, under suitable assumptions.

In terms of distributiosn of the above, it seems that perhaps we could start simple.
Assume for the moment that $\bdel,\bu_1,\ldots,\bu_p \iid \mc{N}(\bmu_p, \bSig_p)$, and let $\bLam=(\bu_1,\ldots,\bu_p)\T$, and $\bSig = \bLam\T \bLam$.

The reason the above is probabilistic is because it is under certain assumptiosn on the \emph{distributions} of $bdel$, $\bSig$, and $\bA$.


Perhaps even simpler is to start with specific assumptions about $\bdel$, $\bSig$, and $\bA$. Because \Lda~is rotationally invariant, I believe that we can assert, without loss of generality, that $\bSig=\bS$, where $\bS$ is a diagonal matrix with diagonal entries $\sigma_1,\ldots, \sigma_p$, where all $\sigma_j > 0$.
Now, the optimal projection $\bSig^{-1} \bdel$ is just a simple dot product,  $\bd\T \bdel$, where $\bd=$diag($\bS$)$\in \Real^p$.


For example, letting $\bA=\bU_d$, and letting $\bU_i=e_i$ be the unit vector, with zeros everywhere except a one in the $i^{th}$ position,  we have
\begin{align*}
\bP_A\T \bP_* %&
= \bdel\T \bU_d\T \bU_d \bSig^{-1} \bU_d\T \bU_d \bSig^{-1} \bdel %\\ =
\bdel\T \bSig_d \bSig^{-1} \bSig_d \bSig^{-1} \bdel %\\&
= \bdel\T \bSig^{-2} \bdel.
\end{align*}


% Consider $\bP_*:=\bP_{\bA_*}$ and $\bP_{\Pca_d} := \bP_{A^{\Pca}_d}$.
% Now, consider $\alpha_{\Pca} := \angle (\bP_*, \bP_{\Pca_d})$.
% We would like to understand scenarios for which $\angle^*_{\Pca_d}$ is small, and when it is big.




% Note that this angle is a random variable, as it is a function of the sampled data $(\bX_i,Y_i) \iid P$.
So, we want to understand the probability that $\alpha_{\Pca}$ is small under different parameter settings, $\bth \in \bTh$.


\section{The R implementation of LOL}

Figure \ref{Rimpl} shows the R implementation of LOL for binary classification
using FlashMatrix \cite{FlashMatrix}. The implementation takes a $D \times I$
matrix, where each column is a training instance and each instance has D
features, and outputs a $D \times k$ projection matrix.

\clearpage
\begin{figure}[t]
\begin{lstlisting}
LOL <- function(m, labels, k) {
	counts <- fm.table(labels)
	num.labels <- length(counts$val)
	num.features <- dim(m)[1]
	nv <- k - (num.labels - 1)
	gr.sum <- fm.groupby(m, 1, fm.as.factor(labels, 2), fm.bo.add)
	gr.mean <- fm.mapply.row(gr.sum, counts$Freq, fm.bo.div, FALSE)
	diff <- fm.get.cols(gr.mean, 1) - fm.get.cols(gr.mean, 2)
	svd <- fm.svd(m, nv=0, nu=nv)
	fm.cbind(diff, svd$u)
}
\end{lstlisting}
\caption{The R implementation of LOL.}
\label{Rimpl}
\end{figure}


% First note that

% \begin{lem}
% $\bP_A = \bP_{WA}$, when $\bW \T \bW = \bI$.
% \end{lem}


% \begin{proof}
% blah
% \end{proof}

\clearpage
% \bibliography{biblol}
\bibliography{biblol.bib}
\bibliographystyle{IEEEtran}


\end{document}
