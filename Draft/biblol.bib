@article{Pearson1901a,
author = {Pearson, Karl},
doi = {10.1080/14786440109462720},
issn = {1941-5982},
journal = {Philosophical Magazine Series 6},
abstract = {this is an "abstract", |pipe|},
language = {en},
month = nov,
number = {11},
pages = {559--572},
publisher = {Taylor \& Francis Group},
title = {{On lines and planes of closest fit to systems of points in space}},
url = {http://www.tandfonline.com/doi/abs/10.1080/14786440109462720\#.VMlOwl7F-oU},
volume = {2},
year = {1901}
}
@article{Fisher1925a,
author = {Fisher, R. A.},
doi = {10.1017/S0305004100009580},
file = {::},
issn = {0305-0041},
journal = {Mathematical Proceedings of the Cambridge Philosophical Society},
language = {English},
month = oct,
number = {05},
pages = {700--725},
publisher = {Cambridge University Press},
title = {{Theory of Statistical Estimation}},
url = {http://journals.cambridge.org/abstract\_S0305004100009580},
volume = {22},
year = {1925}
}
@article{Eckart1936a,
author = {Eckart, Carl and Young, Gale},
doi = {10.1007/BF02288367},
issn = {0033-3123},
journal = {Psychometrika},
keywords = {Behavioral Science},
month = sep,
number = {3},
pages = {211--218},
publisher = {Springer New York},
title = {{The approximation of one matrix by another of lower rank}},
url = {http://www.springerlink.com/content/9v4274h33h75lq24/},
volume = {1},
year = {1936}
}
@article{Householder1938,
abstract = {Necessary and sufficient conditions are given for a set of numbers to be the mutual distances of a set of real points in Euclidean space, and matrices are found whose ranks determine the dimension of the smallest Euclidean space containing such points. Methods are indicated for determining the configuration of these points, and for approximating to them by points in a space of lower dimensionality.},
author = {Householder, A S and Young, Gale},
doi = {10.1007/BF02287916},
issn = {00333123},
journal = {Psychometrika},
number = {1},
pages = {19--22},
publisher = {Springer},
title = {{Discussion of a set of points in terms of their mutual distances}},
url = {http://www.springerlink.com/index/10.1007/BF02287916},
volume = {3},
year = {1938}
}
@article{Trunk1979a,
abstract = {In pattern recognition problems it has been noted that beyond a certain point the inclusion of additional parameters (that have been estimated) leads to higher probabilities of error. A simple problem has been formulated where the probability of error approaches zero as the dimensionality increases and all the parameters are known; on the other hand, the probability of error approaches one-half as the dimensionality increases and parameters are estimated.},
author = {Trunk, G V},
institution = {Naval Research Laboratory, Washington, DC 20375.},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {3},
pages = {306--307},
pmid = {21868861},
publisher = {New York, U.S.A.},
title = {{A problem of dimensionality: a simple example.}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4766926},
volume = {1},
year = {1979}
}
@book{Huber1981a,
abstract = {Book Description Other volumes in the Wiley Series in Probability and Mathematical Statistics Abstract Inference UIf Grenander The traditional setting of statistical inference is when both sample space and parameter space are finite dimensional Euclidean spaces or subjects of such spaces. During the last decades, however, a theory has been developed that allows the sample space to be some abstract space. More recently, mathematical techniquesespecially the method of sieveshave been constructed to enable inferences to be made in abstract parameter spaces. This work began with the authors 1950 monograph on inference in stochastic processes (for general sample space) and with the sieve methodology (for general parameter space) that the author and his co-workers at Brown University developed in the 1970s. Both of these cases are studied in this volume, which is the first comprehensive treatment of the subject. 1980 Order Statistics, 2nd Ed. Herbert A. David Presents up-to-date coverage of the theory and applications of ordered random variables and their functions. Develops the distribution theory of order statistics systematically, and treats short-cut methods, robust estimation, life testing, reliability, and extreme-value theory. Applications include procedures for the treatment of outliers and other data analysis techniques. Provides extensive references to the literature and the tables contained therein. Exercises included. 1980 Theory and Applications of Stochastic Differential Equations Zeev Schuss Presents SDE theory through its applications in the physical sciences, e.g., the description of chemical reactions, solid state diffusion and electrical conductivity, population genetics, filtering problems in communication theory, etc. Introduces the stochastic calculus in a simple way, presupposing only basic analysis and probability theory. Shows the role of first passage times and exit distributions in modeling physical phenomena. Introduces analytical methods in order to obtain information on probabilistic quantities such as moments and distribution of first passage times and transition probabilities, and demonstrates the role of partial differential equations. Methods include singular perturbation techniques, old and new asymptotic methods, and boundary layer theory. 1980 Download Description The first systematic, book-length treatment of the subject. Begins with a general introduction and the formal mathematical background behind qualitative and quantitative robustness. Stresses concepts. Provides selected numerical algorithms for computing robust estimates, as well as convergence proofs. Tables contain quantitative robustness information for a variety of estimates.},
author = {Huber, Peter J.},
booktitle = {Analysis},
doi = {10.1002/9780470434697},
isbn = {9780470434697},
number = {3},
pages = {308},
publisher = {Wiley},
series = {Wiley Series in Probability and Statistics},
title = {{Robust Statistics}},
url = {http://doi.wiley.com/10.1002/9780470434697},
volume = {82},
year = {1981}
}
@article{Kohonen1982a,
author = {Kohonen, Teuvo},
doi = {10.1007/BF00337288},
issn = {0340-1200},
journal = {Biological Cybernetics},
number = {1},
pages = {59--69},
title = {{Self-organized formation of topologically correct feature maps}},
url = {http://link.springer.com/10.1007/BF00337288},
volume = {43},
year = {1982}
}
@article{Huber1985a,
author = {Huber, Peter J.},
issn = {2168-8966},
journal = {The Annals of Statistics},
language = {EN},
month = jun,
number = {2},
pages = {435--475},
publisher = {Institute of Mathematical Statistics},
title = {{Projection Pursuit}},
url = {http://projecteuclid.org/euclid.aos/1176349519},
volume = {13},
year = {1985}
}
@article{Friedman1989a,
abstract = {Abstract Linear and quadratic discriminant analysis are considered in the small-sample, high-dimensional setting. Alternatives to the usual maximum likelihood (plug-in) estimates for the covariance matrices are proposed. These alternatives are characterized by two parameters, the values of which are customized to individual situations by jointly minimizing a sample-based estimate of future misclassification risk. Computationally fast implementations are presented, and the efficacy of the approach is examined through simulation studies and application to data. These studies indicate that in many circumstances dramatic gains in classification accuracy can be achieved.
Abstract Linear and quadratic discriminant analysis are considered in the small-sample, high-dimensional setting. Alternatives to the usual maximum likelihood (plug-in) estimates for the covariance matrices are proposed. These alternatives are characterized by two parameters, the values of which are customized to individual situations by jointly minimizing a sample-based estimate of future misclassification risk. Computationally fast implementations are presented, and the efficacy of the approach is examined through simulation studies and application to data. These studies indicate that in many circumstances dramatic gains in classification accuracy can be achieved.},
author = {Friedman, Jerome H.},
doi = {10.1080/01621459.1989.10478752},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
month = mar,
number = {405},
pages = {165--175},
publisher = {Taylor \& Francis},
title = {{Regularized Discriminant Analysis}},
url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1989.10478752},
volume = {84},
year = {1989}
}
@article{Li1991a,
abstract = {Abstract Modern advances in computing power have greatly widened scientists' scope in gathering and investigating information from many variables, information which might have been ignored in the past. Yet to effectively scan a large pool of variables is not an easy task, although our ability to interact with data has been much enhanced by recent innovations in dynamic graphics. In this article, we propose a novel data-analytic tool, sliced inverse regression (SIR), for reducing the dimension of the input variable x without going through any parametric or nonparametric model-fitting process. This method explores the simplicity of the inverse view of regression; that is, instead of regressing the univariate output variable y against the multivariate x, we regress x against y. Forward regression and inverse regression are connected by a theorem that motivates this method. The theoretical properties of SIR are investigated under a model of the form, y = f(? 1 x, ?, ? K x, $\epsilon$), where the ? k 's are the unknown row vectors. This model looks like a nonlinear regression, except for the crucial difference that the functional form of f is completely unknown. For effectively reducing the dimension, we need only to estimate the space [effective dimension reduction (e.d.r.) space] generated by the ? k 's. This makes our goal different from the usual one in regression analysis, the estimation of all the regression coefficients. In fact, the ? k 's themselves are not identifiable without a specific structural form on f. Our main theorem shows that under a suitable condition, if the distribution of x has been standardized to have the zero mean and the identity covariance, the inverse regression curve, E(x | y), will fall into the e.d.r. space. Hence a principal component analysis on the covariance matrix for the estimated inverse regression curve can be conducted to locate its main orientation, yielding our estimates for e.d.r. directions. Furthermore, we use a simple step function to estimate the inverse regression curve. No complicated smoothing is needed. SIR can be easily implemented on personal computers. By simulation, we demonstrate how SIR can effectively reduce the dimension of the input variable from, say, 10 to K = 2 for a data set with 400 observations. The spin-plot of y against the two projected variables obtained by SIR is found to mimic the spin-plot of y against the true directions very well. A chi-squared statistic is proposed to address the issue of whether or not a direction found by SIR is spurious.
Abstract Modern advances in computing power have greatly widened scientists' scope in gathering and investigating information from many variables, information which might have been ignored in the past. Yet to effectively scan a large pool of variables is not an easy task, although our ability to interact with data has been much enhanced by recent innovations in dynamic graphics. In this article, we propose a novel data-analytic tool, sliced inverse regression (SIR), for reducing the dimension of the input variable x without going through any parametric or nonparametric model-fitting process. This method explores the simplicity of the inverse view of regression; that is, instead of regressing the univariate output variable y against the multivariate x, we regress x against y. Forward regression and inverse regression are connected by a theorem that motivates this method. The theoretical properties of SIR are investigated under a model of the form, y = f(? 1 x, ?, ? K x, $\epsilon$), where the ? k 's are the unknown row vectors. This model looks like a nonlinear regression, except for the crucial difference that the functional form of f is completely unknown. For effectively reducing the dimension, we need only to estimate the space [effective dimension reduction (e.d.r.) space] generated by the ? k 's. This makes our goal different from the usual one in regression analysis, the estimation of all the regression coefficients. In fact, the ? k 's themselves are not identifiable without a specific structural form on f. Our main theorem shows that under a suitable condition, if the distribution of x has been standardized to have the zero mean and the identity covariance, the inverse regression curve, E(x | y), will fall into the e.d.r. space. Hence a principal component analysis on the covariance matrix for the estimated inverse regression curve can be conducted to locate its main orientation, yielding our estimates for e.d.r. directions. Furthermore, we use a simple step function to estimate the inverse regression curve. No complicated smoothing is needed. SIR can be easily implemented on personal computers. By simulation, we demonstrate how SIR can effectively reduce the dimension of the input variable from, say, 10 to K = 2 for a data set with 400 observations. The spin-plot of y against the two projected variables obtained by SIR is found to mimic the spin-plot of y against the true directions very well. A chi-squared statistic is proposed to address the issue of whether or not a direction found by SIR is spurious.},
author = {Li, Ker-Chau},
doi = {10.1080/01621459.1991.10475035},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
month = jun,
number = {414},
pages = {316--327},
publisher = {Taylor \& Francis},
title = {{Sliced Inverse Regression for Dimension Reduction}},
url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1991.10475035},
volume = {86},
year = {1991}
}
@article{Tibshirani1996,
author = {Tibshirani, Robert},
journal = {Journal of the Royal Statistical Society. Series B},
pages = {267--288},
title = {{Regression Shrinkage and Selection via the Lasso}},
volume = {58},
year = {1996}
}
@article{Belhumeur1997a,
abstract = {We develop a face recognition algorithm which is insensitive to
large variation in lighting direction and facial expression. Taking a
pattern classification approach, we consider each pixel in an image as a
coordinate in a high-dimensional space. We take advantage of the
observation that the images of a particular face, under varying
illumination but fixed pose, lie in a 3D linear subspace of the high
dimensional image space-if the face is a Lambertian surface without
shadowing. However, since faces are not truly Lambertian surfaces and do
indeed produce self-shadowing, images will deviate from this linear
subspace. Rather than explicitly modeling this deviation, we linearly
project the image into a subspace in a manner which discounts those
regions of the face with large deviation. Our projection method is based
on Fisher's linear discriminant and produces well separated classes in a
low-dimensional subspace, even under severe variation in lighting and
facial expressions. The eigenface technique, another method based on
linearly projecting the image space to a low dimensional subspace, has
similar computational requirements. Yet, extensive experimental results
demonstrate that the proposed \&amp;ldquo;Fisherface\&amp;rdquo; method has error
rates that are lower than those of the eigenface technique for tests on
the Harvard and Yale face databases},
author = {Belhumeur, Peter N. and Hespanha, Jo\"{a}o P. and Kriegman, David J.},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Appearance-based vision,Face recognition,Fisher's linear discriminant,Illumination invariance},
number = {7},
pages = {711--720},
pmid = {11148084},
title = {{Eigenfaces vs. fisherfaces: Recognition using class specific linear projection}},
volume = {19},
year = {1997}
}
@article{Olshausen1997a,
abstract = {The spatial receptive fields of simple cells in mammalian striate cortex have been reasonably well described physiologically and can be characterized as being localized, oriented, and bandpass, comparable with the basis functions of wavelet transforms. Previously, we have shown that these receptive field properties may be accounted for in terms of astrategy for producing asparse distribution of output activity in response to natural images. Here, in addition to describing this work in a more expansive fashion, we examine the neurobiological implications of sparsecoding. Of particular interest is the case when the code is overcomplete—i.e., when the number of code elements is greater than the effective dimensionality of the input space. Because the basis functions are non-orthogonal and not linearly independent of each other, sparsifying the code will recruit only those basis functions necessary for representing a given input, and so the input-output function will deviate from being purely linear. These deviations from linearity provide a potential explanation for the weak forms of non-linearity observed in the response properties of cortical simple cells, and they further make predictions about the expected interactions among units in response to naturalistic stimuli.},
author = {Olshausen, Bruno A. and Field, David J.},
doi = {10.1016/S0042-6989(97)00169-7},
issn = {00426989},
journal = {Vision Research},
keywords = {coding,gabor-wavelet,natural images,v1},
month = dec,
number = {23},
pages = {3311--3325},
title = {{Sparse coding with an overcomplete basis set: A strategy employed by V1?}},
url = {http://dx.doi.org/10.1016/S0042-6989(97)00169-7},
volume = {37},
year = {1997}
}
@article{Bishop1998a,
abstract = {Latent variable models represent the probability density of data in a space of several dimensions in terms of a smaller number of latent, or hidden, variables. A familiar example is factor analysis, which is based on a linear transformation between the latent space and the data space. In this article, we introduce a form of nonlinear latent variable model called the generative topographic mapping, for which the parameters of the model can be determined using the expectation-maximization algorithm. GTM provides a principled alternative to the widely used self-organizing map (SOM) of Kohonen (1982) and overcomes most of the significant limitations of the SOM. We demonstrate the performance of the GTM algorithm on a toy problem and on simulated data from flow diagnostics for a multiphase oil pipeline.},
author = {Bishop, Christopher M. and Svens\'{e}n, Markus and Williams, Christopher K. I.},
doi = {10.1162/089976698300017953},
issn = {0899-7667},
journal = {Neural Computation},
language = {en},
month = jan,
number = {1},
pages = {215--234},
publisher = {MIT Press  238 Main St., Suite 500, Cambridge, MA 02142-1046 USA journals-info@mit.edu},
title = {{GTM: The Generative Topographic Mapping}},
url = {http://www.mitpressjournals.org/doi/abs/10.1162/089976698300017953\#.VMlRX17F-oU},
volume = {10},
year = {1998}
}
@article{Rousseeuw1999a,
abstract = {The minimum covariance determinant (MCD) method of Rousseeuw is a highly robust estimator of multivariate location and scatter. Its objective is to find h observations (out of n) whose covariance matrix has the lowest determinant. Until now, applications of the MCD were hampered by the computation time of existing algorithms, which were limited to a few hundred objects in a few dimensions. We discuss two important applications of larger size, one about a production process at Philips with n = 677 objects and p = 9 variables, and a dataset from astronomy with n = 137,256 objects and p = 27 variables. To deal with such problems we have developed a new algorithm for the MCD, called FAST-MCD. The basic ideas are an inequality involving order statistics and determinants, and techniques which we call “selective iteration” and “nested extensions.” For small datasets, FAST-MCD typically finds the exact MCD, whereas for larger datasets it gives more accurate results than existing algorithms and is faster by orders...},
author = {Rousseeuw, Peter J. and Driessen, Katrien Van},
journal = {Technometrics},
keywords = {Breakdown value,Multivariate location and scatter,Outlier detection,Regression,Robust estimation},
language = {en},
month = mar,
number = {3},
pages = {212--223},
publisher = {Taylor \& Francis Group},
title = {{A Fast Algorithm for the Minimum Covariance Determinant Estimator}},
url = {http://amstat.tandfonline.com/doi/abs/10.1080/00401706.1999.10485670\#.VTejtq1Viko},
volume = {41},
year = {1999}
}
@article{Ferrari2010a,
author = {Ferrari, Davide and Yang, Yuhong},
doi = {10.1214/09-AOS687},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = {apr},
number = {2},
pages = {753--783},
title = {{Maximum L q -likelihood estimation}},
url = {http://projecteuclid.org/euclid.aos/1266586613},
volume = {38},
year = {2010}
}
@incollection{abello1998functional,
  title={A functional approach to external graph algorithms},
  author={Abello, James and Buchsbaum, Adam L and Westbrook, Jeffery R},
  booktitle={Algorithms—ESA’98},
  pages={332--343},
  year={1998},
  publisher={Springer}
}
@article{Hastie2006,
address = {New York, New York, USA},
author = {Hastie, Trevor and Church, Kenneth Ward and Li, Ping and Kdd, Kenneth Church},
doi = {10.1145/1150402.1150436},
isbn = {1595933395},
journal = {Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '06},
keywords = {expectations,matrix b preserves all,much smaller,of,pairwise dis-,provided that r consists,random projections,rates of convergence,sampling,tances of a in,the},
pages = {287},
publisher = {ACM Press},
title = {{Very sparse random projections}},
url = {http://portal.acm.org/citation.cfm?doid=1150402.1150436},
year = {2006}
}
@article{Candes2006b,
author = {Cand{\`{e}}s, Emmanuel J. and Tao, Terence},
doi = {10.1109/TIT.2006.885507},
issn = {0018-9448},
journal = {IEEE Transactions on Information Theory},
keywords = {acknowledgments,c,concentration of measure,convex optimization,dom projections,duality in optimization,e,is partially supported by,linear programming,matrices,national science foundation grants,principle,ran-,random matrices,signal recovery,singular values of random,sparsity,trigonometric expansions,uncertainty},
month = {dec},
number = {12},
pages = {5406--5425},
title = {{Near-Optimal Signal Recovery From Random Projections: Universal Encoding Strategies?}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4016283},
volume = {52},
year = {2006}
}

@inproceedings{Amdahl1967,
abstract = {For over a decade prophets have voiced the contention that the organization of a single computer has reached its limits and that truly significant advances can be made only by interconnection of a multiplicity of computers in such a manner as to permit cooperative solution. Variously the proper direction has been pointed out as general purpose computers with a generalized interconnection of memories, or as specialized computers with geometrically related memory interconnections and controlled by one or more instruction streams.},
archivePrefix = {arXiv},
arxivId = {arXiv:quant-ph/0611061v2},
author = {Amdahl, Gene M.},
booktitle = {AFIPS Spring Joint Computer Conference, 1967. AFIPS '67 (Spring). Proceedings of the},
doi = {10.1145/1465482.1465560},
eprint = {0611061v2},
isbn = {1558605398},
issn = {18816096},
keywords = {parallel-computing},
pages = {483--485},
pmid = {21914993},
primaryClass = {arXiv:quant-ph},
title = {{Validity of the Single Processor Approach to Achieving Large Scale Computing Capabilities}},
url = {http://delivery.acm.org/10.1145/1470000/1465560/p483-amdahl.pdf?ip=202.189.127.238{\&}id=1465560{\&}acc=ACTIVE SERVICE{\&}key=CDD1E79C27AC4E65.DE0A32330AE3471B.4D4702B0C3E38B35.4D4702B0C3E38B35{\&}CFID=646862329{\&}CFTOKEN=33418015{\&}{\_}{\_}acm{\_}{\_}=1426882996{\_}a666a43ab4250317fe0},
volume = {30},
year = {1967}
}
@inproceedings{Mika1999a,
abstract = {A non-linear classification technique based on Fisher's discriminant is proposed. The main ingredient is the kernel trick which allows the efficient computation of Fisher discriminant in feature space. The linear classification in feature space corresponds to a (powerful) non-linear decision function in input space. Large scale simulations demonstrate the competitiveness of our approach},
author = {Mika, S. and Ratsch, G. and Weston, J. and Scholkopf, B. and Mullers, K.R.},
booktitle = {Neural Networks for Signal Processing IX: Proceedings of the 1999 IEEE Signal Processing Society Workshop (Cat. No.98TH8468)},
doi = {10.1109/NNSP.1999.788121},
isbn = {0-7803-5673-X},
keywords = {Algorithm design and analysis,Bayes methods,Closed-form solution,Computational modeling,Feature extraction,Fisher discriminant analysis,Gaussian distribution,Kernel,Large-scale systems,Principal component analysis,Probability,Support vector machines,decision theory,feature extraction,feature space,input space,kernels,learning (artificial intelligence),linear classification,neural nets,nonlinear classification technique,nonlinear decision function,pattern classification},
pages = {41--48},
publisher = {IEEE},
shorttitle = {Neural Networks for Signal Processing IX, 1999. Pr},
title = {{Fisher discriminant analysis with kernels}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=788121},
year = {1999}
}
@article{Tishby1999a,
archivePrefix = {arXiv},
arxivId = {arXiv:physics/0004057v1},
author = {Tishby, Naftali and Pereira, Fernando C and Bialek, William},
eprint = {0004057v1},
journal = {Neural Computation},
pages = {1--16},
primaryClass = {arXiv:physics},
title = {{The information bottleneck method arXiv : physics / 0004057v1 [ physics . data-an ] 24 Apr 2000}},
year = {1999}
}
@book{Anderson1999a,
abstract = {LAPACK is a library of numerical linear algebra subroutines designed for high performance on workstations, vector computers, and shared memory multiprocessors. Release 3.0 of LAPACK introduces new routines and extends the functionality of existing routines. The most significant new routines and functions include: 1. a faster singular value decomposition computed by divide-and-conquer 2. faster routines for solving rank-deficient least squares problems: Using QR with column pivoting using the SVD based on divide-and-conquer 3. new routines for the generalized symmetric eigenproblem: faster routines based on divide-and-conquer routines based on bisection/inverse iteration, for computing part of the spectrum 4. faster routine for the symmetric eigenproblem using "relatively robust eigenvector algorithm" 5. new simple and expert drivers for the generalized nonsymmetric eigenproblem, including error bounds 6. solver for generalized Sylvester equation, used in 5 7.computational routines used in 5 Each Users' Guide comes with a 'Quick Reference Guide' card.},
author = {Anderson, E. and Bai, Z. and Bischof, C. and Blackford, S. and Demmel, J. and Dongarra, J. and Croz, J. Du and Greenbaum, A. and Hammerling, S. and McKenney, A. and Sorensen, D.},
isbn = {0898714478},
pages = {407},
publisher = {SIAM},
title = {{LAPACK Users' Guide: Third Edition}},
url = {https://books.google.com/books?hl=en\&lr=\&id=AZlvEnr9gCgC\&pgis=1},
year = {1999}
}
@article{Roweis2000a,
abstract = {Many areas of science depend on exploratory data analysis and visualization. The need to analyze large amounts of multivariate data raises the fundamental problem of dimensionality reduction: how to discover compact representations of high-dimensional data. Here, we introduce locally linear embedding (LLE), an unsupervised learning algorithm that computes low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs. Unlike clustering methods for local dimensionality reduction, LLE maps its inputs into a single global coordinate system of lower dimensionality, and its optimizations do not involve local minima. By exploiting the local symmetries of linear reconstructions, LLE is able to learn the global structure of nonlinear manifolds, such as those generated by images of faces or documents of text.},
annote = {From Duplicate 2 ( },
author = {Roweis, Sam T and Saul, L K},
doi = {10.1126/science.290.5500.2323},
issn = {0036-8075},
journal = {Science (New York, N.Y.)},
keywords = {Algorithms,Artificial Intelligence,Face,Humans,Mathematics,Pattern Recognition,Visual},
month = dec,
number = {5500},
pages = {2323--6},
pmid = {11125150},
title = {{Nonlinear dimensionality reduction by locally linear embedding.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/11125150},
volume = {290},
year = {2000}
}
@article{Tenenbaum2000a,
abstract = {Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem of dimensionality reduction: finding meaningful low-dimensional structures hidden in their high-dimensional observations. The human brain confronts the same problem in everyday perception, extracting from its high-dimensional sensory inputs-30,000 auditory nerve fibers or 10(6) optic nerve fibers-a manageably small number of perceptually relevant features. Here we describe an approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set. Unlike classical techniques such as principal component analysis (PCA) and multidimensional scaling (MDS), our approach is capable of discovering the nonlinear degrees of freedom that underlie complex natural observations, such as human handwriting or images of a face under different viewing conditions. In contrast to previous algorithms for nonlinear dimensionality reduction, ours efficiently computes a globally optimal solution, and, for an important class of data manifolds, is guaranteed to converge asymptotically to the true structure.},
annote = {From Duplicate 1 ( },
author = {Tenenbaum, Joshua B and de Silva, V and Langford, John C and Silva, Vin De},
doi = {10.1126/science.290.5500.2319},
isbn = {0036-8075},
issn = {00368075},
journal = {Science},
keywords = {Algorithms,Artificial Intelligence,Face,Humans,Mathematics,Pattern Recognition,Visual,Visual Perception},
month = dec,
number = {5500},
pages = {2319--23},
pmid = {11125149},
title = {{A global geometric framework for nonlinear dimensionality reduction.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/11125149},
volume = {290},
year = {2000}
}
@article{Breiman2001a,
author = {Breiman, Leo},
journal = {Machine learning},
number = {1},
pages = {5--32},
publisher = {Springer},
title = {{Random forests}},
volume = {45},
year = {2001}
}
@article{Tibshirani2002a,
abstract = {We have devised an approach to cancer class prediction from gene expression profiling, based on an enhancement of the simple nearest prototype (centroid) classifier. We shrink the prototypes and hence obtain a classifier that is often more accurate than competing methods. Our method of "nearest shrunken centroids" identifies subsets of genes that best characterize each class. The technique is general and can be used in many other classification problems. To demonstrate its effectiveness, we show that the method was highly efficient in finding genes for classifying small round blue cell tumors and leukemias.},
author = {Tibshirani, Robert and Hastie, Trevor and Narasimhan, Balasubramanian and Chu, Gilbert},
doi = {10.1073/pnas.082099299},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Child,DNA,Discriminant Analysis,Gene Expression,Gene Expression Profiling,Humans,Neoplasm,Neoplasm: analysis,Neoplasms,Neoplasms: classification,Neoplasms: diagnosis,Neoplasms: genetics,Precursor Cell Lymphoblastic Leukemia-Lymphoma,Precursor Cell Lymphoblastic Leukemia-Lymphoma: cl,Precursor Cell Lymphoblastic Leukemia-Lymphoma: di,Precursor Cell Lymphoblastic Leukemia-Lymphoma: ge,Probability},
month = may,
number = {10},
pages = {6567--6572},
pmid = {12011421},
title = {{Diagnosis of multiple cancer types by shrunken centroids of gene expression.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=124443\&tool=pmcentrez\&rendertype=abstract},
volume = {99},
year = {2002}
}
@book{Jolliffe2002a,
abstract = {Timmerman reviews Principal Component Analysis (2nd Ed.). I. T. Jolliffe},
author = {Jolliffe, I T},
booktitle = {Journal of the American Statistical Association},
doi = {10.1007/b98835},
isbn = {0-387-95442-2},
issn = {01621459},
keywords = {principal component analysis,statistical theory methods},
pages = {487},
title = {{Principal Component Analysis}},
url = {http://link.springer.com/10.1007/b98835},
volume = {98},
year = {2002}
}
@inproceedings{deSilva2003,
author = {de Silva, V and Tenenbaum, Joshua B},
booktitle = {Neural Information Processing Systems},
pages = {721--728},
title = {{Global Versus Local Methods in Nonlinear Dimensionality Reduction}},
year = {2003}
}
@article{Globerson2003a,
author = {Globerson, Amir and Tishby, Naftali},
doi = {10.1162/153244303322753689},
editor = {Guyon, Isabelle and Elisseeff, Andre},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {association analysis,feature extraction,information geometry,maximum entropy,mutual information},
month = oct,
number = {7-8},
pages = {1307--1331},
title = {{Sufficient Dimensionality Reduction}},
url = {http://www.crossref.org/jmlr\_DOI.html},
volume = {3},
year = {2003}
}
@article{Belkin2003a,
author = {Belkin, Mikhail and Niyogi, Partha},
journal = {Neural Computation},
pages = {1373--1396},
title = {{Laplacian Eigenmaps for Dimensionality Reduction and Data}},
volume = {15},
year = {2003}
}
@article{Fukumizu2004a,
author = {Fukumizu, Kenji and Bach, Francis R and Jordan, Michael I.},
journal = {Journal of Machine Learning Research},
keywords = {conditional independence,dimensionality reduction,feature selection,kernel methods,regression,variable selection},
pages = {73--99},
title = {{Dimensionality Reduction for Supervised Learning with Reproducing Kernel Hilbert Spaces}},
volume = {5},
year = {2004}
}
@article{Bickel2004a,
author = {Bickel, Peter J. and Levina, Elizaveta},
issn = {1350-7265},
journal = {Bernoulli},
keywords = {Fisher's linear discriminant,Gaussian coloured noise,minimax regret,naive Bayes},
month = dec,
number = {6},
pages = {989--1010},
publisher = {Bernoulli Society for Mathematical Statistics and Probability},
title = {{Some theory for Fisher's linear discriminant function, `naive Bayes', and some alternatives when there are many more variables than observations}},
url = {http://projecteuclid.org/euclid.bj/1106314847},
volume = {10},
year = {2004}
}
@article{Hastie2004,
author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome H.},
journal = {BeiJing: Publishing House of Electronics Industry},
title = {{The Elements of Statistical Learning: Data Mining, Inference, and Prediction}},
year = {2004}
}
@article{Zhang2004f,
abstract = {We present a new algorithm for manifold learning and nonlinear dimensionality reduction. Based on a set of unorganized data points sampled with noise from a parameterized manifold, the local geometry of the manifold is learned by constructing an approximation for the tangent space at each data point, and those tangent spaces are then aligned to give the global coordinates of the data points with respect to the underlying manifold. We also present an error analysis of our algorithm showing that reconstruction errors can be quite small in some cases. We illustrate our algorithm using curves and surfaces both in two-dimensional/three-dimensional (2D/3D) Euclidean spaces and in higher-dimensional Euclidean spaces. We also address several theoretical and algorithmic issues for further research and improvements.},
author = {Zhang, Zhenyue and Zha, Hongyuan},
doi = {10.1137/S1064827502419154},
issn = {1064-8275},
journal = {SIAM Journal on Scientific Computing},
keywords = {10,1137,15A18,15A23,15a18,15a23,65F15,65F50,65f15,65f50,alignment,ams subject classifications,doi,nonlinear dimensionality reduction,principal manifold,s1064827502419154,singular value decomposition,subspace,subspace alignment,tangent space},
language = {en},
month = jan,
number = {1},
pages = {313--338},
publisher = {Society for Industrial and Applied Mathematics},
title = {{Principal Manifolds and Nonlinear Dimensionality Reduction via Tangent Space Alignment}},
url = {http://epubs.siam.org/doi/abs/10.1137/S1064827502419154},
volume = {26},
year = {2004}
}
@article{Cook2005a,
abstract = {A family of dimension-reduction methods, the inverse regression (IR) family, is developed by minimizing a quadratic objective function. An optimal member of this family, the inverse regression estimator (IRE), is proposed, along with inference methods and a computational algorithm. The IRE has at least three desirable properties: (1) Its estimated basis of the central dimension reduction subspace is asymptotically efficient, (2) its test statistic for dimension has an asymptotic chi-squared distribution, and (3) it provides a chi-squared test of the conditional independence hypothesis that the response is independent of a selected subset of predictors given the remaining predictors. Current methods like sliced inverse regression belong to a suboptimal class of the IR family. Comparisons of these methods are reported through simulation studies. The approach developed here also allows a relatively straightforward derivation of the asymptotic null distribution of the test statistic for dimension used in slic...},
author = {Cook, R. Dennis and Ni, Liqiang},
doi = {10.1198/016214504000001501},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {Inverse regression estimator,Sliced average variance estimation,Sliced inverse regression,Sufficient dimension reduction},
language = {en},
month = jun,
number = {470},
pages = {410--428},
publisher = {Taylor \& Francis},
title = {{Sufficient Dimension Reduction via Inverse Regression}},
url = {http://amstat.tandfonline.com/doi/abs/10.1198/016214504000001501\#.U6tH3Y1dUts},
volume = {100},
year = {2005}
}
@article{Coifman2006a,
author = {Coifman, Ronald R and Lafon, S},
doi = {10.1016/j.acha.2006.04.006},
issn = {10635203},
journal = {Applied and Computational Harmonic Analysis},
keywords = {diffusion metric,diffusion processes,dimensionality reduction,eigenmaps,graph laplacian,manifold learning},
month = jul,
number = {1},
pages = {5--30},
title = {{Diffusion maps}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1063520306000546},
volume = {21},
year = {2006}
}
@article{Belkin2006a,
abstract = {We propose a family of learning algorithms based on a new form of regularization that allows us to exploit the geometry of the marginal distribution. We focus on a semi-supervised framework that incorporates labeled and unlabeled data in a general-purpose learner. Some transductive graph learning algorithms and standard methods including support vector machines and regularized least squares can be obtained as special cases. We use properties of reproducing kernel Hilbert spaces to prove new Representer theorems that provide theoretical basis for the algorithms. As a result (in contrast to purely graph-based approaches) we obtain a natural out-of-sample extension to novel examples and so are able to handle both transductive and truly semi-supervised settings. We present experimental evidence suggesting that our semi-supervised algorithms are able to use unlabeled data effectively. Finally we have a brief discussion of unsupervised and fully supervised learning within our general framework. Keywords:},
author = {Belkin, Mikhail and Niyogi, Partha and Sindhwani, Vikas},
doi = {10.1016/j.neuropsychologia.2009.02.028},
file = {:Users/jovo/Research/Articles/Belkin, Niyogi, Sindhwani - 2006.pdf:pdf;:Users/jovo/Research/Articles/Belkin, Niyogi, Sindhwani - 2006(2).pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {The Journal of Machine Learning Research},
keywords = {graph transduction,kernel methods,mani- fold learning,regularization,semi-supervised learning,spectral graph theory,support vector machines,unlabeled data},
month = dec,
pages = {2399--2434},
pmid = {19428409},
publisher = {JMLR.org},
title = {{Manifold Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples}},
url = {http://dl.acm.org/citation.cfm?id=1248547.1248632 http://dl.acm.org/citation.cfm?id=1248632},
volume = {7},
year = {2006}
}
@misc{Zou2006a,
abstract = {The lasso is a popular technique for simultaneous estimation and variable selection. Lasso variable selection has been shown to be consistent under certain conditions. In this work we derive a necessary condition for the lasso variable selection to be consistent. Consequently, there exist certain scenarios where the lasso is inconsistent for variable selection. We then propose a new version of the lasso, called the adaptive lasso, where adaptive weights are used for penalizing different coefficients in the ?1 penalty. We show that the adaptive lasso enjoys the oracle properties; namely, it performs as well as if the true underlying model were given in advance. Similar to the lasso, the adaptive lasso is shown to be near-minimax optimal. Furthermore, the adaptive lasso can be solved by the same efficient algorithm for solving the lasso. We also discuss the extension of the adaptive lasso in generalized linear models and show that the oracle properties still hold under mild regularity conditions. As a byproduct of our theory, the nonnegative garotte is shown to be consistent for variable selection. The lasso is a popular technique for simultaneous estimation and variable selection. Lasso variable selection has been shown to be consistent under certain conditions. In this work we derive a necessary condition for the lasso variable selection to be consistent. Consequently, there exist certain scenarios where the lasso is inconsistent for variable selection. We then propose a new version of the lasso, called the adaptive lasso, where adaptive weights are used for penalizing different coefficients in the ?1 penalty. We show that the adaptive lasso enjoys the oracle properties; namely, it performs as well as if the true underlying model were given in advance. Similar to the lasso, the adaptive lasso is shown to be near-minimax optimal. Furthermore, the adaptive lasso can be solved by the same efficient algorithm for solving the lasso. We also discuss the extension of the adaptive lasso in generalized linear models and show that the oracle properties still hold under mild regularity conditions. As a byproduct of our theory, the nonnegative garotte is shown to be consistent for variable selection.},
author = {Zou, Hui},
booktitle = {Journal of the American Statistical Association},
doi = {10.1198/016214506000000735},
isbn = {0162-1459},
issn = {0162-1459},
pages = {1418--1429},
title = {{The Adaptive Lasso and Its Oracle Properties}},
volume = {101},
year = {2006}
}
@article{Baik2006a,
abstract = {We consider a spiked population model, proposed by Johnstone, in which all the population eigenvalues are one except for a few fixed eigenvalues. The question is to determine how the sample eigenvalues depend on the non-unit population ones when both sample size and population size become large. This paper completely determines the almost sure limits of the sample eigenvalues in a spiked model for a general class of samples. © 2005 Elsevier Inc. All rights reserved.},
author = {Baik, Jinho and Silverstein, Jack W.},
journal = {Journal of Multivariate Analysis},
keywords = {Almost sure limits,Eigenvalues,Non-null case,Sample covariance matrices,Spiked population models},
number = {6},
pages = {1382--1408},
title = {{Eigenvalues of large sample covariance matrices of spiked population models}},
volume = {97},
year = {2006}
}
@article{Candes2006a,
author = {Cand\`{e}s, Emmanuel J.},
journal = {Proceedings of the International Congress of Mathematicians, Madrid, Spain},
pages = {1433--1452},
title = {{Compressive sampling}},
volume = {3},
year = {2006}
}
@book{Bishop2006a,
abstract = {The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the underlying algorithms and techniques. For example, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic techniques. The practical applicability of Bayesian methods has been greatly enhanced by the development of a range of approximate inference algorithms such as variational Bayes and expectation propagation, while new models based on kernels have had a significant impact on both algorithms and applications. This completely new textbook reflects these recent developments while providing a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory. The book is suitable for courses on machine learning, statistics, computer science, signal processing, computer vision, data mining, and bioinformatics. Extensive support is provided for course instructors, including more than 400 exercises, graded according to difficulty. Example solutions for a subset of the exercises are available from the book web site, while solutions for the remainder can be obtained by instructors from the publisher. The book is supported by a great deal of additional material, and the reader is encouraged to visit the book web site for the latest information. A forthcoming companion volume will deal with practical aspects of pattern recognition and machine learning, and will include free software implementations of the key algorithms along with example data sets and demonstration programs. Christopher Bishop is Assistant Director at Microsoft Research Cambridge, and also holds a Chair in Computer Science at the University of Edinburgh. He is a Fellow of Darwin College Cambridge, and was recently elected Fellow of the Royal Academy of Engineering. The author's previous textbook "Neural Networks for Pattern Recognition" has been widely adopted.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {Bishop, Christopher M},
booktitle = {Pattern Recognition},
chapter = {Graphical},
doi = {10.1117/1.2819119},
eprint = {0-387-31073-8},
isbn = {0387310738},
issn = {10179909},
pages = {738},
publisher = {Springer},
series = {Information science and statistics},
title = {{Pattern Recognition and Machine Learning}},
url = {http://www.library.wisc.edu/selectedtocs/bg0137.pdf},
year = {2006}
}
@article{Paul2007a,
abstract = {This paper deals with a multivariate Gaussian observation model where the eigenvalues of the covariance matrix are all one, except for a finite number which are larger. Of interest is the asymptotic behavior of the eigenvalues of the sample covariance matrix when the sample size and the dimension of the obser- vations both grow to infinity so that their ratio converges to a positive constant. When a population eigenvalue is above a certain threshold and of multiplicity one, the corresponding sample eigenvalue has a Gaussian limiting distribution. There is a “phase transition” of the sample eigenvectors in the same setting. Another contribution here is a study of the second order asymptotics of sample eigenvectors when corresponding eigenvalues are simple and sufficiently large.},
author = {Paul, Debashis},
issn = {1017-0405},
journal = {Statistica Sinica},
keywords = {and phrases,eigenvalue distribution,principal component analysis,random matrix theory},
number = {4},
pages = {1617},
title = {{Asymptotics of sample eigenstructure for a large dimensional spiked covariance model}},
url = {http://www3.stat.sinica.edu.tw/statistica/oldpdf/A17n418.pdf},
volume = {17},
year = {2007}
}
@article{Bouveyron2007,
author = {Bouveyron, Charles and Girard, S. and Schmid, C.},
doi = {10.1016/j.csda.2007.02.009},
issn = {01679473},
journal = {Computational Statistics \& Data Analysis},
keywords = {gaussian mixture models,high-dimensional data,model-based clustering,parsimonious models,subspace clustering},
month = sep,
number = {1},
pages = {502--519},
title = {{High-dimensional data clustering}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167947307000692},
volume = {52},
year = {2007}
}
@article{Fan2008a,
author = {Fan, Jianqing and Fan, Yingying},
issn = {2168-8966},
journal = {The Annals of Statistics},
keywords = {Classification,feature extraction,high dimensionality,independence rule,misclassification rates},
language = {EN},
month = dec,
number = {6},
pages = {2605--2637},
publisher = {Institute of Mathematical Statistics},
title = {{High-dimensional classification using features annealed independence rules}},
url = {http://projecteuclid.org/euclid.aos/1231165181},
volume = {36},
year = {2008}
}
@article{Donoho2008a,
abstract = {In important application fields today-genomics and proteomics are examples-selecting a small subset of useful features is crucial for success of Linear Classification Analysis. We study feature selection by thresholding of feature Z-scores and introduce a principle of threshold selection, based on the notion of higher criticism (HC). For i = 1, 2, ..., p, let pi(i) denote the two-sided P-value associated with the ith feature Z-score and pi((i)) denote the ith order statistic of the collection of P-values. The HC threshold is the absolute Z-score corresponding to the P-value maximizing the HC objective (i/p - pi((i)))/sqrt\{i/p(1-i/p)\}. We consider a rare/weak (RW) feature model, where the fraction of useful features is small and the useful features are each too weak to be of much use on their own. HC thresholding (HCT) has interesting behavior in this setting, with an intimate link between maximizing the HC objective and minimizing the error rate of the designed classifier, and very different behavior from popular threshold selection procedures such as false discovery rate thresholding (FDRT). In the most challenging RW settings, HCT uses an unconventionally low threshold; this keeps the missed-feature detection rate under better control than FDRT and yields a classifier with improved misclassification performance. Replacing cross-validated threshold selection in the popular Shrunken Centroid classifier with the computationally less expensive and simpler HCT reduces the variance of the selected threshold and the error rate of the constructed classifier. Results on standard real datasets and in asymptotic theory confirm the advantages of HCT.},
author = {Donoho, David L. and Jin, Jiashun},
doi = {10.1073/pnas.0807471105},
file = {::},
issn = {1091-6490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Bias (Epidemiology),Data Collection,Data Collection: statistics \& numerical data,Genomics,Genomics: statistics \& numerical data,Linear Models,Proteomics,Proteomics: statistics \& numerical data},
month = sep,
number = {39},
pages = {14790--5},
pmid = {18815365},
title = {{Higher criticism thresholding: Optimal feature selection when useful features are rare and weak.}},
url = {http://www.pnas.org/content/105/39/14790 http://www.pnas.org/content/105/39/14790.short},
volume = {105},
year = {2008}
}
@article{Candes2009b,
abstract = {This paper is about a curious phenomenon. Suppose we have a data matrix, which is the superposition of a low-rank component and a sparse component. Can we recover each component individually? We prove that under some suitable assumptions, it is possible to recover both the low-rank and the sparse components exactly by solving a very convenient convex program called Principal Component Pursuit; among all feasible decompositions, simply minimize a weighted combination of the nuclear norm and of the L1 norm. This suggests the possibility of a principled approach to robust principal component analysis since our methodology and results assert that one can recover the principal components of a data matrix even though a positive fraction of its entries are arbitrarily corrupted. This extends to the situation where a fraction of the entries are missing as well. We discuss an algorithm for solving this optimization problem, and present applications in the area of video surveillance, where our methodology allows for the detection of objects in a cluttered background, and in the area of face recognition, where it offers a principled way of removing shadows and specularities in images of faces.},
annote = {From Duplicate 2 ( },
archivePrefix = {arXiv},
arxivId = {0912.3599},
author = {Cand\`{e}s, Emmanuel J. and Li, Xiaodong and Ma, Yi and Wright, John},
doi = {10.1145/1970392.1970395},
eprint = {0912.3599},
issn = {00045411},
journal = {Journal of the ACM},
keywords = {Principal components,duality,low-rank matrices,minimization,nuclear-norm minimization,principal components,robustness vis-a-vis outliers,sparsity,video surveillance,ℓ 1 -norm minimization},
month = dec,
number = {3},
pages = {1--37},
publisher = {ACM},
title = {{Robust Principal Component Analysis?}},
url = {http://dl.acm.org/citation.cfm?id=1970392.1970395 http://arxiv.org/abs/0912.3599},
volume = {58},
year = {2009}
}
@article{Witten2009a,
abstract = {In recent years, many methods have been developed for regression in high-dimensional settings. We propose covariance-regularized regression, a family of methods that use a shrunken estimate of the inverse covariance matrix of the features in order to achieve superior prediction. An estimate of the inverse covariance matrix is obtained by maximizing its log likelihood, under a multivariate normal model, subject to a constraint on its elements; this estimate is then used to estimate coefficients for the regression of the response onto the features. We show that ridge regression, the lasso, and the elastic net are special cases of covariance-regularized regression, and we demonstrate that certain previously unexplored forms of covariance-regularized regression can outperform existing methods in a range of situations. The covariance-regularized regression framework is extended to generalized linear models and linear discriminant analysis, and is used to analyze gene expression data sets with multiple class and survival outcomes.},
author = {Witten, Daniela M. and Tibshirani, Robert},
doi = {10.1111/j.1467-9868.2009.00699.x},
file = {::},
issn = {1369-7412},
journal = {Journal of the Royal Statistical Society. Series B, Statistical methodology},
month = feb,
number = {3},
pages = {615--636},
pmid = {20084176},
title = {{Covariance-regularized regression and classification for high-dimensional problems.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2806603\&tool=pmcentrez\&rendertype=abstract},
volume = {71},
year = {2009}
}
@inproceedings{Mairal2009,
author = {Mairal, Julien and Ponce, Jean and Sapiro, Guillermo and Zisserman, Andrew and Bach, Francis R.},
booktitle = {Advances in Neural Information Processing Systems},
file = {::},
pages = {1033--1040},
title = {{Supervised Dictionary Learning}},
url = {http://papers.nips.cc/paper/3448-supervised},
year = {2009}
}
@article{Halko2011a,
abstract = {Low-rank matrix approximations, such as the truncated singular value decomposition and the rank-revealing QR decomposition, play a central role in data analysis and scientific computing. This work surveys and extends recent research which demonstrates that randomization offers a powerful tool for performing low-rank matrix approximation. These techniques exploit modern computational architectures more fully than classical methods and open the possibility of dealing with truly massive data sets. This paper presents a modular framework for constructing randomized algorithms that compute partial matrix decompositions. These methods use random sampling to identify a subspace that captures most of the action of a matrix. The input matrix is then compressed—either explicitly or implicitly—to this subspace, and the reduced matrix is manipulated deterministically to obtain the desired low-rank factorization. In many cases, this approach beats its classical competitors in terms of accuracy, robustness, and/or spee...},
author = {Halko, N. and Martinsson, P. G. and Tropp, Joel A.},
doi = {10.1137/090771806},
issn = {0036-1445},
journal = {SIAM Review},
keywords = {60B20,65F30,68W20,Johnson–Lindenstrauss lemma,dimension reduction,eigenvalue decomposition,interpolative decomposition,matrix approximation,parallel algorithm,pass-efficient algorithm,principal component analysis,random matrix,randomized algorithm,rank-revealing QR factorization,singular value decomposition,streaming algorithm},
language = {en},
month = jan,
number = {2},
pages = {217--288},
publisher = {Society for Industrial and Applied Mathematics},
title = {{Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions}},
url = {http://epubs.siam.org/doi/abs/10.1137/090771806},
volume = {53},
year = {2011}
}
@inproceedings{Lopes2011a,
author = {Lopes, Miles and Jacob, Laurent and Wainwright, Martin J.},
booktitle = {Neural Information Processing Systems},
file = {::},
pages = {1206--1214},
title = {{A More Powerful Two-Sample Test in High Dimensions using Random Projection}},
url = {http://papers.nips.cc/paper/4260-a-more-powerful-two-sample-test-in-high-dimensions-using-random-projection},
year = {2011}
}
@article{Chang2011a,
author = {Chang, Chih-Chung and Lin, Chih-Jen},
doi = {10.1145/1961189.1961199},
file = {::},
issn = {21576904},
journal = {ACM Transactions on Intelligent Systems and Technology},
keywords = {Classification LIBSVM optimization regression supp},
month = apr,
number = {3},
pages = {1--27},
publisher = {ACM},
title = {{LIBSVM}},
url = {http://dl.acm.org/citation.cfm?id=1961189.1961199},
volume = {2},
year = {2011}
}
@article{Clemmensen2011a,
author = {Clemmensen, Line and Hastie, Trevor and Witten, Daniela M. and Ersb\o ll, Bjarne},
doi = {10.1198/TECH.2011.08118},
issn = {0040-1706},
journal = {Technometrics},
month = nov,
number = {4},
pages = {406--413},
title = {{Sparse Discriminant Analysis}},
url = {http://pubs.amstat.org/doi/abs/10.1198/TECH.2011.08118},
volume = {53},
year = {2011}
}
@article{Eklund2012,
abstract = {The validity of parametric functional magnetic resonance imaging (fMRI) analysis has only been reported for simulated data. Recent advances in computer science and data sharing make it possible to analyze large amounts of real fMRI data. In this study, 1484 rest datasets have been analyzed in SPM8, to estimate true familywise error rates. For a familywise significance threshold of 5\%, significant activity was found in 1\%-70\% of the 1484 rest datasets, depending on repetition time, paradigm and parameter settings. This means that parametric significance thresholds in SPM both can be conservative or very liberal. The main reason for the high familywise error rates seems to be that the global AR(1) auto correlation correction in SPM fails to model the spectra of the residuals, especially for short repetition times. The findings that are reported in this study cannot be generalized to parametric fMRI analysis in general, and other software packages may give different results. By using the computational power of the graphics processing unit (GPU), the 1484 rest datasets were also analyzed with a random permutation test. Significant activity was then found in 1\%-19\% of the datasets. These findings speak to the need for a better model of temporal correlations in fMRI timeseries.},
author = {Eklund, Anders and Andersson, Mats and Josephson, Camilla and Johannesson, Magnus and Knutsson, Hans},
doi = {10.1016/j.neuroimage.2012.03.093},
issn = {1095-9572},
journal = {NeuroImage},
keywords = {Familywise error rate,Functional magnetic resonance imaging (fMRI),Graphics processing unit (GPU),Non-parametric statistics,Random field theory,Random permutation test,functional magnetic resonance imaging},
month = apr,
number = {3},
pages = {565--578},
pmid = {22507229},
publisher = {Elsevier Inc.},
title = {{Does parametric fMRI analysis with SPM yield valid results?-An empirical study of 1484 rest datasets.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22507229},
volume = {61},
year = {2012}
}
@article{Allard2012,
author = {Allard, William K. and Chen, Guangliang and Maggioni, Mauro},
doi = {10.1016/j.acha.2011.08.001},
issn = {10635203},
journal = {Applied and Computational Harmonic Analysis},
keywords = {multi-scale analysis},
month = may,
number = {3},
pages = {435--462},
publisher = {Elsevier Inc.},
title = {{Multi-scale geometric methods for data sets II: Geometric Multi-Resolution Analysis}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1063520311000868},
volume = {32},
year = {2012}
}
@article{Fan2012a,
author = {Fan, Jianqing and Feng, Yang and Tong, Xin},
doi = {10.1111/j.1467-9868.2012.01029.x},
issn = {13697412},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
month = sep,
number = {4},
pages = {745--771},
title = {{A road to classification in high dimensional space: the regularized optimal affine discriminant}},
url = {http://doi.wiley.com/10.1111/j.1467-9868.2012.01029.x},
volume = {74},
year = {2012}
}
@article{Cook2013,
author = {Cook, R. Dennis and Forzani, Liliana and Rothman, Adam J.},
journal = {Electronic Journal of Statistics},
pages = {3059--3088},
title = {{Prediction in abundant high-dimensional linear regression}},
url = {https://projecteuclid.org/euclid.ejs/1387207935},
volume = {7},
year = {2013}
}
@article{Mai2013a,
abstract = {Abstract In this paper we reveal the connection and equivalence of three sparse linear discriminant analysis methods : the ℓ1-Fisher's discriminant analysis proposed in Wu et al.(2008), the sparse optimal scoring proposed in Clemmensen et al.(2011) and the direct ... $\backslash$n},
author = {Mai, Qing and Zou, Hui},
doi = {10.1080/00401706.2012.746208},
issn = {0040-1706},
journal = {Technometrics},
pages = {243--246},
title = {{A Note On the Connection and Equivalence of Three Sparse Linear Discriminant Analysis Methods}},
url = {http://www.tandfonline.com/doi/abs/10.1080/00401706.2012.746208},
volume = {55},
year = {2013}
}
@article{DiMartino2013a,
abstract = {Autism spectrum disorders (ASDs) represent a formidable challenge for psychiatry and neuroscience because of their high prevalence, lifelong nature, complexity and substantial heterogeneity. Facing these obstacles requires large-scale multidisciplinary efforts. Although the field of genetics has pioneered data sharing for these reasons, neuroimaging had not kept pace. In response, we introduce the Autism Brain Imaging Data Exchange (ABIDE)—a grassroots consortium aggregating and openly sharing 1112 existing resting-state functional magnetic resonance imaging (R-fMRI) data sets with corresponding structural MRI and phenotypic information from 539 individuals with ASDs and 573 age-matched typical controls (TCs; 7–64 years) (http://fcon\_1000.projects.nitrc.org/indi/abide/). Here, we present this resource and demonstrate its suitability for advancing knowledge of ASD neurobiology based on analyses of 360 male subjects with ASDs and 403 male age-matched TCs. We focused on whole-brain intrinsic functional connectivity and also survey a range of voxel-wise measures of intrinsic functional brain architecture. Whole-brain analyses reconciled seemingly disparate themes of both hypo- and hyperconnectivity in the ASD literature; both were detected, although hypoconnectivity dominated, particularly for corticocortical and interhemispheric functional connectivity. Exploratory analyses using an array of regional metrics of intrinsic brain function converged on common loci of dysfunction in ASDs (mid- and posterior insula and posterior cingulate cortex), and highlighted less commonly explored regions such as the thalamus. The survey of the ABIDE R-fMRI data sets provides unprecedented demonstrations of both replication and novel discovery. By pooling multiple international data sets, ABIDE is expected to accelerate the pace of discovery setting the stage for the next generation of ASD studies.},
author = {{Di Martino}, Adriana and Yan, C-G and Li, Q and Denio, E and Castellanos, Xavier Francisco and Alaerts, K and Anderson, J S and Assaf, M and Bookheimer, S Y and Dapretto, M and Deen, B and Delmonte, S and Dinstein, I and Ertl-Wagner, B and Fair, D A and Gallagher, L and Kennedy, D P and Keown, C L and Keysers, C and Lainhart, J E and Lord, C and Luna, B and Menon, V and Minshew, N J and Monk, C S and Mueller, S and M\"{u}ller, R-A and Nebel, M B and Nigg, J T and O'Hearn, K and Pelphrey, K A and Peltier, S J and Rudie, J D and Sunaert, S and Thioux, M and Tyszka, J M and Uddin, L Q and Verhoeven, J S and Wenderoth, N and Wiggins, J L and Mostofsky, Stewart H and Milham, Michael Peter},
doi = {10.1038/mp.2013.78},
issn = {1359-4184},
journal = {Molecular Psychiatry},
keywords = {data sharing,default network,interhemispheric connectivity,intrinsic functional connectivity,resting-state fMRI,thalamus},
month = jun,
publisher = {Macmillan Publishers Limited},
shorttitle = {Mol Psychiatry},
title = {{The autism brain imaging data exchange: towards a large-scale evaluation of the intrinsic brain architecture in autism}},
url = {http://dx.doi.org/10.1038/mp.2013.78},
year = {2013}
}
@article{Zhang2014a,
author = {Zhang, Teng and Lerman, Gilad},
file = {::},
journal = {Journal of Machine Learning Research},
pages = {749--808},
title = {{A Novel M-Estimator for Robust PCA}},
url = {http://jmlr.org/papers/v15/zhang14a.html},
volume = {15},
year = {2014}
}
@misc{mnist,
author = {LeCun, Yann and Cortes, Corinna and Burges, Chris},
title = {{MNIST handwritten digit database}},
url = {http://yann.lecun.com/exdb/mnist/},
urldate = {2015-07-01}
}

@article{FlashMatrix,
	title={FlashMatrix: Parallel, Scalable Data Analysis with Generalized Matrix Operations using Commodity SSDs},
	author={Zheng, Da and Mhembere, Disa and Vogelstein, Joshua T and Priebe, Carey E and Burns, Randal},
	journal={arXiv preprint arXiv:1604.06414},
	year={2016}
}

@article{SEM_SpMM,
	author    = {Da Zheng and Disa Mhembere and Vince Lyzinski and Joshua Vogelstein and Carey E. Priebe and Randal Burns},
	title     = {Semi-External Memory Sparse Matrix Multiplication on Billion-node Graphs in a Multicore Architecture},
	journal   = {CoRR},
	volume    = {abs/1602.02864},
	year      = {2016},
}

@article{FlashEigen,
	author    = {Da Zheng and Randal Burns and Joshua Vogelstein and Carey E. Priebe and Alexander S. Szalay},
	title     = {An SSD-based eigensolver for spectral analysis on billion-node graphs},
	journal   = {CoRR},
	volume    = {abs/1602.01421},
	year      = {2016},
}

@inproceedings {FlashGraph,
	author = {Da Zheng and Disa Mhembere and Randal Burns and Joshua Vogelstein and Carey E. Priebe and Alexander S. Szalay},
	title = {{FlashGraph}: Processing Billion-Node Graphs on an Array of Commodity {SSDs}},
	booktitle = {13th USENIX Conference on File and Storage Technologies (FAST 15)},
	year = {2015},
	address = {Santa Clara, CA},
}

@inproceedings{SAFS,
	author = {Zheng, Da and Burns, Randal and Szalay, Alexander S.},
	title = {Toward Millions of File System IOPS on Low-cost, Commodity Hardware},
	booktitle = {Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis},
	year = {2013},
	location = {Denver, Colorado},
} 
